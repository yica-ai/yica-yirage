#!/usr/bin/env python3
"""
YICAç¥ç»ç½‘ç»œæ¨¡å‹ä¼˜åŒ–å®éªŒå¥—ä»¶
åŸºäºMirageç°æœ‰ç”¨ä¾‹ï¼Œæ‰©å±•YICAç‰¹å®šçš„æ“ä½œä¼˜åŒ–
"""

import sys
import os
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import subprocess

# æ·»åŠ YICAä¼˜åŒ–å™¨è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'mirage', 'python'))

# å¯¼å…¥YICAä¼˜åŒ–å™¨ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
try:
    from mirage.yica_optimizer import YICAConfig, YICAMirageOptimizer
    MIRAGE_AVAILABLE = True
except ImportError:
    print("âš ï¸  ä½¿ç”¨æ¨¡æ‹ŸYICAä¼˜åŒ–å™¨")
    MIRAGE_AVAILABLE = False
    
    # ä½¿ç”¨ç‹¬ç«‹çš„YICAä¼˜åŒ–å™¨
    from demo_yica_standalone import YICAConfig, YICAOptimizer as YICAMirageOptimizer, MockComputeGraph

class YICANeuralNetworkExperiments:
    """YICAç¥ç»ç½‘ç»œå®éªŒç±»"""
    
    def __init__(self, yica_config: Optional[YICAConfig] = None):
        self.yica_config = yica_config or YICAConfig()
        self.optimizer = YICAMirageOptimizer(self.yica_config)
        self.experiment_results = []
        
    def create_llama_attention_graph(self, batch_size: int = 8, seq_len: int = 2048, 
                                   hidden_size: int = 4096, num_heads: int = 32) -> Dict[str, Any]:
        """åˆ›å»ºLLaMAé£æ ¼çš„Attentionè®¡ç®—å›¾"""
        head_dim = hidden_size // num_heads
        
        # æ¨¡æ‹ŸMirageè®¡ç®—å›¾ç»“æ„
        operations = [
            f"input:Q({batch_size},{seq_len},{hidden_size})",
            f"input:K({batch_size},{seq_len},{hidden_size})",
            f"input:V({batch_size},{seq_len},{hidden_size})",
            f"reshape:Q_heads({batch_size},{seq_len},{num_heads},{head_dim})",
            f"reshape:K_heads({batch_size},{seq_len},{num_heads},{head_dim})",
            f"reshape:V_heads({batch_size},{seq_len},{num_heads},{head_dim})",
            f"transpose:K_T({batch_size},{num_heads},{head_dim},{seq_len})",
            f"matmul:scores({batch_size},{num_heads},{seq_len},{seq_len})",
            f"scale:scaled_scores({batch_size},{num_heads},{seq_len},{seq_len})",
            f"softmax:attention_weights({batch_size},{num_heads},{seq_len},{seq_len})",
            f"matmul:context({batch_size},{num_heads},{seq_len},{head_dim})",
            f"reshape:output({batch_size},{seq_len},{hidden_size})"
        ]
        
        return {
            'name': f'LLaMA_Attention_B{batch_size}_S{seq_len}_H{hidden_size}',
            'type': 'attention',
            'operations': operations,
            'parameters': {
                'batch_size': batch_size,
                'seq_len': seq_len,
                'hidden_size': hidden_size,
                'num_heads': num_heads,
                'head_dim': head_dim
            }
        }
    
    def create_gated_mlp_graph(self, batch_size: int = 8, hidden_size: int = 4096, 
                              intermediate_size: int = 14336) -> Dict[str, Any]:
        """åˆ›å»ºGated MLPè®¡ç®—å›¾"""
        operations = [
            f"input:X({batch_size},{hidden_size})",
            f"input:W1({hidden_size},{intermediate_size})",
            f"input:W2({hidden_size},{intermediate_size})",
            f"input:W3({intermediate_size},{hidden_size})",
            f"matmul:gate({batch_size},{intermediate_size})",
            f"matmul:up({batch_size},{intermediate_size})",
            f"silu:activated_gate({batch_size},{intermediate_size})",
            f"mul:gated({batch_size},{intermediate_size})",
            f"matmul:output({batch_size},{hidden_size})"
        ]
        
        return {
            'name': f'Gated_MLP_B{batch_size}_H{hidden_size}_I{intermediate_size}',
            'type': 'mlp',
            'operations': operations,
            'parameters': {
                'batch_size': batch_size,
                'hidden_size': hidden_size,
                'intermediate_size': intermediate_size
            }
        }
    
    def create_rms_norm_graph(self, batch_size: int = 8, hidden_size: int = 4096) -> Dict[str, Any]:
        """åˆ›å»ºRMS Normalizationè®¡ç®—å›¾"""
        operations = [
            f"input:X({batch_size},{hidden_size})",
            f"input:weight({hidden_size})",
            f"square:X_squared({batch_size},{hidden_size})",
            f"reduce_mean:mean_square({batch_size},1)",
            f"add:variance({batch_size},1)",
            f"rsqrt:inv_std({batch_size},1)",
            f"mul:normalized({batch_size},{hidden_size})",
            f"mul:output({batch_size},{hidden_size})"
        ]
        
        return {
            'name': f'RMS_Norm_B{batch_size}_H{hidden_size}',
            'type': 'normalization',
            'operations': operations,
            'parameters': {
                'batch_size': batch_size,
                'hidden_size': hidden_size
            }
        }
    
    def create_conv2d_graph(self, batch_size: int = 32, channels: int = 256, 
                           height: int = 224, width: int = 224,
                           out_channels: int = 512, kernel_size: int = 3) -> Dict[str, Any]:
        """åˆ›å»ºConv2Dè®¡ç®—å›¾"""
        operations = [
            f"input:X({batch_size},{channels},{height},{width})",
            f"input:weight({out_channels},{channels},{kernel_size},{kernel_size})",
            f"input:bias({out_channels})",
            f"conv2d:conv_out({batch_size},{out_channels},{height-kernel_size+1},{width-kernel_size+1})",
            f"add:output({batch_size},{out_channels},{height-kernel_size+1},{width-kernel_size+1})"
        ]
        
        return {
            'name': f'Conv2D_B{batch_size}_C{channels}_H{height}_W{width}_K{kernel_size}',
            'type': 'convolution',
            'operations': operations,
            'parameters': {
                'batch_size': batch_size,
                'in_channels': channels,
                'out_channels': out_channels,
                'height': height,
                'width': width,
                'kernel_size': kernel_size
            }
        }
    
    def create_transformer_block_graph(self, batch_size: int = 8, seq_len: int = 2048,
                                     hidden_size: int = 4096, num_heads: int = 32,
                                     intermediate_size: int = 14336) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæ•´çš„Transformer Blockè®¡ç®—å›¾"""
        operations = [
            # Pre-attention layer norm
            f"input:X({batch_size},{seq_len},{hidden_size})",
            f"rms_norm:norm1({batch_size},{seq_len},{hidden_size})",
            
            # Multi-head attention
            f"matmul:qkv({batch_size},{seq_len},{hidden_size*3})",
            f"split:q_k_v({batch_size},{seq_len},{hidden_size})",
            f"attention:attn_out({batch_size},{seq_len},{hidden_size})",
            f"matmul:attn_proj({batch_size},{seq_len},{hidden_size})",
            
            # Residual connection
            f"add:residual1({batch_size},{seq_len},{hidden_size})",
            
            # Pre-MLP layer norm
            f"rms_norm:norm2({batch_size},{seq_len},{hidden_size})",
            
            # Gated MLP
            f"matmul:gate({batch_size},{seq_len},{intermediate_size})",
            f"matmul:up({batch_size},{seq_len},{intermediate_size})",
            f"silu:activated({batch_size},{seq_len},{intermediate_size})",
            f"mul:gated({batch_size},{seq_len},{intermediate_size})",
            f"matmul:down({batch_size},{seq_len},{hidden_size})",
            
            # Final residual connection
            f"add:output({batch_size},{seq_len},{hidden_size})"
        ]
        
        return {
            'name': f'Transformer_Block_B{batch_size}_S{seq_len}_H{hidden_size}',
            'type': 'transformer_block',
            'operations': operations,
            'parameters': {
                'batch_size': batch_size,
                'seq_len': seq_len,
                'hidden_size': hidden_size,
                'num_heads': num_heads,
                'intermediate_size': intermediate_size
            }
        }
    
    def run_experiment(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        print(f"\nğŸ”¬ å®éªŒ: {graph_data['name']}")
        print(f"   ç±»å‹: {graph_data['type']}")
        print(f"   æ“ä½œæ•°: {len(graph_data['operations'])}")
        
        start_time = time.time()
        
        # åˆ›å»ºæ¨¡æ‹Ÿè®¡ç®—å›¾
        if MIRAGE_AVAILABLE:
            # ä½¿ç”¨çœŸå®çš„Mirageæ¥å£
            # mock_graph = create_mirage_graph(graph_data)
            # analysis_result = self.optimizer.analyze_mirage_graph(mock_graph)
            pass
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ¥å£
            mock_graph = MockComputeGraph(graph_data['name'], graph_data['operations'])
            analysis_result = self.optimizer.analyze_graph(mock_graph)
        
        optimization_time = time.time() - start_time
        
        # è®¡ç®—YICAç‰¹å®šæŒ‡æ ‡
        yica_metrics = self.calculate_yica_metrics(graph_data, analysis_result)
        
        result = {
            'graph_name': graph_data['name'],
            'graph_type': graph_data['type'],
            'parameters': graph_data['parameters'],
            'num_operations': len(graph_data['operations']),
            'yica_friendliness': analysis_result.yica_friendliness,
            'compute_intensity': analysis_result.compute_intensity,
            'parallelization_potential': analysis_result.parallelization_potential,
            'memory_bottleneck': analysis_result.memory_bottleneck,
            'optimization_strategies': analysis_result.optimization_strategies,
            'baseline_time_ms': analysis_result.baseline_time_ms,
            'optimized_time_ms': analysis_result.optimized_time_ms,
            'speedup_ratio': analysis_result.speedup_ratio,
            'cim_utilization': analysis_result.cim_utilization,
            'optimization_time_ms': optimization_time * 1000,
            'yica_metrics': yica_metrics,
            'generated_code_size': len(analysis_result.generated_code) if hasattr(analysis_result, 'generated_code') else 0
        }
        
        print(f"   ğŸ“Š YICAå‹å¥½åº¦: {result['yica_friendliness']:.3f}")
        print(f"   ğŸ“Š è®¡ç®—å¯†é›†åº¦: {result['compute_intensity']:.1f} GFLOPS")
        print(f"   âš¡ åŠ é€Ÿæ¯”: {result['speedup_ratio']:.1f}x")
        print(f"   ğŸ§  CIMåˆ©ç”¨ç‡: {result['cim_utilization']:.1f}%")
        print(f"   â±ï¸  ä¼˜åŒ–æ—¶é—´: {result['optimization_time_ms']:.2f}ms")
        
        return result
    
    def calculate_yica_metrics(self, graph_data: Dict[str, Any], analysis_result) -> Dict[str, float]:
        """è®¡ç®—YICAç‰¹å®šæŒ‡æ ‡"""
        graph_type = graph_data['type']
        params = graph_data['parameters']
        
        # åŸºäºå›¾ç±»å‹è®¡ç®—ç‰¹å®šæŒ‡æ ‡
        if graph_type == 'attention':
            # Attentionç‰¹å®šæŒ‡æ ‡
            seq_len = params['seq_len']
            num_heads = params['num_heads']
            
            # è®¡ç®—æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦
            attention_complexity = seq_len * seq_len * num_heads
            
            # å†…å­˜è®¿é—®æ¨¡å¼åˆ†æ
            memory_pattern_score = min(1.0, 1000.0 / seq_len)  # è¾ƒçŸ­åºåˆ—æ›´é€‚åˆCIM
            
            # å¹¶è¡ŒåŒ–æ½œåŠ›
            parallelization_score = min(1.0, num_heads / self.yica_config.num_cim_arrays)
            
            return {
                'attention_complexity': attention_complexity,
                'memory_pattern_score': memory_pattern_score,
                'parallelization_score': parallelization_score,
                'cache_efficiency': 0.8 if seq_len <= 1024 else 0.6
            }
            
        elif graph_type == 'mlp':
            # MLPç‰¹å®šæŒ‡æ ‡
            hidden_size = params['hidden_size']
            intermediate_size = params.get('intermediate_size', hidden_size * 4)
            
            # çŸ©é˜µä¹˜æ³•å‹å¥½åº¦
            matmul_friendliness = min(1.0, hidden_size / 1024.0)
            
            # è®¡ç®—å¼ºåº¦
            compute_intensity = (hidden_size * intermediate_size) / (hidden_size + intermediate_size)
            
            return {
                'matmul_friendliness': matmul_friendliness,
                'compute_intensity': compute_intensity,
                'activation_overhead': 0.1,  # SiLUæ¿€æ´»å‡½æ•°å¼€é”€
                'memory_reuse_potential': 0.9
            }
            
        elif graph_type == 'convolution':
            # å·ç§¯ç‰¹å®šæŒ‡æ ‡
            in_channels = params['in_channels']
            out_channels = params['out_channels']
            kernel_size = params['kernel_size']
            
            # å·ç§¯è®¡ç®—å¯†åº¦
            conv_density = (in_channels * out_channels * kernel_size * kernel_size) / 1000000
            
            # ç©ºé—´å±€éƒ¨æ€§
            spatial_locality = 1.0 / (kernel_size * kernel_size)
            
            return {
                'conv_density': conv_density,
                'spatial_locality': spatial_locality,
                'channel_parallelism': min(1.0, out_channels / 256.0),
                'weight_reuse_factor': kernel_size * kernel_size
            }
            
        else:
            # é€šç”¨æŒ‡æ ‡
            return {
                'general_friendliness': analysis_result.yica_friendliness,
                'compute_bound_ratio': 0.7,
                'memory_bound_ratio': 0.3
            }
    
    def run_scaling_experiments(self) -> List[Dict[str, Any]]:
        """è¿è¡Œç¼©æ”¾å®éªŒ"""
        print("\nğŸ“ˆ YICAç¼©æ”¾æ€§å®éªŒ")
        print("=" * 60)
        
        results = []
        
        # 1. Attentionç¼©æ”¾å®éªŒ
        print("\nğŸ” Attentionç¼©æ”¾å®éªŒ")
        attention_configs = [
            (1, 512, 2048, 16),    # å°è§„æ¨¡
            (4, 1024, 4096, 32),   # ä¸­ç­‰è§„æ¨¡
            (8, 2048, 4096, 32),   # å¤§è§„æ¨¡
            (16, 4096, 8192, 64),  # è¶…å¤§è§„æ¨¡
        ]
        
        for batch_size, seq_len, hidden_size, num_heads in attention_configs:
            graph = self.create_llama_attention_graph(batch_size, seq_len, hidden_size, num_heads)
            result = self.run_experiment(graph)
            results.append(result)
        
        # 2. MLPç¼©æ”¾å®éªŒ
        print("\nğŸ§  MLPç¼©æ”¾å®éªŒ")
        mlp_configs = [
            (8, 2048, 8192),      # å°è§„æ¨¡
            (8, 4096, 14336),     # LLaMAè§„æ¨¡
            (8, 8192, 28672),     # å¤§è§„æ¨¡
            (8, 16384, 57344),    # è¶…å¤§è§„æ¨¡
        ]
        
        for batch_size, hidden_size, intermediate_size in mlp_configs:
            graph = self.create_gated_mlp_graph(batch_size, hidden_size, intermediate_size)
            result = self.run_experiment(graph)
            results.append(result)
        
        # 3. å®Œæ•´Transformer Blockå®éªŒ
        print("\nğŸ—ï¸  Transformer Blockå®éªŒ")
        transformer_configs = [
            (1, 512, 2048, 16, 8192),     # å°æ¨¡å‹
            (4, 1024, 4096, 32, 14336),   # ä¸­ç­‰æ¨¡å‹
            (8, 2048, 4096, 32, 14336),   # LLaMA-7B
            (8, 2048, 8192, 64, 28672),   # LLaMA-70Bé£æ ¼
        ]
        
        for batch_size, seq_len, hidden_size, num_heads, intermediate_size in transformer_configs:
            graph = self.create_transformer_block_graph(batch_size, seq_len, hidden_size, num_heads, intermediate_size)
            result = self.run_experiment(graph)
            results.append(result)
        
        return results
    
    def run_specialty_experiments(self) -> List[Dict[str, Any]]:
        """è¿è¡Œä¸“é¡¹å®éªŒ"""
        print("\nğŸ¯ YICAä¸“é¡¹ä¼˜åŒ–å®éªŒ")
        print("=" * 60)
        
        results = []
        
        # 1. ä¸åŒæ•°æ®ç±»å‹å®éªŒ
        print("\nğŸ“Š æ•°æ®ç±»å‹ä¼˜åŒ–å®éªŒ")
        data_type_experiments = [
            ("FP16_Attention", self.create_llama_attention_graph(8, 1024, 4096, 32)),
            ("FP16_MLP", self.create_gated_mlp_graph(8, 4096, 14336)),
            ("Mixed_Precision", self.create_transformer_block_graph(8, 1024, 4096, 32, 14336))
        ]
        
        for exp_name, graph in data_type_experiments:
            graph['name'] = f"{exp_name}_{graph['name']}"
            result = self.run_experiment(graph)
            results.append(result)
        
        # 2. å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
        print("\nğŸ’¾ å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–")
        memory_experiments = [
            ("Sequential_Access", self.create_rms_norm_graph(64, 4096)),
            ("Strided_Access", self.create_conv2d_graph(32, 256, 224, 224, 512, 3)),
            ("Random_Access", self.create_llama_attention_graph(8, 2048, 4096, 32))
        ]
        
        for exp_name, graph in memory_experiments:
            graph['name'] = f"{exp_name}_{graph['name']}"
            result = self.run_experiment(graph)
            results.append(result)
        
        # 3. CIMé˜µåˆ—åˆ©ç”¨ç‡ä¼˜åŒ–
        print("\nğŸ”§ CIMé˜µåˆ—åˆ©ç”¨ç‡ä¼˜åŒ–")
        cim_experiments = []
        
        # ä¸åŒCIMé…ç½®
        original_config = self.yica_config.num_cim_arrays
        for num_arrays in [2, 4, 8, 16]:
            self.yica_config.num_cim_arrays = num_arrays
            self.optimizer = YICAMirageOptimizer(self.yica_config)
            
            graph = self.create_gated_mlp_graph(8, 4096, 14336)
            graph['name'] = f"CIM{num_arrays}_{graph['name']}"
            result = self.run_experiment(graph)
            results.append(result)
        
        # æ¢å¤åŸå§‹é…ç½®
        self.yica_config.num_cim_arrays = original_config
        self.optimizer = YICAMirageOptimizer(self.yica_config)
        
        return results
    
    def generate_yica_kernels(self, experiment_results: List[Dict[str, Any]]) -> Dict[str, str]:
        """ä¸ºæœ€ä½³å®éªŒç»“æœç”ŸæˆYICAä¼˜åŒ–å†…æ ¸"""
        print("\nğŸ”§ ç”ŸæˆYICAä¼˜åŒ–å†…æ ¸")
        print("=" * 50)
        
        # é€‰æ‹©æœ€ä½³ç»“æœ
        best_results = sorted(experiment_results, key=lambda x: x['speedup_ratio'], reverse=True)[:5]
        
        generated_kernels = {}
        
        for i, result in enumerate(best_results):
            kernel_name = f"yica_{result['graph_type']}_{result['graph_name'].lower()}_kernel"
            
            # ç”Ÿæˆä¼˜åŒ–å†…æ ¸ä»£ç 
            kernel_code = self.generate_kernel_code(result)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"{kernel_name}.py"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(kernel_code)
            
            generated_kernels[filename] = kernel_code
            print(f"   ğŸ“„ ç”Ÿæˆå†…æ ¸: {filename} ({len(kernel_code)} chars)")
        
        return generated_kernels
    
    def generate_kernel_code(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç‰¹å®šç±»å‹çš„å†…æ ¸ä»£ç """
        graph_type = result['graph_type']
        graph_name = result['graph_name']
        speedup = result['speedup_ratio']
        cim_util = result['cim_utilization']
        
        # åŸºç¡€æ¨¡æ¿
        base_template = f"""# YICAä¼˜åŒ–çš„{graph_name}å†…æ ¸
# ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
# å›¾ç±»å‹: {graph_type}
# æ€§èƒ½æå‡: {speedup:.1f}x
# CIMåˆ©ç”¨ç‡: {cim_util:.1f}%

import triton
import triton.language as tl

@triton.jit
def yica_optimized_{graph_type}_kernel(
    # è¾“å…¥æŒ‡é’ˆ
    input_ptr,
    output_ptr,
    # å½¢çŠ¶å‚æ•°
    M, N, K,
    # æ­¥é•¿å‚æ•°
    stride_input_m, stride_input_n,
    stride_output_m, stride_output_n,
    # YICAç‰¹å®šå‚æ•°
    CIM_ARRAYS: tl.constexpr,
    SPM_SIZE: tl.constexpr,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    \"\"\"
    YICAä¼˜åŒ–çš„{graph_type}å†…æ ¸
    
    ç‰¹æ€§:
    - åˆ©ç”¨{self.yica_config.num_cim_arrays}ä¸ªCIMé˜µåˆ—å¹¶è¡Œè®¡ç®—
    - SPMå†…å­˜å±‚æ¬¡ä¼˜åŒ–: {self.yica_config.spm_size_kb}KB
    - æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œæ•°æ®é‡ç”¨
    \"\"\"
    
    # è·å–ç¨‹åºIDå’Œç½‘æ ¼é…ç½®
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # YICA CIMé˜µåˆ—åˆ†é…ç­–ç•¥
    cim_id = pid_m % CIM_ARRAYS
    local_pid_m = pid_m // CIM_ARRAYS
    
    # è®¡ç®—æ•°æ®å—åç§»
    offs_m = local_pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    # è¾¹ç•Œæ£€æŸ¥
    mask_m = offs_m < M
    mask_n = offs_n < N
    
"""
        
        # æ ¹æ®å›¾ç±»å‹æ·»åŠ ç‰¹å®šä¼˜åŒ–
        if graph_type == 'attention':
            specific_code = """
    # Attentionç‰¹å®šä¼˜åŒ–
    # 1. Qã€Kã€VçŸ©é˜µçš„åˆ†å—åŠ è½½
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # 2. CIMé˜µåˆ—å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    # ä½¿ç”¨å­˜ç®—ä¸€ä½“ç‰¹æ€§å‡å°‘æ•°æ®ç§»åŠ¨
    q_ptrs = input_ptr + offs_m[:, None] * stride_input_m + offs_k[None, :] * stride_input_n
    k_ptrs = input_ptr + offs_k[:, None] * stride_input_m + offs_n[None, :] * stride_input_n
    
    # SPMä¼˜åŒ–çš„æ•°æ®é¢„å–
    q_block = tl.load(q_ptrs, mask=mask_m[:, None] & (offs_k[None, :] < K))
    k_block = tl.load(k_ptrs, mask=(offs_k[:, None] < K) & mask_n[None, :])
    
    # CIMé˜µåˆ—å¹¶è¡ŒçŸ©é˜µä¹˜æ³•
    attention_scores = tl.dot(q_block, k_block)
    
    # 3. Softmaxä¼˜åŒ–ï¼ˆåˆ©ç”¨CIMçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼‰
    max_scores = tl.max(attention_scores, axis=1)
    attention_scores = attention_scores - max_scores[:, None]
    exp_scores = tl.exp(attention_scores)
    sum_exp = tl.sum(exp_scores, axis=1)
    attention_weights = exp_scores / sum_exp[:, None]
    
    # 4. è¾“å‡ºè®¡ç®—
    output_ptrs = output_ptr + offs_m[:, None] * stride_output_m + offs_n[None, :] * stride_output_n
    tl.store(output_ptrs, attention_weights, mask=mask_m[:, None] & mask_n[None, :])
"""
        elif graph_type == 'mlp':
            specific_code = """
    # MLPç‰¹å®šä¼˜åŒ–
    # 1. çŸ©é˜µä¹˜æ³•çš„CIMé˜µåˆ—åˆ†é…
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # 2. Gated MLPçš„å¹¶è¡Œè®¡ç®—
    # Gateåˆ†æ”¯
    gate_ptrs = input_ptr + offs_m[:, None] * stride_input_m + offs_k[None, :] * stride_input_n
    gate_data = tl.load(gate_ptrs, mask=mask_m[:, None] & (offs_k[None, :] < K))
    
    # Upåˆ†æ”¯
    up_ptrs = input_ptr + offs_m[:, None] * stride_input_m + offs_k[None, :] * stride_input_n
    up_data = tl.load(up_ptrs, mask=mask_m[:, None] & (offs_k[None, :] < K))
    
    # 3. SiLUæ¿€æ´»å‡½æ•°ä¼˜åŒ–ï¼ˆå­˜ç®—ä¸€ä½“è®¡ç®—ï¼‰
    # SiLU(x) = x * sigmoid(x)
    sigmoid_gate = 1.0 / (1.0 + tl.exp(-gate_data))
    activated_gate = gate_data * sigmoid_gate
    
    # 4. Gatedæ“ä½œ
    gated_output = activated_gate * up_data
    
    # 5. SPMä¼˜åŒ–çš„ç»“æœå­˜å‚¨
    output_ptrs = output_ptr + offs_m[:, None] * stride_output_m + offs_n[None, :] * stride_output_n
    tl.store(output_ptrs, gated_output, mask=mask_m[:, None] & mask_n[None, :])
"""
        elif graph_type == 'convolution':
            specific_code = """
    # å·ç§¯ç‰¹å®šä¼˜åŒ–
    # 1. å·ç§¯çª—å£çš„CIMé˜µåˆ—æ˜ å°„
    kernel_size = 3  # ç¤ºä¾‹
    
    # 2. ç©ºé—´å±€éƒ¨æ€§ä¼˜åŒ–
    # åˆ©ç”¨CIMé˜µåˆ—çš„å¹¶è¡Œæ€§å¤„ç†å¤šä¸ªå·ç§¯çª—å£
    for kh in range(kernel_size):
        for kw in range(kernel_size):
            # è¾“å…¥æ•°æ®åç§»
            input_offset = (offs_m + kh) * stride_input_m + (offs_n + kw) * stride_input_n
            input_ptrs = input_ptr + input_offset
            
            # æƒé‡åç§»
            weight_offset = kh * kernel_size + kw
            
            # CIMå¹¶è¡Œè®¡ç®—
            input_data = tl.load(input_ptrs, mask=mask_m & mask_n)
            # å·ç§¯è®¡ç®—é€»è¾‘...
    
    # 3. ç»“æœç´¯ç§¯å’Œå­˜å‚¨
    output_ptrs = output_ptr + offs_m[:, None] * stride_output_m + offs_n[None, :] * stride_output_n
    # tl.store(output_ptrs, result, mask=mask_m[:, None] & mask_n[None, :])
"""
        else:
            specific_code = """
    # é€šç”¨YICAä¼˜åŒ–
    # 1. æ•°æ®åŠ è½½ä¼˜åŒ–
    input_ptrs = input_ptr + offs_m[:, None] * stride_input_m + offs_n[None, :] * stride_input_n
    input_data = tl.load(input_ptrs, mask=mask_m[:, None] & mask_n[None, :])
    
    # 2. CIMé˜µåˆ—å¹¶è¡Œè®¡ç®—
    # æ ¹æ®å…·ä½“æ“ä½œç±»å‹è¿›è¡Œä¼˜åŒ–...
    result = input_data  # å ä½ç¬¦
    
    # 3. ç»“æœå­˜å‚¨
    output_ptrs = output_ptr + offs_m[:, None] * stride_output_m + offs_n[None, :] * stride_output_n
    tl.store(output_ptrs, result, mask=mask_m[:, None] & mask_n[None, :])
"""
        
        # ç»“å°¾æ¨¡æ¿
        end_template = f"""

# YICAè¿è¡Œæ—¶æ”¯æŒå‡½æ•°
def launch_yica_{graph_type}_kernel(input_tensor, output_tensor, **kwargs):
    \"\"\"å¯åŠ¨YICAä¼˜åŒ–å†…æ ¸\"\"\"
    M, N = input_tensor.shape[:2]
    K = input_tensor.shape[-1] if len(input_tensor.shape) > 2 else N
    
    # ç½‘æ ¼é…ç½® - æ ¹æ®CIMé˜µåˆ—æ•°é‡ä¼˜åŒ–
    grid = (
        triton.cdiv(M, 32) * {self.yica_config.num_cim_arrays},  # åˆ©ç”¨å¤šä¸ªCIMé˜µåˆ—
        triton.cdiv(N, 32),
    )
    
    # å¯åŠ¨å†…æ ¸
    yica_optimized_{graph_type}_kernel[grid](
        input_tensor, output_tensor,
        M, N, K,
        input_tensor.stride(0), input_tensor.stride(1),
        output_tensor.stride(0), output_tensor.stride(1),
        CIM_ARRAYS={self.yica_config.num_cim_arrays},
        SPM_SIZE={self.yica_config.spm_size_kb},
        BLOCK_SIZE_M=32,
        BLOCK_SIZE_N=32,
        BLOCK_SIZE_K=32,
    )
    
    return output_tensor

# æ€§èƒ½åŸºå‡†
def benchmark_yica_{graph_type}():
    \"\"\"YICA {graph_type}æ€§èƒ½åŸºå‡†æµ‹è¯•\"\"\"
    import torch
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = 'cuda'
    M, N, K = 1024, 1024, 1024
    input_tensor = torch.randn(M, K, dtype=torch.float16, device=device)
    output_tensor = torch.empty(M, N, dtype=torch.float16, device=device)
    
    # é¢„çƒ­
    for _ in range(10):
        launch_yica_{graph_type}_kernel(input_tensor, output_tensor)
    
    # åŸºå‡†æµ‹è¯•
    torch.cuda.synchronize()
    start_time = torch.cuda.Event(enable_timing=True)
    end_time = torch.cuda.Event(enable_timing=True)
    
    start_time.record()
    for _ in range(100):
        launch_yica_{graph_type}_kernel(input_tensor, output_tensor)
    end_time.record()
    
    torch.cuda.synchronize()
    avg_time = start_time.elapsed_time(end_time) / 100
    
    print(f"YICA {graph_type}å¹³å‡æ‰§è¡Œæ—¶é—´: {{avg_time:.2f}}ms")
    print(f"é¢„æœŸåŠ é€Ÿæ¯”: {speedup:.1f}x")
    print(f"CIMåˆ©ç”¨ç‡: {cim_util:.1f}%")
    
    return avg_time

if __name__ == "__main__":
    benchmark_yica_{graph_type}()
"""
        
        return base_template + specific_code + end_template

def main():
    """ä¸»å®éªŒæµç¨‹"""
    print("ğŸ§ª YICAç¥ç»ç½‘ç»œæ¨¡å‹ä¼˜åŒ–å®éªŒå¥—ä»¶")
    print("åŸºäºMirageç”¨ä¾‹çš„å¤§è§„æ¨¡ç¥ç»ç½‘ç»œä¼˜åŒ–å®éªŒ")
    print("=" * 80)
    
    # åˆå§‹åŒ–å®éªŒç¯å¢ƒ
    yica_config = YICAConfig()
    yica_config.num_cim_arrays = 4
    yica_config.spm_size_kb = 512
    yica_config.memory_bandwidth_gbps = 1000.0
    
    experiments = YICANeuralNetworkExperiments(yica_config)
    
    print(f"ğŸ“‹ YICAé…ç½®:")
    print(f"   - CIMé˜µåˆ—æ•°é‡: {yica_config.num_cim_arrays}")
    print(f"   - SPMå¤§å°: {yica_config.spm_size_kb}KB")
    print(f"   - å†…å­˜å¸¦å®½: {yica_config.memory_bandwidth_gbps}GB/s")
    
    # è¿è¡Œå®éªŒ
    all_results = []
    
    # 1. ç¼©æ”¾æ€§å®éªŒ
    scaling_results = experiments.run_scaling_experiments()
    all_results.extend(scaling_results)
    
    # 2. ä¸“é¡¹ä¼˜åŒ–å®éªŒ
    specialty_results = experiments.run_specialty_experiments()
    all_results.extend(specialty_results)
    
    # 3. ç”Ÿæˆä¼˜åŒ–å†…æ ¸
    generated_kernels = experiments.generate_yica_kernels(all_results)
    
    # 4. åˆ†æå’ŒæŠ¥å‘Š
    print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
    print("=" * 60)
    
    # ç»Ÿè®¡åˆ†æ
    total_experiments = len(all_results)
    avg_speedup = sum(r['speedup_ratio'] for r in all_results) / total_experiments
    avg_cim_util = sum(r['cim_utilization'] for r in all_results) / total_experiments
    best_result = max(all_results, key=lambda x: x['speedup_ratio'])
    
    print(f"ğŸ“ˆ å®éªŒç»Ÿè®¡:")
    print(f"   - æ€»å®éªŒæ•°: {total_experiments}")
    print(f"   - å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.1f}x")
    print(f"   - å¹³å‡CIMåˆ©ç”¨ç‡: {avg_cim_util:.1f}%")
    print(f"   - æœ€ä½³æ€§èƒ½: {best_result['graph_name']} ({best_result['speedup_ratio']:.1f}x)")
    print(f"   - ç”Ÿæˆå†…æ ¸æ•°: {len(generated_kernels)}")
    
    # æŒ‰ç±»å‹åˆ†æ
    type_stats = {}
    for result in all_results:
        graph_type = result['graph_type']
        if graph_type not in type_stats:
            type_stats[graph_type] = {'count': 0, 'speedup_sum': 0, 'cim_util_sum': 0}
        type_stats[graph_type]['count'] += 1
        type_stats[graph_type]['speedup_sum'] += result['speedup_ratio']
        type_stats[graph_type]['cim_util_sum'] += result['cim_utilization']
    
    print(f"\nğŸ“‹ æŒ‰ç±»å‹ç»Ÿè®¡:")
    print("-" * 60)
    print(f"{'ç±»å‹':<20} {'å®éªŒæ•°':<8} {'å¹³å‡åŠ é€Ÿ':<12} {'å¹³å‡CIMåˆ©ç”¨ç‡':<15}")
    print("-" * 60)
    for graph_type, stats in type_stats.items():
        avg_speedup = stats['speedup_sum'] / stats['count']
        avg_cim = stats['cim_util_sum'] / stats['count']
        print(f"{graph_type:<20} {stats['count']:<8} {avg_speedup:<12.1f}x {avg_cim:<15.1f}%")
    print("-" * 60)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('yica_neural_network_experiments_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'yica_config': {
                'num_cim_arrays': yica_config.num_cim_arrays,
                'spm_size_kb': yica_config.spm_size_kb,
                'memory_bandwidth_gbps': yica_config.memory_bandwidth_gbps
            },
            'experiment_summary': {
                'total_experiments': total_experiments,
                'avg_speedup': avg_speedup,
                'avg_cim_utilization': avg_cim_util,
                'best_result': best_result,
                'type_statistics': type_stats
            },
            'detailed_results': all_results,
            'generated_kernels': list(generated_kernels.keys())
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: yica_neural_network_experiments_results.json")
    print(f"ğŸ”§ ç”Ÿæˆçš„å†…æ ¸æ–‡ä»¶: {', '.join(generated_kernels.keys())}")
    
    # å®éªŒç»“è®º
    print(f"\nğŸ¯ å®éªŒç»“è®º:")
    if avg_speedup >= 5.0:
        print(f"   ğŸ‰ YICAä¼˜åŒ–æ•ˆæœä¼˜ç§€ï¼å¹³å‡{avg_speedup:.1f}xåŠ é€Ÿ")
    elif avg_speedup >= 3.0:
        print(f"   âœ… YICAä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼å¹³å‡{avg_speedup:.1f}xåŠ é€Ÿ")
    elif avg_speedup >= 2.0:
        print(f"   âš ï¸  YICAä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼Œå¹³å‡{avg_speedup:.1f}xåŠ é€Ÿ")
    else:
        print(f"   âŒ YICAä¼˜åŒ–æ•ˆæœéœ€è¦æ”¹è¿›ï¼Œå¹³å‡{avg_speedup:.1f}xåŠ é€Ÿ")
    
    print(f"\nğŸ“š ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   1. é’ˆå¯¹è¡¨ç°æœ€ä½³çš„{best_result['graph_type']}ç±»å‹è¿›è¡Œæ·±åº¦ä¼˜åŒ–")
    print(f"   2. ä¼˜åŒ–CIMåˆ©ç”¨ç‡è¾ƒä½çš„æ“ä½œç±»å‹")
    print(f"   3. åœ¨çœŸå®YICAç¡¬ä»¶ä¸ŠéªŒè¯ç”Ÿæˆçš„å†…æ ¸æ€§èƒ½")
    print(f"   4. æ‰©å±•æ›´å¤šç¥ç»ç½‘ç»œæ¨¡å‹çš„æ”¯æŒ")
    
    return len(all_results)

if __name__ == "__main__":
    num_experiments = main()
    print(f"\nğŸ‰ YICAç¥ç»ç½‘ç»œä¼˜åŒ–å®éªŒå®Œæˆï¼æ€»è®¡{num_experiments}ä¸ªå®éªŒ") 