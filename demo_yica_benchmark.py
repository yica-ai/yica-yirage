#!/usr/bin/env python3
"""
YICA-Mirage åŸºå‡†æµ‹è¯•æ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ YICA-Mirage åŸºå‡†æµ‹è¯•å¥—ä»¶è¿›è¡Œæ€§èƒ½åˆ†æã€‚
åŒ…å«ï¼š
- åŸºç¡€åŸºå‡†æµ‹è¯•è¿è¡Œ
- è‡ªå®šä¹‰é…ç½®ä½¿ç”¨
- ç»“æœåˆ†æå’Œå¯è§†åŒ–
- æ€§èƒ½å¯¹æ¯”æŠ¥å‘Šç”Ÿæˆ
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root / "mirage"))

try:
    from mirage.benchmark.yica_benchmark_suite import (
        YICABenchmarkSuite, 
        BenchmarkConfig,
        BasicOperationBenchmark,
        TransformerBenchmark
    )
    BENCHMARK_AVAILABLE = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥åŸºå‡†æµ‹è¯•æ¨¡å—: {e}")
    BENCHMARK_AVAILABLE = False

try:
    from mirage.yica_pytorch_backend import initialize as yica_initialize
    YICA_AVAILABLE = True
except ImportError:
    print("Warning: YICA åç«¯ä¸å¯ç”¨")
    YICA_AVAILABLE = False


def load_config_from_file(config_file: str, config_name: str = "benchmark_config") -> Dict[str, Any]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            configs = json.load(f)
        
        if config_name not in configs:
            available_configs = list(configs.keys())
            print(f"Error: é…ç½® '{config_name}' ä¸å­˜åœ¨")
            print(f"å¯ç”¨é…ç½®: {available_configs}")
            return {}
        
        return configs[config_name]
    except Exception as e:
        print(f"Error: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_file}: {e}")
        return {}


def run_basic_demo():
    """è¿è¡ŒåŸºç¡€æ¼”ç¤º"""
    print("ğŸš€ å¼€å§‹åŸºç¡€åŸºå‡†æµ‹è¯•æ¼”ç¤º...")
    
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # åˆ›å»ºå¿«é€Ÿæµ‹è¯•é…ç½®
    config = BenchmarkConfig(
        warmup_iterations=3,
        benchmark_iterations=10,
        batch_sizes=[1, 4, 8],
        sequence_lengths=[128, 512],
        hidden_sizes=[768, 1024],
        enable_memory_profiling=True,
        enable_energy_profiling=False,
        output_dir="./demo_benchmark_results",
        device="auto"
    )
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = YICABenchmarkSuite(config)
    
    print("ğŸ“Š è¿è¡ŒåŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•...")
    basic_results = benchmark_suite.run_basic_operation_benchmarks()
    
    print(f"âœ… å®Œæˆ {len(basic_results)} ä¸ªåŸºç¡€æ“ä½œæµ‹è¯•")
    
    # ä¿å­˜ç»“æœ
    results_file = benchmark_suite.save_results()
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
    
    return benchmark_suite


def run_transformer_demo():
    """è¿è¡Œ Transformer æ¨¡å‹æ¼”ç¤º"""
    print("ğŸ” å¼€å§‹ Transformer åŸºå‡†æµ‹è¯•æ¼”ç¤º...")
    
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # åˆ›å»ºé’ˆå¯¹ Transformer çš„é…ç½®
    config = BenchmarkConfig(
        warmup_iterations=5,
        benchmark_iterations=20,
        batch_sizes=[1, 8, 16],
        sequence_lengths=[256, 512, 1024],
        hidden_sizes=[768, 1024],
        enable_memory_profiling=True,
        output_dir="./demo_transformer_results",
        device="auto"
    )
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = YICABenchmarkSuite(config)
    
    print("ğŸ§  è¿è¡Œ Transformer åŸºå‡†æµ‹è¯•...")
    transformer_results = benchmark_suite.run_transformer_benchmarks()
    
    print(f"âœ… å®Œæˆ {len(transformer_results)} ä¸ª Transformer æµ‹è¯•")
    
    # ä¿å­˜ç»“æœå’Œç”Ÿæˆå¯è§†åŒ–
    benchmark_suite.save_results()
    chart_file = benchmark_suite.generate_visualization()
    report_file = benchmark_suite.generate_report()
    
    print(f"ğŸ“ˆ å›¾è¡¨å·²ç”Ÿæˆ: {chart_file}")
    print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    return benchmark_suite


def run_config_file_demo():
    """è¿è¡Œé…ç½®æ–‡ä»¶æ¼”ç¤º"""
    print("âš™ï¸ å¼€å§‹é…ç½®æ–‡ä»¶åŸºå‡†æµ‹è¯•æ¼”ç¤º...")
    
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    config_file = project_root / "mirage/benchmark/configs/yica_benchmark_config.json"
    
    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    # åŠ è½½å¿«é€Ÿé…ç½®
    config_data = load_config_from_file(str(config_file), "quick_config")
    if not config_data:
        return
    
    # è½¬æ¢ä¸º BenchmarkConfig
    config = BenchmarkConfig(
        warmup_iterations=config_data.get("warmup_iterations", 3),
        benchmark_iterations=config_data.get("benchmark_iterations", 10),
        batch_sizes=config_data.get("batch_sizes", [1, 8]),
        sequence_lengths=config_data.get("sequence_lengths", [128, 512]),
        hidden_sizes=config_data.get("hidden_sizes", [768, 1024]),
        enable_memory_profiling=config_data.get("enable_memory_profiling", True),
        enable_energy_profiling=config_data.get("enable_energy_profiling", False),
        output_dir=config_data.get("output_dir", "./config_demo_results"),
        device=config_data.get("device", "auto"),
        precision=config_data.get("precision", "fp32")
    )
    
    print(f"ğŸ“ ä½¿ç”¨é…ç½®: {config_data.get('description', 'Unknown')}")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_suite = YICABenchmarkSuite(config)
    benchmark_suite.run_all_benchmarks()
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    benchmark_suite.save_results()
    benchmark_suite.generate_visualization()
    benchmark_suite.generate_report()
    
    print("âœ… é…ç½®æ–‡ä»¶æ¼”ç¤ºå®Œæˆ")
    
    return benchmark_suite


def run_yica_optimization_demo():
    """è¿è¡Œ YICA ä¼˜åŒ–æ¼”ç¤º"""
    print("ğŸ¯ å¼€å§‹ YICA ä¼˜åŒ–åŸºå‡†æµ‹è¯•æ¼”ç¤º...")
    
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    if not YICA_AVAILABLE:
        print("âš ï¸ YICA åç«¯ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
    # åˆ›å»º YICA ä¼˜åŒ–é…ç½®
    config = BenchmarkConfig(
        warmup_iterations=5,
        benchmark_iterations=25,
        batch_sizes=[1, 8, 16, 32],
        sequence_lengths=[512, 1024],
        hidden_sizes=[1024, 2048],
        enable_memory_profiling=True,
        enable_energy_profiling=True,
        output_dir="./demo_yica_optimization_results",
        device="yica" if YICA_AVAILABLE else "auto"
    )
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = YICABenchmarkSuite(config)
    
    print("ğŸ”§ è¿è¡Œ YICA ä¼˜åŒ–åŸºå‡†æµ‹è¯•...")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    all_results = benchmark_suite.run_all_benchmarks()
    
    print(f"âœ… å®Œæˆ {len(all_results)} ä¸ªä¼˜åŒ–æµ‹è¯•")
    
    # ç”Ÿæˆå®Œæ•´åˆ†æ
    results_file = benchmark_suite.save_results()
    chart_file = benchmark_suite.generate_visualization()
    report_file = benchmark_suite.generate_report()
    
    print(f"ğŸ“Š å®Œæ•´åˆ†æå·²ç”Ÿæˆ:")
    print(f"  - æ•°æ®: {results_file}")
    print(f"  - å›¾è¡¨: {chart_file}")
    print(f"  - æŠ¥å‘Š: {report_file}")
    
    return benchmark_suite


def run_performance_comparison_demo():
    """è¿è¡Œæ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("ğŸ† å¼€å§‹æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•æ¼”ç¤º...")
    
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # å¯¹æ¯”é…ç½®ï¼šCPU vs CUDA vs YICA
    devices = ["cpu"]
    if YICA_AVAILABLE:
        devices.append("yica")
    
    # å°è¯•æ£€æµ‹ CUDA
    try:
        import torch
        if torch.cuda.is_available():
            devices.append("cuda")
    except ImportError:
        pass
    
    print(f"ğŸ” å°†å¯¹æ¯”ä»¥ä¸‹è®¾å¤‡: {devices}")
    
    comparison_results = {}
    
    for device in devices:
        print(f"\nğŸ“± æµ‹è¯•è®¾å¤‡: {device}")
        
        config = BenchmarkConfig(
            warmup_iterations=3,
            benchmark_iterations=15,
            batch_sizes=[1, 8, 16],
            sequence_lengths=[256, 512],
            hidden_sizes=[768, 1024],
            enable_memory_profiling=True,
            output_dir=f"./demo_comparison_{device}_results",
            device=device
        )
        
        benchmark_suite = YICABenchmarkSuite(config)
        
        # åªè¿è¡ŒåŸºç¡€æ“ä½œï¼ˆä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼‰
        basic_results = benchmark_suite.run_basic_operation_benchmarks()
        
        # ä¿å­˜è®¾å¤‡ç‰¹å®šç»“æœ
        results_file = benchmark_suite.save_results()
        comparison_results[device] = {
            "results": basic_results,
            "results_file": results_file,
            "device": device
        }
        
        print(f"âœ… {device} æµ‹è¯•å®Œæˆ: {len(basic_results)} ä¸ªç»“æœ")
    
    # ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print("\nğŸ“ˆ ç”Ÿæˆæ€§èƒ½å¯¹æ¯”åˆ†æ...")
    generate_comparison_analysis(comparison_results)
    
    return comparison_results


def generate_comparison_analysis(comparison_results: Dict[str, Any]):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”åˆ†æ"""
    comparison_file = Path("./demo_performance_comparison.md")
    
    with open(comparison_file, 'w', encoding='utf-8') as f:
        f.write("# YICA-Mirage æ€§èƒ½å¯¹æ¯”åˆ†æ\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {Path(__file__).stat().st_mtime}\n\n")
        
        f.write("## æµ‹è¯•è®¾å¤‡\n\n")
        for device, data in comparison_results.items():
            f.write(f"- **{device.upper()}**: {len(data['results'])} ä¸ªæµ‹è¯•ç»“æœ\n")
        f.write("\n")
        
        f.write("## æ€§èƒ½æ‘˜è¦\n\n")
        f.write("| è®¾å¤‡ | å¹³å‡å»¶è¿Ÿ (ms) | å¹³å‡ååé‡ (ops/sec) | æµ‹è¯•æ•°é‡ |\n")
        f.write("|------|---------------|---------------------|----------|\n")
        
        for device, data in comparison_results.items():
            results = data['results']
            if results:
                avg_latency = sum(r.mean_latency_ms for r in results) / len(results)
                avg_throughput = sum(r.throughput_ops_per_sec for r in results) / len(results)
                f.write(f"| {device.upper()} | {avg_latency:.3f} | {avg_throughput:.2f} | {len(results)} |\n")
        
        f.write("\n## è¯¦ç»†ç»“æœ\n\n")
        for device, data in comparison_results.items():
            f.write(f"### {device.upper()} è¯¦ç»†ç»“æœ\n\n")
            f.write(f"ç»“æœæ–‡ä»¶: `{data['results_file']}`\n\n")
        
        f.write("## ç»“è®º\n\n")
        if YICA_AVAILABLE:
            f.write("- âœ… YICA ä¼˜åŒ–å·²å¯ç”¨\n")
            f.write("- ğŸš€ YICA è®¾å¤‡åœ¨æ”¯æŒçš„æ“ä½œä¸Šæ˜¾ç¤ºå‡ºæ€§èƒ½ä¼˜åŠ¿\n")
        else:
            f.write("- âš ï¸ YICA åç«¯æœªå¯ç”¨ï¼Œç»“æœä»…ä½œä¸ºåŸºå‡†å¯¹ç…§\n")
        
        f.write("- ğŸ“Š è¯¦ç»†æ€§èƒ½æ•°æ®è¯·å‚è€ƒå„è®¾å¤‡çš„ç»“æœæ–‡ä»¶\n")
        f.write("- ğŸ” å»ºè®®æ ¹æ®å…·ä½“å·¥ä½œè´Ÿè½½é€‰æ‹©æœ€ä¼˜è®¾å¤‡é…ç½®\n\n")
        
        f.write("---\n")
        f.write("*æ­¤æŠ¥å‘Šç”± YICA-Mirage åŸºå‡†æµ‹è¯•æ¼”ç¤ºè‡ªåŠ¨ç”Ÿæˆ*\n")
    
    print(f"ğŸ“‹ å¯¹æ¯”åˆ†æå·²ä¿å­˜: {comparison_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="YICA-Mirage åŸºå‡†æµ‹è¯•æ¼”ç¤º")
    parser.add_argument("--demo", type=str, 
                       choices=["basic", "transformer", "config", "yica", "comparison", "all"],
                       default="basic", help="é€‰æ‹©æ¼”ç¤ºç±»å‹")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    print("ğŸ¯ YICA-Mirage åŸºå‡†æµ‹è¯•æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not BENCHMARK_AVAILABLE:
        print("âŒ åŸºå‡†æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        print("ğŸ’¡ ç¡®ä¿å·²æ­£ç¡®å®‰è£… YICA-Mirage å’Œæ‰€æœ‰ä¾èµ–")
        return
    
    if YICA_AVAILABLE:
        try:
            yica_initialize()
            print("âœ… YICA åç«¯å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸ YICA åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # è¿è¡ŒæŒ‡å®šæ¼”ç¤º
    if args.demo == "basic" or args.demo == "all":
        run_basic_demo()
        print()
    
    if args.demo == "transformer" or args.demo == "all":
        run_transformer_demo()
        print()
    
    if args.demo == "config" or args.demo == "all":
        run_config_file_demo()
        print()
    
    if args.demo == "yica" or args.demo == "all":
        run_yica_optimization_demo()
        print()
    
    if args.demo == "comparison" or args.demo == "all":
        run_performance_comparison_demo()
        print()
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„ç»“æœæ–‡ä»¶ä»¥è·å–è¯¦ç»†åˆ†æ")
    
    # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Š")
    print("2. æ ¹æ®ç»“æœè°ƒæ•´æ¨¡å‹å’Œç¡¬ä»¶é…ç½®")
    print("3. è¿è¡Œæ›´å…¨é¢çš„åŸºå‡†æµ‹è¯•")
    print("4. æ¢ç´¢ YICA ç‰¹å®šçš„ä¼˜åŒ–é€‰é¡¹")


if __name__ == "__main__":
    main() 