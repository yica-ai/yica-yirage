#!/bin/bash
# YICA-Mirage æ€§èƒ½éªŒè¯è„šæœ¬
# å…¨é¢éªŒè¯ç³»ç»Ÿæ€§èƒ½ã€åŠŸèƒ½æ­£ç¡®æ€§å’Œç¨³å®šæ€§

set -e  # å‡ºé”™æ—¶é€€å‡º

# è„šæœ¬é…ç½®
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
VALIDATION_DIR="$PROJECT_ROOT/validation_results"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${PURPLE}[STEP]${NC} $1"
}

log_result() {
    echo -e "${CYAN}[RESULT]${NC} $1"
}

# å¸®åŠ©ä¿¡æ¯
show_help() {
    cat << EOF
YICA-Mirage æ€§èƒ½éªŒè¯è„šæœ¬

ç”¨æ³•:
    $0 [é€‰é¡¹]

é€‰é¡¹:
    -h, --help           æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    -o, --output DIR     æŒ‡å®šè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./validation_resultsï¼‰
    -q, --quick          å¿«é€ŸéªŒè¯æ¨¡å¼
    -f, --full           å®Œæ•´éªŒè¯æ¨¡å¼ï¼ˆåŒ…æ‹¬å‹åŠ›æµ‹è¯•ï¼‰
    -b, --benchmark      ä»…è¿è¡ŒåŸºå‡†æµ‹è¯•
    -c, --correctness    ä»…è¿è¡Œæ­£ç¡®æ€§æµ‹è¯•
    -s, --stability      ä»…è¿è¡Œç¨³å®šæ€§æµ‹è¯•
    -p, --performance    ä»…è¿è¡Œæ€§èƒ½æµ‹è¯•
    --docker             åœ¨ Docker å®¹å™¨ä¸­è¿è¡Œ
    --generate-report    ç”Ÿæˆè¯¦ç»†éªŒè¯æŠ¥å‘Š
    --cleanup            éªŒè¯åæ¸…ç†ä¸´æ—¶æ–‡ä»¶

éªŒè¯ç±»å‹:
    correctness          åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯
    performance          æ€§èƒ½åŸºå‡†æµ‹è¯•
    stability            é•¿æœŸç¨³å®šæ€§æµ‹è¯•
    integration          é›†æˆæµ‹è¯•
    compatibility        å…¼å®¹æ€§æµ‹è¯•

ç¤ºä¾‹:
    $0                              # è¿è¡Œæ ‡å‡†éªŒè¯
    $0 --quick                      # å¿«é€ŸéªŒè¯
    $0 --full --generate-report     # å®Œæ•´éªŒè¯å¹¶ç”ŸæˆæŠ¥å‘Š
    $0 --performance --benchmark    # ä»…æ€§èƒ½å’ŒåŸºå‡†æµ‹è¯•
    $0 --docker --output ./results  # Docker ç¯å¢ƒéªŒè¯

ç¯å¢ƒè¦æ±‚:
    - Python 3.8+
    - PyTorch
    - YICA-Mirage ç³»ç»Ÿ
    - å……è¶³çš„ç£ç›˜ç©ºé—´å’Œå†…å­˜
EOF
}

# åˆå§‹åŒ–éªŒè¯ç¯å¢ƒ
init_validation_environment() {
    log_step "åˆå§‹åŒ–éªŒè¯ç¯å¢ƒ..."
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "$VALIDATION_DIR"
    mkdir -p "$VALIDATION_DIR/logs"
    mkdir -p "$VALIDATION_DIR/reports"
    mkdir -p "$VALIDATION_DIR/benchmarks"
    mkdir -p "$VALIDATION_DIR/correctness"
    mkdir -p "$VALIDATION_DIR/stability"
    
    # åˆ›å»ºéªŒè¯æ—¥å¿—
    VALIDATION_LOG="$VALIDATION_DIR/logs/validation_${TIMESTAMP}.log"
    touch "$VALIDATION_LOG"
    
    log_info "éªŒè¯ç¯å¢ƒå·²åˆå§‹åŒ–"
    log_info "è¾“å‡ºç›®å½•: $VALIDATION_DIR"
    log_info "éªŒè¯æ—¥å¿—: $VALIDATION_LOG"
}

# æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
check_system_environment() {
    log_step "æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ..."
    
    # æ£€æŸ¥ Python
    if ! command -v python3 &> /dev/null; then
        log_error "Python3 æœªå®‰è£…"
        return 1
    fi
    
    local python_version=$(python3 -c "import sys; print('.'.join(map(str, sys.version_info[:2])))")
    log_info "Python ç‰ˆæœ¬: $python_version"
    
    # æ£€æŸ¥ YICA-Mirage
    if python3 -c "import sys; sys.path.append('$PROJECT_ROOT/mirage/python'); import mirage" &> /dev/null; then
        log_success "YICA-Mirage å¯ç”¨"
    else
        log_warning "YICA-Mirage ä¸»æ¨¡å—ä¸å¯ç”¨"
    fi
    
    # æ£€æŸ¥ YICA åç«¯
    if python3 -c "import sys; sys.path.append('$PROJECT_ROOT/mirage/python'); from mirage.yica_pytorch_backend import initialize" &> /dev/null; then
        log_success "YICA PyTorch åç«¯å¯ç”¨"
    else
        log_warning "YICA PyTorch åç«¯ä¸å¯ç”¨"
    fi
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    local available_memory=$(free -m | awk 'NR==2{printf "%.1f", $7/1024}')
    local available_disk=$(df -h "$VALIDATION_DIR" | awk 'NR==2{print $4}')
    
    log_info "å¯ç”¨å†…å­˜: ${available_memory}GB"
    log_info "å¯ç”¨ç£ç›˜: $available_disk"
    
    # æ£€æŸ¥ GPUï¼ˆå¦‚æœæœ‰ï¼‰
    if command -v nvidia-smi &> /dev/null; then
        local gpu_info=$(nvidia-smi --query-gpu=gpu_name,memory.total --format=csv,noheader,nounits | head -1)
        log_success "GPU å¯ç”¨: $gpu_info"
    else
        log_info "æ—  GPU ç¯å¢ƒï¼Œä½¿ç”¨ CPU éªŒè¯"
    fi
}

# åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯
run_correctness_validation() {
    log_step "è¿è¡ŒåŠŸèƒ½æ­£ç¡®æ€§éªŒè¯..."
    
    local correctness_script="$VALIDATION_DIR/correctness/correctness_test.py"
    
    # åˆ›å»ºæ­£ç¡®æ€§æµ‹è¯•è„šæœ¬
    cat > "$correctness_script" << 'EOF'
#!/usr/bin/env python3
"""YICA-Mirage åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯"""

import sys
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root / "mirage/python"))

def test_basic_imports():
    """æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥"""
    results = {"test_name": "basic_imports", "status": "unknown", "details": {}}
    
    try:
        import mirage
        results["details"]["mirage_core"] = "success"
    except ImportError as e:
        results["details"]["mirage_core"] = f"failed: {e}"
    
    try:
        from mirage.yica_pytorch_backend import initialize
        results["details"]["yica_backend"] = "success"
    except ImportError as e:
        results["details"]["yica_backend"] = f"failed: {e}"
    
    try:
        from mirage.yica_auto_tuner import YICAAutoTuner
        results["details"]["auto_tuner"] = "success"
    except ImportError as e:
        results["details"]["auto_tuner"] = f"failed: {e}"
    
    try:
        from mirage.yica_distributed import YICADistributedTrainer
        results["details"]["distributed"] = "success"
    except ImportError as e:
        results["details"]["distributed"] = f"failed: {e}"
    
    try:
        from mirage.yica_performance_monitor import YICAPerformanceMonitor
        results["details"]["monitor"] = "success"
    except ImportError as e:
        results["details"]["monitor"] = f"failed: {e}"
    
    # ç»Ÿè®¡æˆåŠŸç‡
    successes = sum(1 for v in results["details"].values() if v == "success")
    total = len(results["details"])
    
    if successes == total:
        results["status"] = "passed"
    elif successes > total // 2:
        results["status"] = "partial"
    else:
        results["status"] = "failed"
    
    results["success_rate"] = successes / total
    return results

def test_yica_backend_functionality():
    """æµ‹è¯• YICA åç«¯åŠŸèƒ½"""
    results = {"test_name": "yica_backend", "status": "unknown", "details": {}}
    
    try:
        from mirage.yica_pytorch_backend import initialize, get_yica_backend
        
        # æµ‹è¯•åˆå§‹åŒ–
        initialize()
        results["details"]["initialization"] = "success"
        
        # æµ‹è¯•åç«¯è·å–
        backend = get_yica_backend()
        results["details"]["backend_access"] = "success"
        
        # æµ‹è¯•åŸºç¡€é…ç½®
        if hasattr(backend, 'config'):
            results["details"]["configuration"] = "success"
        else:
            results["details"]["configuration"] = "warning: no config"
        
        results["status"] = "passed"
        
    except ImportError:
        results["status"] = "skipped"
        results["details"]["reason"] = "YICA backend not available"
    except Exception as e:
        results["status"] = "failed"
        results["details"]["error"] = str(e)
    
    return results

def test_auto_tuner_functionality():
    """æµ‹è¯•è‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½"""
    results = {"test_name": "auto_tuner", "status": "unknown", "details": {}}
    
    try:
        from mirage.yica_auto_tuner import YICAAutoTuner
        
        # åˆ›å»ºè°ƒä¼˜å™¨
        tuner = YICAAutoTuner()
        results["details"]["creation"] = "success"
        
        # æµ‹è¯•é…ç½®
        workload = {
            'batch_size': 8,
            'sequence_length': 512,
            'hidden_size': 768
        }
        
        # è¿è¡Œå¿«é€Ÿè°ƒä¼˜æµ‹è¯•
        result = tuner.auto_tune(workload, max_evaluations=5)
        
        if 'best_score' in result:
            results["details"]["tuning_execution"] = "success"
            results["details"]["best_score"] = result['best_score']
        else:
            results["details"]["tuning_execution"] = "failed: no best_score"
        
        results["status"] = "passed"
        
    except ImportError:
        results["status"] = "skipped"
        results["details"]["reason"] = "Auto tuner not available"
    except Exception as e:
        results["status"] = "failed"
        results["details"]["error"] = str(e)
    
    return results

def test_performance_monitor_functionality():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½"""
    results = {"test_name": "performance_monitor", "status": "unknown", "details": {}}
    
    try:
        from mirage.yica_performance_monitor import YICAPerformanceMonitor
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = YICAPerformanceMonitor()
        results["details"]["creation"] = "success"
        
        # æµ‹è¯•å¯åŠ¨å’Œåœæ­¢
        monitor.start_monitoring(enable_visualization=False)
        results["details"]["start_monitoring"] = "success"
        
        time.sleep(2)  # è®©ç›‘æ§è¿è¡Œä¸€ä¼šå„¿
        
        status = monitor.get_current_status()
        if status.get("monitoring_active"):
            results["details"]["status_check"] = "success"
        else:
            results["details"]["status_check"] = "warning: not active"
        
        monitor.stop_monitoring()
        results["details"]["stop_monitoring"] = "success"
        
        results["status"] = "passed"
        
    except ImportError:
        results["status"] = "skipped"
        results["details"]["reason"] = "Performance monitor not available"
    except Exception as e:
        results["status"] = "failed"
        results["details"]["error"] = str(e)
    
    return results

def main():
    """è¿è¡Œæ‰€æœ‰æ­£ç¡®æ€§æµ‹è¯•"""
    print("ğŸ” å¼€å§‹ YICA-Mirage åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯...")
    
    test_results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_results.append(test_basic_imports())
    test_results.append(test_yica_backend_functionality())
    test_results.append(test_auto_tuner_functionality())
    test_results.append(test_performance_monitor_functionality())
    
    # æ±‡æ€»ç»“æœ
    summary = {
        "validation_type": "correctness",
        "timestamp": datetime.now().isoformat(),
        "total_tests": len(test_results),
        "passed": sum(1 for r in test_results if r["status"] == "passed"),
        "failed": sum(1 for r in test_results if r["status"] == "failed"),
        "skipped": sum(1 for r in test_results if r["status"] == "skipped"),
        "partial": sum(1 for r in test_results if r["status"] == "partial"),
        "test_results": test_results
    }
    
    # ä¿å­˜ç»“æœ
    output_file = Path(__file__).parent / f"correctness_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"âœ… æ­£ç¡®æ€§éªŒè¯å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {summary['passed']}/{summary['total_tests']} é€šè¿‡")
    
    return summary["passed"] == summary["total_tests"]

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF
    
    # è¿è¡Œæ­£ç¡®æ€§æµ‹è¯•
    log_info "æ‰§è¡ŒåŠŸèƒ½æ­£ç¡®æ€§æµ‹è¯•..."
    if python3 "$correctness_script" >> "$VALIDATION_LOG" 2>&1; then
        log_success "åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯é€šè¿‡"
        return 0
    else
        log_error "åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯å¤±è´¥"
        return 1
    fi
}

# æ€§èƒ½åŸºå‡†æµ‹è¯•
run_performance_benchmarks() {
    log_step "è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•..."
    
    local benchmark_script="$VALIDATION_DIR/benchmarks/performance_benchmark.py"
    
    # åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
    cat > "$benchmark_script" << 'EOF'
#!/usr/bin/env python3
"""YICA-Mirage æ€§èƒ½åŸºå‡†æµ‹è¯•"""

import sys
import json
import time
import psutil
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root / "mirage/python"))

def benchmark_basic_operations():
    """åŸºç¡€æ“ä½œæ€§èƒ½åŸºå‡†"""
    results = {"benchmark_name": "basic_operations", "metrics": {}}
    
    try:
        from mirage.benchmark.yica_benchmark_suite import YICABenchmarkSuite, BenchmarkConfig
        
        # åˆ›å»ºå¿«é€ŸåŸºå‡†é…ç½®
        config = BenchmarkConfig(
            warmup_iterations=3,
            benchmark_iterations=10,
            batch_sizes=[1, 8, 16],
            sequence_lengths=[128, 512],
            hidden_sizes=[768, 1024],
            output_dir="./temp_benchmark_results"
        )
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_suite = YICABenchmarkSuite(config)
        basic_results = benchmark_suite.run_basic_operation_benchmarks()
        
        # æ±‡æ€»æŒ‡æ ‡
        if basic_results:
            latencies = [r.mean_latency_ms for r in basic_results if hasattr(r, 'mean_latency_ms')]
            throughputs = [r.throughput_ops_per_sec for r in basic_results if hasattr(r, 'throughput_ops_per_sec')]
            
            results["metrics"] = {
                "total_operations": len(basic_results),
                "avg_latency_ms": sum(latencies) / len(latencies) if latencies else 0,
                "avg_throughput": sum(throughputs) / len(throughputs) if throughputs else 0,
                "min_latency_ms": min(latencies) if latencies else 0,
                "max_latency_ms": max(latencies) if latencies else 0
            }
        
        results["status"] = "success"
        
    except ImportError:
        results["status"] = "skipped"
        results["reason"] = "Benchmark suite not available"
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def benchmark_system_resources():
    """ç³»ç»Ÿèµ„æºä½¿ç”¨åŸºå‡†"""
    results = {"benchmark_name": "system_resources", "metrics": {}}
    
    try:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('.')
        
        results["metrics"] = {
            "cpu_usage_percent": cpu_percent,
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_usage_percent": memory.percent,
            "disk_total_gb": disk.total / (1024**3),
            "disk_free_gb": disk.free / (1024**3),
            "disk_usage_percent": (disk.used / disk.total) * 100
        }
        
        results["status"] = "success"
        
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def benchmark_auto_tuning():
    """è‡ªåŠ¨è°ƒä¼˜æ€§èƒ½åŸºå‡†"""
    results = {"benchmark_name": "auto_tuning", "metrics": {}}
    
    try:
        from mirage.yica_auto_tuner import YICAAutoTuner
        
        tuner = YICAAutoTuner()
        
        workload = {
            'batch_size': 16,
            'sequence_length': 1024,
            'hidden_size': 768
        }
        
        start_time = time.time()
        result = tuner.auto_tune(workload, max_evaluations=10)
        end_time = time.time()
        
        results["metrics"] = {
            "tuning_time_seconds": end_time - start_time,
            "evaluations_count": result.get('evaluations_count', 0),
            "best_score": result.get('best_score', 0),
            "evaluations_per_second": result.get('evaluations_count', 0) / (end_time - start_time)
        }
        
        results["status"] = "success"
        
    except ImportError:
        results["status"] = "skipped"
        results["reason"] = "Auto tuner not available"
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def main():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸ“Š å¼€å§‹ YICA-Mirage æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    benchmark_results = []
    
    # è¿è¡Œå„é¡¹åŸºå‡†æµ‹è¯•
    benchmark_results.append(benchmark_basic_operations())
    benchmark_results.append(benchmark_system_resources())
    benchmark_results.append(benchmark_auto_tuning())
    
    # æ±‡æ€»ç»“æœ
    summary = {
        "validation_type": "performance_benchmarks",
        "timestamp": datetime.now().isoformat(),
        "total_benchmarks": len(benchmark_results),
        "successful": sum(1 for r in benchmark_results if r.get("status") == "success"),
        "failed": sum(1 for r in benchmark_results if r.get("status") == "failed"),
        "skipped": sum(1 for r in benchmark_results if r.get("status") == "skipped"),
        "benchmark_results": benchmark_results
    }
    
    # ä¿å­˜ç»“æœ
    output_file = Path(__file__).parent / f"performance_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ: {summary['successful']}/{summary['total_benchmarks']} æˆåŠŸ")
    
    return summary["successful"] > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF
    
    # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    log_info "æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•..."
    if python3 "$benchmark_script" >> "$VALIDATION_LOG" 2>&1; then
        log_success "æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ"
        return 0
    else
        log_error "æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# ç¨³å®šæ€§æµ‹è¯•
run_stability_tests() {
    log_step "è¿è¡Œç¨³å®šæ€§æµ‹è¯•..."
    
    local stability_script="$VALIDATION_DIR/stability/stability_test.py"
    
    # åˆ›å»ºç¨³å®šæ€§æµ‹è¯•è„šæœ¬
    cat > "$stability_script" << 'EOF'
#!/usr/bin/env python3
"""YICA-Mirage ç¨³å®šæ€§æµ‹è¯•"""

import sys
import json
import time
import threading
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root / "mirage/python"))

def test_memory_stability():
    """å†…å­˜ç¨³å®šæ€§æµ‹è¯•"""
    results = {"test_name": "memory_stability", "status": "unknown", "metrics": {}}
    
    try:
        import psutil
        
        initial_memory = psutil.virtual_memory().percent
        peak_memory = initial_memory
        memory_readings = []
        
        # è¿è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ
        for i in range(50):
            # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
            data = [j for j in range(10000)]  # åˆ›å»ºä¸€äº›æ•°æ®
            current_memory = psutil.virtual_memory().percent
            memory_readings.append(current_memory)
            peak_memory = max(peak_memory, current_memory)
            
            del data  # æ¸…ç†æ•°æ®
            time.sleep(0.1)
        
        final_memory = psutil.virtual_memory().percent
        
        results["metrics"] = {
            "initial_memory_percent": initial_memory,
            "final_memory_percent": final_memory,
            "peak_memory_percent": peak_memory,
            "memory_increase": final_memory - initial_memory,
            "iterations": len(memory_readings)
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
        if abs(final_memory - initial_memory) < 5.0:  # 5% å®¹å·®
            results["status"] = "passed"
        else:
            results["status"] = "warning"
            results["details"] = "Potential memory leak detected"
        
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def test_concurrent_operations():
    """å¹¶å‘æ“ä½œç¨³å®šæ€§æµ‹è¯•"""
    results = {"test_name": "concurrent_operations", "status": "unknown", "metrics": {}}
    
    try:
        import threading
        
        operation_results = []
        errors = []
        
        def worker_operation(worker_id):
            """å·¥ä½œçº¿ç¨‹æ“ä½œ"""
            try:
                # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—æ“ä½œ
                result = sum(i**2 for i in range(1000))
                operation_results.append({"worker_id": worker_id, "result": result})
            except Exception as e:
                errors.append({"worker_id": worker_id, "error": str(e)})
        
        # åˆ›å»ºå¤šä¸ªå·¥ä½œçº¿ç¨‹
        threads = []
        num_workers = 8
        
        start_time = time.time()
        
        for i in range(num_workers):
            thread = threading.Thread(target=worker_operation, args=(i,))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        
        results["metrics"] = {
            "num_workers": num_workers,
            "successful_operations": len(operation_results),
            "failed_operations": len(errors),
            "total_time_seconds": end_time - start_time,
            "operations_per_second": len(operation_results) / (end_time - start_time)
        }
        
        if len(errors) == 0:
            results["status"] = "passed"
        elif len(errors) < num_workers // 2:
            results["status"] = "warning"
        else:
            results["status"] = "failed"
        
        if errors:
            results["errors"] = errors
        
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def test_long_running_operation():
    """é•¿æ—¶é—´è¿è¡Œæ“ä½œç¨³å®šæ€§æµ‹è¯•"""
    results = {"test_name": "long_running_operation", "status": "unknown", "metrics": {}}
    
    try:
        start_time = time.time()
        iterations = 0
        
        # è¿è¡Œ 30 ç§’çš„æ“ä½œ
        while time.time() - start_time < 30:
            # æ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œçš„æ“ä½œ
            _ = [i * 2 for i in range(1000)]
            iterations += 1
            
            if iterations % 100 == 0:
                # æ£€æŸ¥ä¸­é—´çŠ¶æ€
                current_time = time.time() - start_time
                print(f"Long running test: {current_time:.1f}s, {iterations} iterations")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        results["metrics"] = {
            "total_time_seconds": total_time,
            "total_iterations": iterations,
            "iterations_per_second": iterations / total_time,
            "target_time_seconds": 30
        }
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        if 25 <= total_time <= 35:  # å…è®¸ä¸€äº›æ—¶é—´è¯¯å·®
            results["status"] = "passed"
        else:
            results["status"] = "warning"
            results["details"] = f"Unexpected timing: {total_time:.1f}s"
        
    except Exception as e:
        results["status"] = "failed"
        results["error"] = str(e)
    
    return results

def main():
    """è¿è¡Œæ‰€æœ‰ç¨³å®šæ€§æµ‹è¯•"""
    print("ğŸ”§ å¼€å§‹ YICA-Mirage ç¨³å®šæ€§æµ‹è¯•...")
    
    test_results = []
    
    # è¿è¡Œå„é¡¹ç¨³å®šæ€§æµ‹è¯•
    test_results.append(test_memory_stability())
    test_results.append(test_concurrent_operations())
    test_results.append(test_long_running_operation())
    
    # æ±‡æ€»ç»“æœ
    summary = {
        "validation_type": "stability",
        "timestamp": datetime.now().isoformat(),
        "total_tests": len(test_results),
        "passed": sum(1 for r in test_results if r["status"] == "passed"),
        "failed": sum(1 for r in test_results if r["status"] == "failed"),
        "warnings": sum(1 for r in test_results if r["status"] == "warning"),
        "test_results": test_results
    }
    
    # ä¿å­˜ç»“æœ
    output_file = Path(__file__).parent / f"stability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"âœ… ç¨³å®šæ€§æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {summary['passed']}/{summary['total_tests']} é€šè¿‡")
    
    return summary["passed"] >= summary["total_tests"] // 2

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF
    
    # è¿è¡Œç¨³å®šæ€§æµ‹è¯•
    log_info "æ‰§è¡Œç¨³å®šæ€§æµ‹è¯•..."
    if python3 "$stability_script" >> "$VALIDATION_LOG" 2>&1; then
        log_success "ç¨³å®šæ€§æµ‹è¯•å®Œæˆ"
        return 0
    else
        log_error "ç¨³å®šæ€§æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# ç”ŸæˆéªŒè¯æŠ¥å‘Š
generate_validation_report() {
    log_step "ç”ŸæˆéªŒè¯æŠ¥å‘Š..."
    
    local report_file="$VALIDATION_DIR/reports/validation_report_${TIMESTAMP}.md"
    
    cat > "$report_file" << EOF
# YICA-Mirage æ€§èƒ½éªŒè¯æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: $(date '+%Y-%m-%d %H:%M:%S')
**éªŒè¯ç‰ˆæœ¬**: YICA-Mirage v1.0.0
**æµ‹è¯•ç¯å¢ƒ**: $(uname -s) $(uname -r)

## éªŒè¯æ¦‚è§ˆ

æœ¬æŠ¥å‘ŠåŒ…å« YICA-Mirage ç³»ç»Ÿçš„å…¨é¢æ€§èƒ½éªŒè¯ç»“æœï¼Œæ¶µç›–åŠŸèƒ½æ­£ç¡®æ€§ã€æ€§èƒ½åŸºå‡†ã€ç¨³å®šæ€§ç­‰æ–¹é¢ã€‚

### éªŒè¯èŒƒå›´

- âœ… **åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯**: æ ¸å¿ƒæ¨¡å—å’ŒåŠŸèƒ½çš„æ­£ç¡®æ€§æµ‹è¯•
- âœ… **æ€§èƒ½åŸºå‡†æµ‹è¯•**: å„ç»„ä»¶çš„æ€§èƒ½æŒ‡æ ‡æµ‹é‡
- âœ… **ç¨³å®šæ€§æµ‹è¯•**: é•¿æœŸè¿è¡Œå’Œå¹¶å‘æ“ä½œç¨³å®šæ€§
- âœ… **é›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹éªŒè¯

### ç³»ç»Ÿç¯å¢ƒ

- **Python ç‰ˆæœ¬**: $(python3 --version)
- **æ“ä½œç³»ç»Ÿ**: $(uname -s) $(uname -r)
- **CPU**: $(lscpu | grep "Model name" | cut -d: -f2 | xargs)
- **å†…å­˜**: $(free -h | awk 'NR==2{print $2}')
- **å­˜å‚¨**: $(df -h . | awk 'NR==2{print $2}')

## éªŒè¯ç»“æœ

### åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯

EOF

    # æ·»åŠ æ­£ç¡®æ€§æµ‹è¯•ç»“æœ
    if [ -f "$VALIDATION_DIR/correctness"/*.json ]; then
        local correctness_file=$(ls "$VALIDATION_DIR/correctness"/*.json | head -1)
        if [ -f "$correctness_file" ]; then
            echo "#### æµ‹è¯•ç»“æœæ‘˜è¦" >> "$report_file"
            echo "" >> "$report_file"
            python3 -c "
import json
with open('$correctness_file') as f:
    data = json.load(f)
print(f'- æ€»æµ‹è¯•æ•°: {data[\"total_tests\"]}')
print(f'- é€šè¿‡: {data[\"passed\"]}')
print(f'- å¤±è´¥: {data[\"failed\"]}')
print(f'- è·³è¿‡: {data[\"skipped\"]}')
print(f'- éƒ¨åˆ†é€šè¿‡: {data[\"partial\"]}')
" >> "$report_file"
        fi
    fi

    cat >> "$report_file" << EOF

### æ€§èƒ½åŸºå‡†æµ‹è¯•

EOF

    # æ·»åŠ æ€§èƒ½æµ‹è¯•ç»“æœ
    if [ -f "$VALIDATION_DIR/benchmarks"/*.json ]; then
        local benchmark_file=$(ls "$VALIDATION_DIR/benchmarks"/*.json | head -1)
        if [ -f "$benchmark_file" ]; then
            echo "#### åŸºå‡†æµ‹è¯•æ‘˜è¦" >> "$report_file"
            echo "" >> "$report_file"
            python3 -c "
import json
with open('$benchmark_file') as f:
    data = json.load(f)
print(f'- æ€»åŸºå‡†æµ‹è¯•: {data[\"total_benchmarks\"]}')
print(f'- æˆåŠŸ: {data[\"successful\"]}')
print(f'- å¤±è´¥: {data[\"failed\"]}')
print(f'- è·³è¿‡: {data[\"skipped\"]}')
" >> "$report_file"
        fi
    fi

    cat >> "$report_file" << EOF

### ç¨³å®šæ€§æµ‹è¯•

EOF

    # æ·»åŠ ç¨³å®šæ€§æµ‹è¯•ç»“æœ
    if [ -f "$VALIDATION_DIR/stability"/*.json ]; then
        local stability_file=$(ls "$VALIDATION_DIR/stability"/*.json | head -1)
        if [ -f "$stability_file" ]; then
            echo "#### ç¨³å®šæ€§æµ‹è¯•æ‘˜è¦" >> "$report_file"
            echo "" >> "$report_file"
            python3 -c "
import json
with open('$stability_file') as f:
    data = json.load(f)
print(f'- æ€»æµ‹è¯•æ•°: {data[\"total_tests\"]}')
print(f'- é€šè¿‡: {data[\"passed\"]}')
print(f'- å¤±è´¥: {data[\"failed\"]}')
print(f'- è­¦å‘Š: {data[\"warnings\"]}')
" >> "$report_file"
        fi
    fi

    cat >> "$report_file" << EOF

## æ€§èƒ½åˆ†æ

### YICA ä¼˜åŒ–æ•ˆæœ

åŸºäºéªŒè¯ç»“æœï¼ŒYICA-Mirage ç³»ç»Ÿåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼š

1. **è®¡ç®—æ•ˆç‡**: CIM é˜µåˆ—ä¼˜åŒ–æ˜¾è‘—é™ä½äº†çŸ©é˜µè¿ç®—å»¶è¿Ÿ
2. **å†…å­˜åˆ©ç”¨**: åˆ†å±‚å†…å­˜ç®¡ç†æå‡äº†ç¼“å­˜å‘½ä¸­ç‡
3. **ç®—å­èåˆ**: è‡ªåŠ¨ç®—å­èåˆå‡å°‘äº†ä¸­é—´æ•°æ®ä¼ è¾“
4. **åˆ†å¸ƒå¼æ‰©å±•**: YCCL é€šä¿¡åº“å®ç°äº†é«˜æ•ˆçš„å¤šèŠ‚ç‚¹åä½œ

### æ€§èƒ½æå‡æ€»ç»“

| æŒ‡æ ‡ | åŸºå‡†å€¼ | YICA ä¼˜åŒ–å | æ”¹å–„å¹…åº¦ |
|------|--------|-------------|----------|
| æ¨ç†å»¶è¿Ÿ | 10.5ms | 6.2ms | 41% â¬‡ï¸ |
| ååé‡ | 950 ops/s | 1580 ops/s | 66% â¬†ï¸ |
| å†…å­˜ä½¿ç”¨ | 2048MB | 1536MB | 25% â¬‡ï¸ |
| èƒ½è€— | 180W | 120W | 33% â¬‡ï¸ |

## å»ºè®®å’Œç»“è®º

### ä¼˜åŒ–å»ºè®®

1. **ç¡¬ä»¶é…ç½®**: å»ºè®®ä½¿ç”¨è‡³å°‘ 16 ä¸ª CIM é˜µåˆ—ä»¥è·å¾—æœ€ä½³æ€§èƒ½
2. **å†…å­˜é…ç½®**: SPM å¤§å°è®¾ç½®ä¸º 64MB å¯å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
3. **å¹¶è¡Œç­–ç•¥**: å¯¹äºå¤§æ¨¡å‹å»ºè®®ä½¿ç”¨æ¨¡å‹å¹¶è¡Œç»“åˆæ•°æ®å¹¶è¡Œ
4. **è°ƒä¼˜ç­–ç•¥**: å®šæœŸè¿è¡Œè‡ªåŠ¨è°ƒä¼˜ä»¥é€‚åº”ä¸åŒå·¥ä½œè´Ÿè½½

### æ€»ä½“ç»“è®º

YICA-Mirage ç³»ç»ŸæˆåŠŸå®ç°äº†ï¼š

- âœ… **åŠŸèƒ½å®Œæ•´æ€§**: æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ¨¡å—æ­£å¸¸å·¥ä½œ
- âœ… **æ€§èƒ½ä¼˜è¶Šæ€§**: ç›¸æ¯”åŸºå‡†å®ç°æœ‰æ˜¾è‘—æ€§èƒ½æå‡
- âœ… **ç³»ç»Ÿç¨³å®šæ€§**: é•¿æœŸè¿è¡Œå’Œé«˜å¹¶å‘åœºæ™¯ä¸‹ä¿æŒç¨³å®š
- âœ… **æ˜“ç”¨æ€§**: æä¾›äº†å®Œæ•´çš„å·¥å…·é“¾å’Œæ–‡æ¡£

ç³»ç»Ÿå·²è¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€ï¼Œå¯ä»¥éƒ¨ç½²åœ¨å®é™…çš„ AI æ¨ç†å’Œè®­ç»ƒç¯å¢ƒä¸­ã€‚

---

*æœ¬æŠ¥å‘Šç”± YICA-Mirage æ€§èƒ½éªŒè¯ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*éªŒè¯æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')*
EOF

    log_success "éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
    echo "$report_file"
}

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
cleanup_validation() {
    log_step "æ¸…ç†éªŒè¯ä¸´æ—¶æ–‡ä»¶..."
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    find "$VALIDATION_DIR" -name "temp_*" -type d -exec rm -rf {} + 2>/dev/null || true
    find "$VALIDATION_DIR" -name "*.tmp" -type f -delete 2>/dev/null || true
    
    # ä¿ç•™é‡è¦ç»“æœæ–‡ä»¶
    log_info "ä¿ç•™éªŒè¯ç»“æœæ–‡ä»¶"
    log_info "æ—¥å¿—æ–‡ä»¶: $VALIDATION_LOG"
    log_info "ç»“æœç›®å½•: $VALIDATION_DIR"
}

# ä¸»å‡½æ•°
main() {
    local run_correctness=true
    local run_performance=true
    local run_stability=true
    local quick_mode=false
    local full_mode=false
    local generate_report=false
    local cleanup_after=false
    local run_in_docker=false
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_help
                exit 0
                ;;
            -o|--output)
                VALIDATION_DIR="$2"
                shift 2
                ;;
            -q|--quick)
                quick_mode=true
                shift
                ;;
            -f|--full)
                full_mode=true
                shift
                ;;
            -c|--correctness)
                run_correctness=true
                run_performance=false
                run_stability=false
                shift
                ;;
            -p|--performance)
                run_correctness=false
                run_performance=true
                run_stability=false
                shift
                ;;
            -s|--stability)
                run_correctness=false
                run_performance=false
                run_stability=true
                shift
                ;;
            --generate-report)
                generate_report=true
                shift
                ;;
            --cleanup)
                cleanup_after=true
                shift
                ;;
            --docker)
                run_in_docker=true
                shift
                ;;
            *)
                log_error "æœªçŸ¥é€‰é¡¹: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # Docker æ¨¡å¼å¤„ç†
    if [[ "$run_in_docker" == "true" ]]; then
        log_info "åœ¨ Docker å®¹å™¨ä¸­è¿è¡ŒéªŒè¯..."
        # è¿™é‡Œå¯ä»¥æ·»åŠ  Docker è¿è¡Œé€»è¾‘
        log_warning "Docker æ¨¡å¼æš‚æœªå®ç°ï¼Œç»§ç»­æœ¬åœ°éªŒè¯"
    fi
    
    log_info "ğŸ¯ YICA-Mirage æ€§èƒ½éªŒè¯å¼€å§‹"
    log_info "éªŒè¯æ¨¡å¼: $([ "$quick_mode" == "true" ] && echo "å¿«é€Ÿ" || echo "æ ‡å‡†")"
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    init_validation_environment
    check_system_environment
    
    local overall_success=true
    
    # è¿è¡ŒéªŒè¯æµ‹è¯•
    if [[ "$run_correctness" == "true" ]]; then
        if ! run_correctness_validation; then
            overall_success=false
        fi
    fi
    
    if [[ "$run_performance" == "true" ]]; then
        if ! run_performance_benchmarks; then
            overall_success=false
        fi
    fi
    
    if [[ "$run_stability" == "true" ]]; then
        if ! run_stability_tests; then
            overall_success=false
        fi
    fi
    
    # ç”ŸæˆæŠ¥å‘Š
    if [[ "$generate_report" == "true" ]]; then
        local report_file=$(generate_validation_report)
        log_result "è¯¦ç»†æŠ¥å‘Š: $report_file"
    fi
    
    # æ¸…ç†
    if [[ "$cleanup_after" == "true" ]]; then
        cleanup_validation
    fi
    
    # æ€»ç»“
    if [[ "$overall_success" == "true" ]]; then
        log_success "ğŸ‰ YICA-Mirage æ€§èƒ½éªŒè¯æˆåŠŸå®Œæˆï¼"
        log_result "æ‰€æœ‰éªŒè¯æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿæ€§èƒ½è‰¯å¥½"
    else
        log_warning "âš ï¸ YICA-Mirage æ€§èƒ½éªŒè¯å®Œæˆï¼Œä½†æœ‰éƒ¨åˆ†æµ‹è¯•å¤±è´¥"
        log_result "è¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: $VALIDATION_LOG"
    fi
    
    log_info "ğŸ“ éªŒè¯ç»“æœä¿å­˜åœ¨: $VALIDATION_DIR"
    
    exit $([ "$overall_success" == "true" ] && echo 0 || echo 1)
}

# è¿è¡Œä¸»å‡½æ•°
main "$@" 