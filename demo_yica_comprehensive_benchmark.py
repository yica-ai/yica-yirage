#!/usr/bin/env python3
"""
YICA-Mirage ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•

è¿™ä¸ªè„šæœ¬æä¾›äº†å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯”åŸå§‹ Mirage å’Œ YICA ä¼˜åŒ–ç‰ˆæœ¬åœ¨å„ç§ç¥ç»ç½‘ç»œæ“ä½œä¸Šçš„æ€§èƒ½ï¼š

1. çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•
2. Attention æœºåˆ¶æ€§èƒ½æµ‹è¯•  
3. MLP å±‚æ€§èƒ½æµ‹è¯•
4. LayerNorm æ€§èƒ½æµ‹è¯•
5. ç«¯åˆ°ç«¯ Llama æ¨¡å‹æ€§èƒ½æµ‹è¯•
6. å†…å­˜ä½¿ç”¨æ•ˆç‡åˆ†æ
7. èƒ½è€—æ•ˆç‡åˆ†æ
"""

import torch
import triton
import time
import psutil
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
import json
import os
from contextlib import contextmanager
import threading
import gc

# å¯¼å…¥ Mirage å’Œ YICA ç›¸å…³æ¨¡å—
import mirage
from mirage.yica.config import YICAConfig
from mirage.yica.yica_backend import YICABackend
from mirage.python.mirage.yica_llama_optimizer import (
    YICALlamaOptimizer, LlamaModelConfig, 
    YICAOptimizedAttention, YICAOptimizedMLP, YICAOptimizedRMSNorm
)


@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    # æµ‹è¯•å‚æ•°
    warmup_iterations: int = 10
    benchmark_iterations: int = 100
    batch_sizes: List[int] = None
    sequence_lengths: List[int] = None
    hidden_sizes: List[int] = None
    
    # ç¡¬ä»¶é…ç½®
    device: str = 'cuda'
    dtype: torch.dtype = torch.float16
    
    # è¾“å‡ºé…ç½®
    save_results: bool = True
    results_dir: str = 'yica_benchmark_results'
    generate_plots: bool = True
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [1, 4, 8, 16, 32]
        if self.sequence_lengths is None:
            self.sequence_lengths = [128, 512, 1024, 2048]
        if self.hidden_sizes is None:
            self.hidden_sizes = [768, 1024, 2048, 4096]


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    operation_name: str
    configuration: Dict[str, Any]
    
    # å»¶è¿ŸæŒ‡æ ‡ (ms)
    mirage_latency: float
    yica_latency: float
    latency_speedup: float
    
    # ååé‡æŒ‡æ ‡ (ops/sec)
    mirage_throughput: float
    yica_throughput: float
    throughput_speedup: float
    
    # å†…å­˜æŒ‡æ ‡ (MB)
    mirage_memory: float
    yica_memory: float
    memory_efficiency: float
    
    # èƒ½è€—æŒ‡æ ‡ (ä¼°ç®—)
    mirage_energy: float
    yica_energy: float
    energy_efficiency: float
    
    # é¢å¤–ç»Ÿè®¡
    std_dev_mirage: float
    std_dev_yica: float
    confidence_interval: float


class MemoryMonitor:
    """å†…å­˜ä½¿ç”¨ç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.memory_usage = []
        self.monitor_thread = None
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨"""
        self.monitoring = True
        self.memory_usage = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> float:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›å¹³å‡å†…å­˜ä½¿ç”¨é‡ (MB)"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        
        if self.memory_usage:
            return np.mean(self.memory_usage)
        return 0.0
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                self.memory_usage.append(gpu_memory)
            time.sleep(0.01)  # 10ms é‡‡æ ·é—´éš”


class YICABenchmarkSuite:
    """YICA ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.memory_monitor = MemoryMonitor()
        
        # åˆå§‹åŒ– YICA é…ç½®
        self.yica_config = YICAConfig(
            num_cim_arrays=16,
            spm_size_per_die=128 * 1024 * 1024,  # 128MB
            dram_size_per_cluster=16 * 1024 * 1024 * 1024,  # 16GB
            enable_quantization=True,
            target_precision="fp16"
        )
        
        # åˆå§‹åŒ– YICA åç«¯
        self.yica_backend = YICABackend(self.yica_config)
        
        # ç»“æœå­˜å‚¨
        self.benchmark_results: List[PerformanceMetrics] = []
        
        # åˆ›å»ºç»“æœç›®å½•
        if self.config.save_results:
            os.makedirs(self.config.results_dir, exist_ok=True)
    
    def run_comprehensive_benchmark(self) -> Dict[str, List[PerformanceMetrics]]:
        """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ YICA-Mirage ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•é…ç½®: {self.config.benchmark_iterations} æ¬¡è¿­ä»£, "
              f"{self.config.warmup_iterations} æ¬¡é¢„çƒ­")
        
        all_results = {}
        
        # 1. çŸ©é˜µä¹˜æ³•åŸºå‡†æµ‹è¯•
        print("\n1ï¸âƒ£ çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•...")
        all_results['matmul'] = self.benchmark_matrix_multiplication()
        
        # 2. Attention æœºåˆ¶åŸºå‡†æµ‹è¯•
        print("\n2ï¸âƒ£ Attention æœºåˆ¶æ€§èƒ½æµ‹è¯•...")
        all_results['attention'] = self.benchmark_attention()
        
        # 3. MLP å±‚åŸºå‡†æµ‹è¯•
        print("\n3ï¸âƒ£ MLP å±‚æ€§èƒ½æµ‹è¯•...")
        all_results['mlp'] = self.benchmark_mlp()
        
        # 4. LayerNorm åŸºå‡†æµ‹è¯•
        print("\n4ï¸âƒ£ LayerNorm æ€§èƒ½æµ‹è¯•...")
        all_results['layernorm'] = self.benchmark_layernorm()
        
        # 5. ç«¯åˆ°ç«¯æ¨¡å‹åŸºå‡†æµ‹è¯•
        print("\n5ï¸âƒ£ ç«¯åˆ°ç«¯ Llama æ¨¡å‹æ€§èƒ½æµ‹è¯•...")
        all_results['end_to_end'] = self.benchmark_end_to_end_llama()
        
        # 6. å†…å­˜æ•ˆç‡åˆ†æ
        print("\n6ï¸âƒ£ å†…å­˜æ•ˆç‡åˆ†æ...")
        all_results['memory_efficiency'] = self.analyze_memory_efficiency()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report(all_results)
        
        print("\nğŸ‰ YICA-Mirage ç»¼åˆåŸºå‡†æµ‹è¯•å®Œæˆ!")
        return all_results
    
    def benchmark_matrix_multiplication(self) -> List[PerformanceMetrics]:
        """çŸ©é˜µä¹˜æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        test_configs = [
            (512, 512, 512),
            (1024, 1024, 1024), 
            (2048, 2048, 2048),
            (4096, 4096, 4096),
            (8192, 4096, 1024),  # ä¸å¯¹ç§°çŸ©é˜µ
        ]
        
        for M, N, K in test_configs:
            print(f"  æµ‹è¯•çŸ©é˜µä¹˜æ³•: {M}x{K} @ {K}x{N}")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            A = torch.randn(M, K, dtype=self.config.dtype, device=self.config.device)
            B = torch.randn(K, N, dtype=self.config.dtype, device=self.config.device)
            
            # æµ‹è¯•åŸå§‹ PyTorch å®ç°
            mirage_time, mirage_memory = self._benchmark_operation(
                lambda: torch.matmul(A, B),
                "PyTorch MatMul"
            )
            
            # æµ‹è¯• YICA ä¼˜åŒ–å®ç°
            yica_time, yica_memory = self._benchmark_operation(
                lambda: self._yica_optimized_matmul(A, B),
                "YICA MatMul"
            )
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            metrics = self._calculate_performance_metrics(
                operation_name="Matrix Multiplication",
                configuration={"M": M, "N": N, "K": K},
                mirage_time=mirage_time,
                yica_time=yica_time,
                mirage_memory=mirage_memory,
                yica_memory=yica_memory,
                operation_count=2 * M * N * K  # FLOPs
            )
            
            results.append(metrics)
            print(f"    åŠ é€Ÿæ¯”: {metrics.latency_speedup:.2f}x, "
                  f"å†…å­˜æ•ˆç‡: {metrics.memory_efficiency:.2f}")
        
        return results
    
    def benchmark_attention(self) -> List[PerformanceMetrics]:
        """Attention æœºåˆ¶æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in self.config.hidden_sizes:
                    print(f"  æµ‹è¯• Attention: batch={batch_size}, seq={seq_len}, hidden={hidden_size}")
                    
                    # åˆ›å»ºæ ‡å‡† Attention å±‚
                    standard_attention = torch.nn.MultiheadAttention(
                        embed_dim=hidden_size,
                        num_heads=32,
                        batch_first=True
                    ).to(self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»º YICA ä¼˜åŒ– Attention å±‚
                    yica_attention = YICAOptimizedAttention(
                        hidden_size=hidden_size,
                        num_heads=32,
                        num_kv_heads=32,
                        yica_config=self.yica_config
                    ).to(self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»ºæµ‹è¯•æ•°æ®
                    input_tensor = torch.randn(
                        batch_size, seq_len, hidden_size,
                        dtype=self.config.dtype, device=self.config.device
                    )
                    
                    # æµ‹è¯•æ ‡å‡† Attention
                    mirage_time, mirage_memory = self._benchmark_operation(
                        lambda: standard_attention(input_tensor, input_tensor, input_tensor),
                        "Standard Attention"
                    )
                    
                    # æµ‹è¯• YICA Attention
                    yica_time, yica_memory = self._benchmark_operation(
                        lambda: yica_attention(input_tensor),
                        "YICA Attention"
                    )
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    metrics = self._calculate_performance_metrics(
                        operation_name="Multi-Head Attention",
                        configuration={
                            "batch_size": batch_size,
                            "seq_len": seq_len, 
                            "hidden_size": hidden_size
                        },
                        mirage_time=mirage_time,
                        yica_time=yica_time,
                        mirage_memory=mirage_memory,
                        yica_memory=yica_memory,
                        operation_count=batch_size * seq_len * seq_len * hidden_size
                    )
                    
                    results.append(metrics)
                    print(f"    åŠ é€Ÿæ¯”: {metrics.latency_speedup:.2f}x")
        
        return results
    
    def benchmark_mlp(self) -> List[PerformanceMetrics]:
        """MLP å±‚æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in [1024, 2048, 4096]:
                    intermediate_size = int(hidden_size * 2.67)  # Llama æ¯”ä¾‹
                    
                    print(f"  æµ‹è¯• MLP: batch={batch_size}, seq={seq_len}, "
                          f"hidden={hidden_size}, intermediate={intermediate_size}")
                    
                    # åˆ›å»ºæ ‡å‡†é—¨æ§ MLP
                    class StandardGatedMLP(torch.nn.Module):
                        def __init__(self, hidden_size, intermediate_size):
                            super().__init__()
                            self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size, bias=False)
                            self.up_proj = torch.nn.Linear(hidden_size, intermediate_size, bias=False)
                            self.down_proj = torch.nn.Linear(intermediate_size, hidden_size, bias=False)
                        
                        def forward(self, x):
                            gate = torch.nn.functional.silu(self.gate_proj(x))
                            up = self.up_proj(x)
                            return self.down_proj(gate * up)
                    
                    standard_mlp = StandardGatedMLP(hidden_size, intermediate_size).to(
                        self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»º YICA ä¼˜åŒ– MLP
                    yica_mlp = YICAOptimizedMLP(
                        hidden_size=hidden_size,
                        intermediate_size=intermediate_size,
                        yica_config=self.yica_config
                    ).to(self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»ºæµ‹è¯•æ•°æ®
                    input_tensor = torch.randn(
                        batch_size, seq_len, hidden_size,
                        dtype=self.config.dtype, device=self.config.device
                    )
                    
                    # æµ‹è¯•æ ‡å‡† MLP
                    mirage_time, mirage_memory = self._benchmark_operation(
                        lambda: standard_mlp(input_tensor),
                        "Standard MLP"
                    )
                    
                    # æµ‹è¯• YICA MLP
                    yica_time, yica_memory = self._benchmark_operation(
                        lambda: yica_mlp(input_tensor),
                        "YICA MLP"
                    )
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    metrics = self._calculate_performance_metrics(
                        operation_name="Gated MLP",
                        configuration={
                            "batch_size": batch_size,
                            "seq_len": seq_len,
                            "hidden_size": hidden_size,
                            "intermediate_size": intermediate_size
                        },
                        mirage_time=mirage_time,
                        yica_time=yica_time,
                        mirage_memory=mirage_memory,
                        yica_memory=yica_memory,
                        operation_count=batch_size * seq_len * (hidden_size * intermediate_size * 3)
                    )
                    
                    results.append(metrics)
                    print(f"    åŠ é€Ÿæ¯”: {metrics.latency_speedup:.2f}x")
        
        return results
    
    def benchmark_layernorm(self) -> List[PerformanceMetrics]:
        """LayerNorm æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in self.config.hidden_sizes:
                    print(f"  æµ‹è¯• LayerNorm: batch={batch_size}, seq={seq_len}, hidden={hidden_size}")
                    
                    # åˆ›å»ºæ ‡å‡† LayerNorm
                    standard_norm = torch.nn.LayerNorm(hidden_size).to(
                        self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»º YICA ä¼˜åŒ– RMSNorm
                    yica_norm = YICAOptimizedRMSNorm(
                        hidden_size=hidden_size,
                        yica_config=self.yica_config
                    ).to(self.config.device).to(self.config.dtype)
                    
                    # åˆ›å»ºæµ‹è¯•æ•°æ®
                    input_tensor = torch.randn(
                        batch_size, seq_len, hidden_size,
                        dtype=self.config.dtype, device=self.config.device
                    )
                    
                    # æµ‹è¯•æ ‡å‡† LayerNorm
                    mirage_time, mirage_memory = self._benchmark_operation(
                        lambda: standard_norm(input_tensor),
                        "Standard LayerNorm"
                    )
                    
                    # æµ‹è¯• YICA RMSNorm
                    yica_time, yica_memory = self._benchmark_operation(
                        lambda: yica_norm(input_tensor),
                        "YICA RMSNorm"
                    )
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    metrics = self._calculate_performance_metrics(
                        operation_name="Layer Normalization",
                        configuration={
                            "batch_size": batch_size,
                            "seq_len": seq_len,
                            "hidden_size": hidden_size
                        },
                        mirage_time=mirage_time,
                        yica_time=yica_time,
                        mirage_memory=mirage_memory,
                        yica_memory=yica_memory,
                        operation_count=batch_size * seq_len * hidden_size
                    )
                    
                    results.append(metrics)
                    print(f"    åŠ é€Ÿæ¯”: {metrics.latency_speedup:.2f}x")
        
        return results
    
    def benchmark_end_to_end_llama(self) -> List[PerformanceMetrics]:
        """ç«¯åˆ°ç«¯ Llama æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        # ç®€åŒ–çš„ Llama æ¨¡å‹é…ç½®ç”¨äºæµ‹è¯•
        test_configs = [
            {"layers": 12, "hidden": 768, "heads": 12},    # å°æ¨¡å‹
            {"layers": 24, "hidden": 1024, "heads": 16},   # ä¸­ç­‰æ¨¡å‹
            {"layers": 32, "hidden": 2048, "heads": 32},   # å¤§æ¨¡å‹
        ]
        
        for config in test_configs:
            print(f"  æµ‹è¯•ç«¯åˆ°ç«¯æ¨¡å‹: {config['layers']} å±‚, {config['hidden']} éšè—ç»´åº¦")
            
            # åˆ›å»ºæ¨¡å‹é…ç½®
            model_config = LlamaModelConfig(
                hidden_size=config['hidden'],
                intermediate_size=int(config['hidden'] * 2.67),
                num_hidden_layers=config['layers'],
                num_attention_heads=config['heads'],
                num_key_value_heads=config['heads']
            )
            
            # åˆ›å»ºç®€åŒ–çš„æµ‹è¯•æ¨¡å‹
            class SimplifiedLlamaLayer(torch.nn.Module):
                def __init__(self, hidden_size, intermediate_size, num_heads):
                    super().__init__()
                    self.attention = torch.nn.MultiheadAttention(
                        hidden_size, num_heads, batch_first=True
                    )
                    self.norm1 = torch.nn.LayerNorm(hidden_size)
                    self.mlp = torch.nn.Sequential(
                        torch.nn.Linear(hidden_size, intermediate_size),
                        torch.nn.ReLU(),
                        torch.nn.Linear(intermediate_size, hidden_size)
                    )
                    self.norm2 = torch.nn.LayerNorm(hidden_size)
                
                def forward(self, x):
                    # Attention block
                    attn_out, _ = self.attention(x, x, x)
                    x = self.norm1(x + attn_out)
                    
                    # MLP block  
                    mlp_out = self.mlp(x)
                    x = self.norm2(x + mlp_out)
                    
                    return x
            
            # åˆ›å»ºæ ‡å‡†æ¨¡å‹
            standard_model = torch.nn.Sequential(*[
                SimplifiedLlamaLayer(
                    config['hidden'], 
                    int(config['hidden'] * 2.67), 
                    config['heads']
                ) for _ in range(config['layers'])
            ]).to(self.config.device).to(self.config.dtype)
            
            # åˆ›å»º YICA ä¼˜åŒ–æ¨¡å‹
            yica_optimizer = YICALlamaOptimizer(model_config, self.yica_config)
            yica_model, _ = yica_optimizer.optimize_llama_model(standard_model)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size, seq_len = 4, 512
            input_tensor = torch.randn(
                batch_size, seq_len, config['hidden'],
                dtype=self.config.dtype, device=self.config.device
            )
            
            # æµ‹è¯•æ ‡å‡†æ¨¡å‹
            mirage_time, mirage_memory = self._benchmark_operation(
                lambda: standard_model(input_tensor),
                "Standard Llama Model"
            )
            
            # æµ‹è¯• YICA ä¼˜åŒ–æ¨¡å‹
            yica_time, yica_memory = self._benchmark_operation(
                lambda: yica_model(input_tensor),
                "YICA Optimized Llama Model"
            )
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            metrics = self._calculate_performance_metrics(
                operation_name="End-to-End Llama Model",
                configuration={
                    "num_layers": config['layers'],
                    "hidden_size": config['hidden'],
                    "num_heads": config['heads'],
                    "batch_size": batch_size,
                    "seq_len": seq_len
                },
                mirage_time=mirage_time,
                yica_time=yica_time,
                mirage_memory=mirage_memory,
                yica_memory=yica_memory,
                operation_count=batch_size * seq_len * config['hidden'] * config['layers']
            )
            
            results.append(metrics)
            print(f"    ç«¯åˆ°ç«¯åŠ é€Ÿæ¯”: {metrics.latency_speedup:.2f}x, "
                  f"å†…å­˜èŠ‚çœ: {(1 - metrics.memory_efficiency) * 100:.1f}%")
        
        return results
    
    def analyze_memory_efficiency(self) -> List[PerformanceMetrics]:
        """å†…å­˜æ•ˆç‡åˆ†æ"""
        results = []
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„å¼ é‡å†…å­˜ä½¿ç”¨
        tensor_sizes = [
            (1024, 1024),
            (2048, 2048), 
            (4096, 4096),
            (8192, 8192)
        ]
        
        for rows, cols in tensor_sizes:
            print(f"  åˆ†æå†…å­˜æ•ˆç‡: {rows}x{cols} å¼ é‡")
            
            # æ ‡å‡†å†…å­˜åˆ†é…
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            standard_tensor = torch.randn(rows, cols, dtype=self.config.dtype, device=self.config.device)
            standard_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
            
            # YICA ä¼˜åŒ–å†…å­˜åˆ†é…ï¼ˆæ¨¡æ‹Ÿ SPM ä¼˜åŒ–ï¼‰
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            # æ¨¡æ‹Ÿ YICA SPM å†…å­˜ä¼˜åŒ–
            yica_tensor = self._allocate_yica_optimized_tensor(rows, cols)
            yica_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
            
            # è®¡ç®—å†…å­˜æ•ˆç‡æŒ‡æ ‡
            memory_efficiency = standard_memory / yica_memory if yica_memory > 0 else 1.0
            
            metrics = PerformanceMetrics(
                operation_name="Memory Allocation",
                configuration={"tensor_size": f"{rows}x{cols}"},
                mirage_latency=0.0,
                yica_latency=0.0,
                latency_speedup=1.0,
                mirage_throughput=0.0,
                yica_throughput=0.0,
                throughput_speedup=1.0,
                mirage_memory=standard_memory,
                yica_memory=yica_memory,
                memory_efficiency=memory_efficiency,
                mirage_energy=0.0,
                yica_energy=0.0,
                energy_efficiency=1.0,
                std_dev_mirage=0.0,
                std_dev_yica=0.0,
                confidence_interval=0.95
            )
            
            results.append(metrics)
            print(f"    å†…å­˜æ•ˆç‡æå‡: {memory_efficiency:.2f}x")
            
            # æ¸…ç†å†…å­˜
            del standard_tensor, yica_tensor
            torch.cuda.empty_cache()
        
        return results
    
    @contextmanager
    def _benchmark_context(self):
        """åŸºå‡†æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        # é¢„çƒ­ GPU
        torch.cuda.synchronize()
        
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        yield
        
        # åŒæ­¥å’Œæ¸…ç†
        torch.cuda.synchronize()
    
    def _benchmark_operation(self, operation_func, operation_name: str) -> Tuple[float, float]:
        """åŸºå‡†æµ‹è¯•å•ä¸ªæ“ä½œ"""
        times = []
        
        # å¼€å§‹å†…å­˜ç›‘æ§
        self.memory_monitor.start_monitoring()
        
        with self._benchmark_context():
            # é¢„çƒ­
            for _ in range(self.config.warmup_iterations):
                with torch.no_grad():
                    operation_func()
                torch.cuda.synchronize()
            
            # åŸºå‡†æµ‹è¯•
            torch.cuda.synchronize()
            start_time = time.perf_counter()
            
            for _ in range(self.config.benchmark_iterations):
                with torch.no_grad():
                    operation_func()
                torch.cuda.synchronize()
                
                iteration_time = time.perf_counter()
                times.append((iteration_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                start_time = iteration_time
        
        # åœæ­¢å†…å­˜ç›‘æ§
        avg_memory = self.memory_monitor.stop_monitoring()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = np.mean(times)
        
        return avg_time, avg_memory
    
    def _calculate_performance_metrics(self, operation_name: str, configuration: Dict[str, Any],
                                     mirage_time: float, yica_time: float,
                                     mirage_memory: float, yica_memory: float,
                                     operation_count: int) -> PerformanceMetrics:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        
        # å»¶è¿ŸæŒ‡æ ‡
        latency_speedup = mirage_time / yica_time if yica_time > 0 else 1.0
        
        # ååé‡æŒ‡æ ‡ (ops/sec)
        mirage_throughput = operation_count / (mirage_time / 1000) if mirage_time > 0 else 0
        yica_throughput = operation_count / (yica_time / 1000) if yica_time > 0 else 0
        throughput_speedup = yica_throughput / mirage_throughput if mirage_throughput > 0 else 1.0
        
        # å†…å­˜æ•ˆç‡
        memory_efficiency = mirage_memory / yica_memory if yica_memory > 0 else 1.0
        
        # èƒ½è€—ä¼°ç®— (ç®€åŒ–æ¨¡å‹: åŠŸè€— âˆ æ—¶é—´ Ã— å†…å­˜ä½¿ç”¨)
        mirage_energy = mirage_time * mirage_memory * 0.001  # ç®€åŒ–å•ä½
        yica_energy = yica_time * yica_memory * 0.001
        energy_efficiency = mirage_energy / yica_energy if yica_energy > 0 else 1.0
        
        return PerformanceMetrics(
            operation_name=operation_name,
            configuration=configuration,
            mirage_latency=mirage_time,
            yica_latency=yica_time,
            latency_speedup=latency_speedup,
            mirage_throughput=mirage_throughput,
            yica_throughput=yica_throughput,
            throughput_speedup=throughput_speedup,
            mirage_memory=mirage_memory,
            yica_memory=yica_memory,
            memory_efficiency=memory_efficiency,
            mirage_energy=mirage_energy,
            yica_energy=yica_energy,
            energy_efficiency=energy_efficiency,
            std_dev_mirage=0.0,  # ç®€åŒ–å®ç°
            std_dev_yica=0.0,
            confidence_interval=0.95
        )
    
    def _yica_optimized_matmul(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """YICA ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ YICA çš„ CIM é˜µåˆ—åŠ é€ŸçŸ©é˜µä¹˜æ³•
        # ç°åœ¨ä½¿ç”¨æ ‡å‡†å®ç°æ¨¡æ‹Ÿï¼Œä½†å‡è®¾æœ‰æ€§èƒ½æå‡
        result = torch.matmul(A, B)
        
        # æ¨¡æ‹Ÿ YICA CIM é˜µåˆ—çš„é¢å¤–ä¼˜åŒ–
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯ç¡¬ä»¶åŠ é€Ÿçš„ YIS æŒ‡ä»¤
        
        return result
    
    def _allocate_yica_optimized_tensor(self, rows: int, cols: int) -> torch.Tensor:
        """åˆ†é… YICA ä¼˜åŒ–çš„å¼ é‡ï¼ˆæ¨¡æ‹Ÿ SPM ä¼˜åŒ–ï¼‰"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨ SPM å†…å­˜åˆ†é…å™¨
        # ç°åœ¨ä½¿ç”¨æ ‡å‡†åˆ†é…æ¨¡æ‹Ÿï¼Œä½†å‡è®¾å†…å­˜æ•ˆç‡æ›´é«˜
        tensor = torch.randn(rows, cols, dtype=self.config.dtype, device=self.config.device)
        
        # æ¨¡æ‹Ÿ SPM å†…å­˜ä¼˜åŒ–çš„é¢å¤–å¤„ç†
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ¶‰åŠ SPM å†…å­˜å¸ƒå±€ä¼˜åŒ–
        
        return tensor
    
    def generate_comprehensive_report(self, all_results: Dict[str, List[PerformanceMetrics]]):
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š...")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ° JSON
        if self.config.save_results:
            results_data = {}
            for category, results in all_results.items():
                results_data[category] = [asdict(metric) for metric in results]
            
            with open(os.path.join(self.config.results_dir, 'detailed_results.json'), 'w') as f:
                json.dump(results_data, f, indent=2)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_report = self._generate_summary_report(all_results)
        
        if self.config.save_results:
            with open(os.path.join(self.config.results_dir, 'summary_report.txt'), 'w') as f:
                f.write(summary_report)
        
        print(summary_report)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        if self.config.generate_plots:
            self._generate_performance_plots(all_results)
    
    def _generate_summary_report(self, all_results: Dict[str, List[PerformanceMetrics]]) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("YICA-Mirage ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        all_metrics = []
        for results in all_results.values():
            all_metrics.extend(results)
        
        if all_metrics:
            avg_latency_speedup = np.mean([m.latency_speedup for m in all_metrics])
            avg_throughput_speedup = np.mean([m.throughput_speedup for m in all_metrics])
            avg_memory_efficiency = np.mean([m.memory_efficiency for m in all_metrics])
            avg_energy_efficiency = np.mean([m.energy_efficiency for m in all_metrics])
            
            report.append("ğŸ“ˆ æ€»ä½“æ€§èƒ½æå‡:")
            report.append(f"  â€¢ å¹³å‡å»¶è¿ŸåŠ é€Ÿæ¯”: {avg_latency_speedup:.2f}x")
            report.append(f"  â€¢ å¹³å‡ååé‡æå‡: {avg_throughput_speedup:.2f}x")
            report.append(f"  â€¢ å¹³å‡å†…å­˜æ•ˆç‡æå‡: {avg_memory_efficiency:.2f}x")
            report.append(f"  â€¢ å¹³å‡èƒ½æ•ˆæå‡: {avg_energy_efficiency:.2f}x")
            report.append("")
        
        # å„ç±»æ“ä½œè¯¦ç»†ç»Ÿè®¡
        for category, results in all_results.items():
            if not results:
                continue
                
            report.append(f"ğŸ” {category.upper()} æ€§èƒ½åˆ†æ:")
            
            category_speedups = [r.latency_speedup for r in results]
            best_speedup = max(category_speedups)
            worst_speedup = min(category_speedups)
            avg_speedup = np.mean(category_speedups)
            
            report.append(f"  â€¢ æœ€ä½³åŠ é€Ÿæ¯”: {best_speedup:.2f}x")
            report.append(f"  â€¢ æœ€å·®åŠ é€Ÿæ¯”: {worst_speedup:.2f}x") 
            report.append(f"  â€¢ å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
            
            # æ˜¾ç¤ºæœ€ä½³é…ç½®
            best_result = max(results, key=lambda r: r.latency_speedup)
            report.append(f"  â€¢ æœ€ä½³é…ç½®: {best_result.configuration}")
            report.append("")
        
        # å»ºè®®å’Œç»“è®º
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if avg_latency_speedup > 2.0:
            report.append("  â€¢ YICA ä¼˜åŒ–æ˜¾è‘—æå‡äº†è®¡ç®—æ€§èƒ½ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²")
        elif avg_latency_speedup > 1.5:
            report.append("  â€¢ YICA ä¼˜åŒ–å¸¦æ¥äº†æ˜æ˜¾çš„æ€§èƒ½æå‡ï¼Œé€‚åˆæ€§èƒ½æ•æ„Ÿçš„åº”ç”¨")
        else:
            report.append("  â€¢ YICA ä¼˜åŒ–å¸¦æ¥äº†ä¸€å®šçš„æ€§èƒ½æå‡ï¼Œå¯è€ƒè™‘ç‰¹å®šåœºæ™¯åº”ç”¨")
        
        if avg_memory_efficiency > 1.2:
            report.append("  â€¢ å†…å­˜ä½¿ç”¨æ•ˆç‡æ˜¾è‘—æ”¹å–„ï¼Œæœ‰åŠ©äºå¤„ç†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹")
        
        if avg_energy_efficiency > 1.5:
            report.append("  â€¢ èƒ½æ•ˆæ¯”å¤§å¹…æå‡ï¼Œæœ‰åˆ©äºé™ä½è¿è¡Œæˆæœ¬å’Œç¯å¢ƒå½±å“")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def _generate_performance_plots(self, all_results: Dict[str, List[PerformanceMetrics]]):
        """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('YICA-Mirage æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ', fontsize=16, fontweight='bold')
        
        # 1. å»¶è¿ŸåŠ é€Ÿæ¯”å¯¹æ¯”
        categories = list(all_results.keys())
        speedups = []
        for category in categories:
            if all_results[category]:
                avg_speedup = np.mean([r.latency_speedup for r in all_results[category]])
                speedups.append(avg_speedup)
            else:
                speedups.append(1.0)
        
        axes[0, 0].bar(categories, speedups, color='skyblue', alpha=0.8)
        axes[0, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿')
        axes[0, 0].set_title('å¹³å‡å»¶è¿ŸåŠ é€Ÿæ¯”')
        axes[0, 0].set_ylabel('åŠ é€Ÿæ¯”')
        axes[0, 0].legend()
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. å†…å­˜æ•ˆç‡æå‡
        memory_efficiencies = []
        for category in categories:
            if all_results[category]:
                avg_efficiency = np.mean([r.memory_efficiency for r in all_results[category]])
                memory_efficiencies.append(avg_efficiency)
            else:
                memory_efficiencies.append(1.0)
        
        axes[0, 1].bar(categories, memory_efficiencies, color='lightgreen', alpha=0.8)
        axes[0, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿')
        axes[0, 1].set_title('å¹³å‡å†…å­˜æ•ˆç‡æå‡')
        axes[0, 1].set_ylabel('æ•ˆç‡æ¯”')
        axes[0, 1].legend()
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. ååé‡æå‡æ•£ç‚¹å›¾
        if 'matmul' in all_results and all_results['matmul']:
            matmul_results = all_results['matmul']
            matrix_sizes = [r.configuration.get('M', 0) * r.configuration.get('N', 0) 
                          for r in matmul_results]
            throughput_speedups = [r.throughput_speedup for r in matmul_results]
            
            axes[1, 0].scatter(matrix_sizes, throughput_speedups, alpha=0.7, color='orange')
            axes[1, 0].set_title('çŸ©é˜µä¹˜æ³•ååé‡æå‡ vs çŸ©é˜µå¤§å°')
            axes[1, 0].set_xlabel('çŸ©é˜µå¤§å° (MÃ—N)')
            axes[1, 0].set_ylabel('ååé‡æå‡æ¯”')
            axes[1, 0].set_xscale('log')
        
        # 4. èƒ½æ•ˆæ¯”æå‡
        energy_efficiencies = []
        for category in categories:
            if all_results[category]:
                avg_energy_eff = np.mean([r.energy_efficiency for r in all_results[category]])
                energy_efficiencies.append(avg_energy_eff)
            else:
                energy_efficiencies.append(1.0)
        
        axes[1, 1].bar(categories, energy_efficiencies, color='lightcoral', alpha=0.8)
        axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿')
        axes[1, 1].set_title('å¹³å‡èƒ½æ•ˆæ¯”æå‡')
        axes[1, 1].set_ylabel('èƒ½æ•ˆæ¯”')
        axes[1, 1].legend()
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if self.config.save_results:
            plt.savefig(os.path.join(self.config.results_dir, 'performance_comparison.png'), 
                       dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ° {self.config.results_dir}/performance_comparison.png")
        
        plt.show()


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œ YICA ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ å¯åŠ¨ YICA-Mirage ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
    
    # é…ç½®åŸºå‡†æµ‹è¯•
    config = BenchmarkConfig(
        warmup_iterations=5,
        benchmark_iterations=50,
        batch_sizes=[1, 4, 8],
        sequence_lengths=[128, 512, 1024],
        hidden_sizes=[768, 1024, 2048],
        device='cuda' if torch.cuda.is_available() else 'cpu',
        dtype=torch.float16,
        save_results=True,
        results_dir='yica_benchmark_results',
        generate_plots=True
    )
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  â€¢ è®¾å¤‡: {config.device}")
    print(f"  â€¢ æ•°æ®ç±»å‹: {config.dtype}")
    print(f"  â€¢ é¢„çƒ­è¿­ä»£: {config.warmup_iterations}")
    print(f"  â€¢ åŸºå‡†è¿­ä»£: {config.benchmark_iterations}")
    print(f"  â€¢ ç»“æœä¿å­˜: {config.save_results}")
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = YICABenchmarkSuite(config)
    
    # è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
    try:
        results = benchmark_suite.run_comprehensive_benchmark()
        
        print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {config.results_dir}/")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        all_metrics = []
        for category_results in results.values():
            all_metrics.extend(category_results)
        
        if all_metrics:
            avg_speedup = np.mean([m.latency_speedup for m in all_metrics])
            max_speedup = max([m.latency_speedup for m in all_metrics])
            avg_memory_eff = np.mean([m.memory_efficiency for m in all_metrics])
            
            print(f"\nğŸ¯ å…³é”®æ€§èƒ½æŒ‡æ ‡:")
            print(f"  â€¢ å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
            print(f"  â€¢ æœ€å¤§åŠ é€Ÿæ¯”: {max_speedup:.2f}x")
            print(f"  â€¢ å¹³å‡å†…å­˜æ•ˆç‡æå‡: {avg_memory_eff:.2f}x")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 