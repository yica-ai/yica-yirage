#!/usr/bin/env python3
"""
YICA å†…æ ¸ç”Ÿæˆå™¨æ¼”ç¤º

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº† YICA å†…æ ¸ç”Ÿæˆå™¨çš„å„ç§åŠŸèƒ½ï¼š
1. ä¸åŒç±»å‹å†…æ ¸æ¨¡æ¿çš„ç”Ÿæˆ
2. CIM é˜µåˆ—ä¼˜åŒ–å’Œ SPM å†…å­˜ä¼˜åŒ–
3. å†…æ ¸èåˆå’Œæ€§èƒ½åˆ†æ
4. è‡ªåŠ¨è°ƒä¼˜å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import os
import sys
import time
import json
from typing import Dict, List, Any, Tuple
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict

# æ·»åŠ  Mirage è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from mirage.yica.config import YICAConfig


@dataclass
class KernelBenchmarkConfig:
    """å†…æ ¸åŸºå‡†æµ‹è¯•é…ç½®"""
    kernel_types: List[str]
    input_shapes: List[Tuple[int, ...]]
    batch_sizes: List[int]
    precisions: List[str]
    num_iterations: int = 100
    warmup_iterations: int = 10


@dataclass
class KernelGenerationResult:
    """å†…æ ¸ç”Ÿæˆç»“æœ"""
    kernel_name: str
    kernel_type: str
    yis_code: str
    triton_code: str
    
    # æ€§èƒ½é¢„æµ‹
    estimated_latency: float
    estimated_throughput: float
    memory_footprint: int
    cim_utilization: float
    spm_utilization: float
    
    # èµ„æºä½¿ç”¨
    cim_arrays_used: int
    spm_memory_used: int
    register_count: int
    instruction_count: int
    
    # ä¼˜åŒ–ä¿¡æ¯
    optimization_log: List[str]
    generation_successful: bool
    error_message: str = ""


class YICAKernelGeneratorDemo:
    """YICA å†…æ ¸ç”Ÿæˆå™¨æ¼”ç¤ºç±»"""
    
    def __init__(self):
        # YICA ç¡¬ä»¶é…ç½®
        self.yica_config = YICAConfig(
            num_cim_arrays=32,
            spm_size_per_die=256 * 1024 * 1024,  # 256MB
            dram_size_per_cluster=16 * 1024 * 1024 * 1024,  # 16GB
            enable_quantization=True,
            target_precision="fp16"
        )
        
        # å†…æ ¸ç”Ÿæˆå™¨ï¼ˆæ¨¡æ‹Ÿï¼‰
        self.kernel_generator = None  # åœ¨å®é™…å®ç°ä¸­ä¼šåˆå§‹åŒ–çœŸå®çš„ç”Ÿæˆå™¨
        
        # ç»“æœå­˜å‚¨
        self.generated_kernels = []
        self.benchmark_results = []
        
    def demonstrate_kernel_templates(self):
        """æ¼”ç¤ºå„ç§å†…æ ¸æ¨¡æ¿"""
        print("ğŸ”§ YICA å†…æ ¸æ¨¡æ¿æ¼”ç¤º")
        print("=" * 60)
        
        # å®šä¹‰æµ‹è¯•ç”¨çš„å†…æ ¸é…ç½®
        kernel_configs = {
            "CIM_MATMUL": {
                "template_type": "CIM_MATMUL",
                "compute_mode": "CIM_PARALLEL",
                "input_shapes": [(1024, 512), (512, 2048)],
                "cim_arrays": 16,
                "spm_size": 64 * 1024 * 1024,  # 64MB
                "optimizations": ["loop_unroll", "instruction_fusion"]
            },
            
            "CIM_CONV2D": {
                "template_type": "CIM_CONV2D",
                "compute_mode": "CIM_PARALLEL",
                "input_shapes": [(32, 3, 224, 224), (64, 3, 7, 7)],
                "cim_arrays": 24,
                "spm_size": 128 * 1024 * 1024,  # 128MB
                "optimizations": ["register_tiling", "vectorization"]
            },
            
            "CIM_ATTENTION": {
                "template_type": "CIM_ATTENTION",
                "compute_mode": "CIM_PARALLEL",
                "input_shapes": [(32, 512, 768)],  # [batch, seq_len, hidden]
                "num_heads": 12,
                "head_dim": 64,
                "cim_arrays": 32,
                "spm_size": 256 * 1024 * 1024,  # 256MB
                "optimizations": ["instruction_fusion", "prefetch"]
            },
            
            "FUSED_MLP": {
                "template_type": "FUSED_MLP",
                "compute_mode": "PIPELINE_FUSION",
                "input_shapes": [(1024, 768), (768, 3072), (3072, 768)],
                "cim_arrays": 20,
                "spm_size": 128 * 1024 * 1024,  # 128MB
                "optimizations": ["vertical_fusion", "double_buffer"]
            },
            
            "SPM_LAYERNORM": {
                "template_type": "SPM_LAYERNORM",
                "compute_mode": "SPM_OPTIMIZED",
                "input_shapes": [(32, 512, 768)],
                "cim_arrays": 8,
                "spm_size": 32 * 1024 * 1024,  # 32MB
                "optimizations": ["cache_locality", "prefetch"]
            }
        }
        
        # ç”Ÿæˆå„ç§ç±»å‹çš„å†…æ ¸
        for kernel_name, config in kernel_configs.items():
            print(f"\nğŸš€ ç”Ÿæˆ {kernel_name} å†…æ ¸...")
            
            result = self._generate_kernel_mock(kernel_name, config)
            self.generated_kernels.append(result)
            
            self._print_kernel_info(result)
            
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(self.generated_kernels)} ä¸ªå†…æ ¸")
    
    def _generate_kernel_mock(self, kernel_name: str, config: Dict[str, Any]) -> KernelGenerationResult:
        """æ¨¡æ‹Ÿå†…æ ¸ç”Ÿæˆï¼ˆå®é™…å®ç°ä¸­ä¼šè°ƒç”¨çœŸå®çš„ç”Ÿæˆå™¨ï¼‰"""
        
        # æ¨¡æ‹Ÿ YIS ä»£ç ç”Ÿæˆ
        yis_code = self._generate_mock_yis_code(kernel_name, config)
        
        # æ¨¡æ‹Ÿ Triton ä»£ç ç”Ÿæˆ
        triton_code = self._generate_mock_triton_code(kernel_name, config)
        
        # æ¨¡æ‹Ÿæ€§èƒ½é¢„æµ‹
        performance = self._predict_mock_performance(config)
        
        # æ¨¡æ‹Ÿèµ„æºåˆ†æ
        resources = self._analyze_mock_resources(config)
        
        return KernelGenerationResult(
            kernel_name=f"yica_{kernel_name.lower()}_kernel",
            kernel_type=kernel_name,
            yis_code=yis_code,
            triton_code=triton_code,
            estimated_latency=performance["latency"],
            estimated_throughput=performance["throughput"],
            memory_footprint=performance["memory"],
            cim_utilization=performance["cim_util"],
            spm_utilization=performance["spm_util"],
            cim_arrays_used=resources["cim_arrays"],
            spm_memory_used=resources["spm_memory"],
            register_count=resources["registers"],
            instruction_count=resources["instructions"],
            optimization_log=config.get("optimizations", []),
            generation_successful=True
        )
    
    def _generate_mock_yis_code(self, kernel_name: str, config: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„ YIS ä»£ç """
        
        yis_templates = {
            "CIM_MATMUL": f"""
// YICA Generated Kernel: {kernel_name}
kernel yica_cim_matmul_kernel {{
  // CIM Array Setup
  cim_init {config['cim_arrays']}
  cim_config array_size 256 256
  cim_enable_pipeline
  cim_set_utilization 0.9
  
  // SPM Memory Setup
  spm_alloc {config['smp_size']}
  spm_strategy locality_first
  spm_enable_prefetch
  spm_enable_double_buffer
  
  // Matrix multiplication loop
  for (int tile_m = 0; tile_m < {config['input_shapes'][0][0]}; tile_m += 32) {{
    for (int tile_n = 0; tile_n < {config['input_shapes'][1][1]}; tile_n += 32) {{
      for (int tile_k = 0; tile_k < {config['input_shapes'][0][1]}; tile_k += 32) {{
        cim_load_tile a_tile, tile_m, tile_k, 32, 32
        cim_load_tile b_tile, tile_k, tile_n, 32, 32
        cim_matmul_tile a_tile, b_tile, c_tile
        cim_accumulate c_result, c_tile
      }}
    }}
  }}
  
  yis_sync
}}""",
            
            "CIM_ATTENTION": f"""
// YICA Generated Kernel: {kernel_name}
kernel yica_cim_attention_kernel {{
  cim_init {config['cim_arrays']}
  spm_alloc {config['spm_size']}
  
  // Q, K, V projection using CIM arrays
  cim_parallel_begin {config.get('num_heads', 12)}
    cim_matmul input_tensor, q_weight, q_projection
    cim_matmul input_tensor, k_weight, k_projection
    cim_matmul input_tensor, v_weight, v_projection
  cim_parallel_end
  
  // Attention score computation
  cim_matmul q_projection, k_projection, attention_scores
  cim_softmax attention_scores
  
  // Output computation
  cim_matmul attention_scores, v_projection, output
  
  yis_sync
}}""",
            
            "FUSED_MLP": f"""
// YICA Generated Kernel: {kernel_name}
kernel yica_fused_mlp_kernel {{
  cim_init {config['cim_arrays']}
  spm_alloc {config['spm_size']}
  
  // Fused MLP computation: Linear -> Activation -> Linear
  spm_load input_tensor, spm_addr_0
  spm_load weight1_tensor, spm_addr_1
  cim_matmul input_tensor, weight1_tensor, hidden_output
  cim_gelu hidden_output  // Fused activation
  spm_load weight2_tensor, spm_addr_2
  cim_matmul hidden_output, weight2_tensor, final_output
  spm_store final_output, spm_addr_3
  
  yis_sync
}}""",
        }
        
        return yis_templates.get(kernel_name, f"// Mock YIS code for {kernel_name}")
    
    def _generate_mock_triton_code(self, kernel_name: str, config: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„ Triton ä»£ç """
        
        return f"""
import triton
import triton.language as tl

@triton.jit
def yica_{kernel_name.lower()}_kernel(
    input_ptr, output_ptr,
    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,
    BLOCK_SIZE_M: tl.constexpr = 128,
    BLOCK_SIZE_N: tl.constexpr = 128,
    BLOCK_SIZE_K: tl.constexpr = 32,
):
    \"\"\"YICA-optimized Triton kernel for {kernel_name}\"\"\"
    
    # Get program IDs
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Simulate YICA CIM array computation
    # Using {config['cim_arrays']} CIM arrays
    # SPM size: {config['spm_size'] // (1024*1024)}MB
    
    # Compute offsets
    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    # Load and compute (YICA CIM simulation)
    input_ptrs = input_ptr + offs_m[:, None] * N + offs_n[None, :]
    output_ptrs = output_ptr + offs_m[:, None] * N + offs_n[None, :]
    
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    input_data = tl.load(input_ptrs, mask=mask)
    
    # YICA-specific optimizations: {', '.join(config.get('optimizations', []))}
    result = input_data  # Placeholder computation
    
    tl.store(output_ptrs, result, mask=mask)
"""
    
    def _predict_mock_performance(self, config: Dict[str, Any]) -> Dict[str, float]:
        """æ¨¡æ‹Ÿæ€§èƒ½é¢„æµ‹"""
        
        # åŸºäºé…ç½®å‚æ•°çš„ç®€åŒ–æ€§èƒ½æ¨¡å‹
        base_latency = 1.0  # ms
        cim_speedup = min(config['cim_arrays'] / 8.0, 4.0)  # æœ€å¤š4å€åŠ é€Ÿ
        spm_speedup = (config['spm_size'] / (64 * 1024 * 1024)) * 0.2 + 0.8  # SPM å¤§å°å½±å“
        
        # ä¼˜åŒ–å¸¦æ¥çš„åŠ é€Ÿ
        opt_speedup = 1.0
        for opt in config.get('optimizations', []):
            if opt in ['loop_unroll', 'instruction_fusion']:
                opt_speedup *= 1.15
            elif opt in ['register_tiling', 'vectorization']:
                opt_speedup *= 1.25
            elif opt in ['prefetch', 'double_buffer']:
                opt_speedup *= 1.1
        
        total_speedup = cim_speedup * spm_speedup * opt_speedup
        estimated_latency = base_latency / total_speedup
        
        return {
            "latency": estimated_latency,
            "throughput": 1000.0 / estimated_latency,  # GFLOPS
            "memory": config['spm_size'],
            "cim_util": min(0.95, 0.6 + config['cim_arrays'] * 0.01),
            "smp_util": min(0.90, 0.5 + (config['spm_size'] / (128 * 1024 * 1024)) * 0.3)
        }
    
    def _analyze_mock_resources(self, config: Dict[str, Any]) -> Dict[str, int]:
        """æ¨¡æ‹Ÿèµ„æºåˆ†æ"""
        
        return {
            "cim_arrays": config['cim_arrays'],
            "spm_memory": config['spm_size'],
            "registers": config['cim_arrays'] * 32,  # æ¯ä¸ªé˜µåˆ— 32 ä¸ªå¯„å­˜å™¨
            "instructions": len(config.get('optimizations', [])) * 50 + 100
        }
    
    def _print_kernel_info(self, result: KernelGenerationResult):
        """æ‰“å°å†…æ ¸ä¿¡æ¯"""
        
        print(f"  ğŸ“Š å†…æ ¸: {result.kernel_name}")
        print(f"     ç±»å‹: {result.kernel_type}")
        print(f"     é¢„ä¼°å»¶è¿Ÿ: {result.estimated_latency:.3f} ms")
        print(f"     é¢„ä¼°ååé‡: {result.estimated_throughput:.1f} GFLOPS")
        print(f"     å†…å­˜å ç”¨: {result.memory_footprint // (1024*1024)} MB")
        print(f"     CIM åˆ©ç”¨ç‡: {result.cim_utilization:.1%}")
        print(f"     SPM åˆ©ç”¨ç‡: {result.spm_utilization:.1%}")
        print(f"     ä½¿ç”¨ CIM é˜µåˆ—: {result.cim_arrays_used}")
        print(f"     æŒ‡ä»¤æ•°é‡: {result.instruction_count}")
        print(f"     ä¼˜åŒ–ç­–ç•¥: {', '.join(result.optimization_log)}")
    
    def demonstrate_kernel_fusion(self):
        """æ¼”ç¤ºå†…æ ¸èåˆåŠŸèƒ½"""
        print("\nğŸ”€ å†…æ ¸èåˆæ¼”ç¤º")
        print("=" * 60)
        
        # é€‰æ‹©å‡ ä¸ªå†…æ ¸è¿›è¡Œèåˆ
        if len(self.generated_kernels) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ä¸ªå†…æ ¸æ‰èƒ½æ¼”ç¤ºèåˆ")
            return
        
        # èåˆåœºæ™¯1ï¼šMLP å±‚èåˆ
        print("ğŸ§© åœºæ™¯1: MLP å±‚èåˆ (Linear + Activation + Linear)")
        
        mlp_kernels = [k for k in self.generated_kernels if 'MLP' in k.kernel_type or 'MATMUL' in k.kernel_type]
        if len(mlp_kernels) >= 2:
            fused_result = self._simulate_kernel_fusion(mlp_kernels[:2], "VERTICAL_FUSION")
            print(f"  èåˆå‰å»¶è¿Ÿ: {sum(k.estimated_latency for k in mlp_kernels[:2]):.3f} ms")
            print(f"  èåˆåå»¶è¿Ÿ: {fused_result['latency']:.3f} ms")
            print(f"  æ€§èƒ½æå‡: {fused_result['speedup']:.2f}x")
            print(f"  å†…å­˜èŠ‚çœ: {fused_result['memory_savings'] // (1024*1024)} MB")
        
        # èåˆåœºæ™¯2ï¼šæ³¨æ„åŠ›æœºåˆ¶èåˆ
        print("\nğŸ§© åœºæ™¯2: æ³¨æ„åŠ›æœºåˆ¶èåˆ (Q/K/V + Attention + Output)")
        
        attention_kernels = [k for k in self.generated_kernels if 'ATTENTION' in k.kernel_type]
        if attention_kernels:
            fused_result = self._simulate_kernel_fusion([attention_kernels[0]], "HORIZONTAL_FUSION")
            print(f"  èåˆç­–ç•¥: æ°´å¹³èåˆ (å¹¶è¡Œè®¡ç®— Q/K/V)")
            print(f"  é¢„ä¼°åŠ é€Ÿæ¯”: {fused_result['speedup']:.2f}x")
            print(f"  CIM é˜µåˆ—åˆ©ç”¨ç‡æå‡: {fused_result['utilization_improvement']:.1%}")
    
    def _simulate_kernel_fusion(self, kernels: List[KernelGenerationResult], 
                               fusion_type: str) -> Dict[str, float]:
        """æ¨¡æ‹Ÿå†…æ ¸èåˆ"""
        
        total_latency = sum(k.estimated_latency for k in kernels)
        total_memory = sum(k.memory_footprint for k in kernels)
        
        # èåˆå¸¦æ¥çš„ä¼˜åŒ–
        if fusion_type == "VERTICAL_FUSION":
            # å‚ç›´èåˆï¼šå‡å°‘ä¸­é—´ç»“æœå­˜å‚¨
            fusion_speedup = 1.3 + len(kernels) * 0.1
            memory_savings = total_memory * 0.25  # èŠ‚çœ25%å†…å­˜
            
        elif fusion_type == "HORIZONTAL_FUSION":
            # æ°´å¹³èåˆï¼šæé«˜å¹¶è¡Œåº¦
            fusion_speedup = 1.2 + len(kernels) * 0.05
            memory_savings = total_memory * 0.15  # èŠ‚çœ15%å†…å­˜
            
        else:
            fusion_speedup = 1.1
            memory_savings = total_memory * 0.1
        
        fused_latency = total_latency / fusion_speedup
        utilization_improvement = (fusion_speedup - 1.0) * 0.2  # åˆ©ç”¨ç‡æå‡
        
        return {
            "latency": fused_latency,
            "speedup": fusion_speedup,
            "memory_savings": memory_savings,
            "utilization_improvement": utilization_improvement
        }
    
    def run_performance_benchmark(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        # å®šä¹‰åŸºå‡†æµ‹è¯•é…ç½®
        benchmark_config = KernelBenchmarkConfig(
            kernel_types=["CIM_MATMUL", "CIM_ATTENTION", "FUSED_MLP"],
            input_shapes=[(512, 512), (1024, 1024), (2048, 2048)],
            batch_sizes=[1, 8, 32],
            precisions=["fp16", "fp32"],
            num_iterations=50,
            warmup_iterations=5
        )
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        for kernel_type in benchmark_config.kernel_types:
            print(f"\nğŸ”¬ æµ‹è¯• {kernel_type} å†…æ ¸...")
            
            kernels = [k for k in self.generated_kernels if k.kernel_type == kernel_type]
            if not kernels:
                print(f"  âš ï¸  æœªæ‰¾åˆ° {kernel_type} ç±»å‹çš„å†…æ ¸")
                continue
            
            kernel = kernels[0]
            
            # ä¸åŒè¾“å…¥å¤§å°çš„æ€§èƒ½æµ‹è¯•
            for shape in benchmark_config.input_shapes:
                for batch_size in benchmark_config.batch_sizes:
                    for precision in benchmark_config.precisions:
                        
                        # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•
                        result = self._run_mock_benchmark(kernel, shape, batch_size, precision)
                        self.benchmark_results.append(result)
                        
                        print(f"    ğŸ“Š {shape} x {batch_size} ({precision}): "
                              f"{result['latency']:.3f}ms, "
                              f"{result['throughput']:.1f} GFLOPS, "
                              f"æ•ˆç‡: {result['efficiency']:.1%}")
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self._generate_performance_report()
    
    def _run_mock_benchmark(self, kernel: KernelGenerationResult, 
                           shape: Tuple[int, int], batch_size: int, 
                           precision: str) -> Dict[str, float]:
        """æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•è¿è¡Œ"""
        
        # åŸºäºå‚æ•°çš„æ€§èƒ½æ¨¡æ‹Ÿ
        base_latency = kernel.estimated_latency
        
        # è¾“å…¥å¤§å°å½±å“
        size_factor = (shape[0] * shape[1]) / (1024 * 1024)
        size_latency = base_latency * size_factor
        
        # æ‰¹æ¬¡å¤§å°å½±å“
        batch_latency = size_latency * (1.0 + (batch_size - 1) * 0.8)  # æ‰¹æ¬¡å¹¶è¡Œæ•ˆç‡
        
        # ç²¾åº¦å½±å“
        precision_factor = 1.0 if precision == "fp16" else 1.3  # fp32 æ›´æ…¢
        final_latency = batch_latency * precision_factor
        
        # è®¡ç®—ååé‡
        flops = 2 * shape[0] * shape[1] * batch_size  # ç®€åŒ–çš„ FLOPS è®¡ç®—
        throughput = flops / (final_latency / 1000) / 1e9  # GFLOPS
        
        # æ•ˆç‡è®¡ç®—
        theoretical_peak = kernel.estimated_throughput * 2  # å‡è®¾çš„ç†è®ºå³°å€¼
        efficiency = throughput / theoretical_peak
        
        return {
            "kernel_type": kernel.kernel_type,
            "shape": shape,
            "batch_size": batch_size,
            "precision": precision,
            "latency": final_latency,
            "throughput": throughput,
            "efficiency": efficiency,
            "flops": flops
        }
    
    def _generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        if not self.benchmark_results:
            print("âŒ æ²¡æœ‰åŸºå‡†æµ‹è¯•ç»“æœ")
            return
        
        # æŒ‰å†…æ ¸ç±»å‹åˆ†ç»„ç»Ÿè®¡
        kernel_stats = {}
        for result in self.benchmark_results:
            kernel_type = result['kernel_type']
            if kernel_type not in kernel_stats:
                kernel_stats[kernel_type] = {
                    'latencies': [],
                    'throughputs': [],
                    'efficiencies': []
                }
            
            kernel_stats[kernel_type]['latencies'].append(result['latency'])
            kernel_stats[kernel_type]['throughputs'].append(result['throughput'])
            kernel_stats[kernel_type]['efficiencies'].append(result['efficiency'])
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        for kernel_type, stats in kernel_stats.items():
            print(f"\nğŸ”§ {kernel_type} ç»Ÿè®¡:")
            print(f"  å¹³å‡å»¶è¿Ÿ: {np.mean(stats['latencies']):.3f} Â± {np.std(stats['latencies']):.3f} ms")
            print(f"  å¹³å‡ååé‡: {np.mean(stats['throughputs']):.1f} Â± {np.std(stats['throughputs']):.1f} GFLOPS")
            print(f"  å¹³å‡æ•ˆç‡: {np.mean(stats['efficiencies']):.1%} Â± {np.std(stats['efficiencies']):.1%}")
            print(f"  æœ€ä½³æ€§èƒ½: {np.max(stats['throughputs']):.1f} GFLOPS")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_benchmark_results()
    
    def _save_benchmark_results(self):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        
        results_data = {
            'yica_config': asdict(self.yica_config),
            'generated_kernels': [asdict(k) for k in self.generated_kernels],
            'benchmark_results': self.benchmark_results,
            'summary': {
                'total_kernels': len(self.generated_kernels),
                'total_benchmarks': len(self.benchmark_results),
                'avg_cim_utilization': np.mean([k.cim_utilization for k in self.generated_kernels]),
                'avg_spm_utilization': np.mean([k.spm_utilization for k in self.generated_kernels])
            }
        }
        
        with open('yica_kernel_generator_results.json', 'w') as f:
            json.dump(results_data, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: yica_kernel_generator_results.json")
    
    def demonstrate_auto_tuning(self):
        """æ¼”ç¤ºè‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½"""
        print("\nğŸ¯ è‡ªåŠ¨è°ƒä¼˜æ¼”ç¤º")
        print("=" * 60)
        
        # é€‰æ‹©ä¸€ä¸ªå†…æ ¸è¿›è¡Œè°ƒä¼˜
        if not self.generated_kernels:
            print("âŒ æ²¡æœ‰å¯è°ƒä¼˜çš„å†…æ ¸")
            return
        
        kernel = self.generated_kernels[0]
        print(f"ğŸ”§ è°ƒä¼˜å†…æ ¸: {kernel.kernel_name}")
        
        # å®šä¹‰è°ƒä¼˜å‚æ•°ç©ºé—´
        tuning_space = {
            'cim_arrays': [8, 16, 24, 32],
            'tile_size': [16, 32, 64, 128],
            'spm_allocation': [32, 64, 128, 256],  # MB
            'optimization_level': [1, 2, 3]
        }
        
        print("ğŸ“Š è°ƒä¼˜å‚æ•°ç©ºé—´:")
        for param, values in tuning_space.items():
            print(f"  {param}: {values}")
        
        # æ¨¡æ‹Ÿè‡ªåŠ¨è°ƒä¼˜è¿‡ç¨‹
        best_config = None
        best_performance = 0
        
        print("\nğŸ” è°ƒä¼˜è¿‡ç¨‹:")
        
        for i, cim_arrays in enumerate([8, 16, 24, 32]):
            for j, tile_size in enumerate([32, 64, 128]):
                # æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•
                config = {
                    'cim_arrays': cim_arrays,
                    'tile_size': tile_size,
                    'smp_allocation': 64,
                    'optimization_level': 2
                }
                
                # ç®€åŒ–çš„æ€§èƒ½æ¨¡æ‹Ÿ
                performance = self._simulate_tuning_performance(config)
                
                print(f"  é…ç½® {i*3+j+1}: CIM={cim_arrays}, Tile={tile_size} -> "
                      f"{performance:.1f} GFLOPS")
                
                if performance > best_performance:
                    best_performance = performance
                    best_config = config
        
        print(f"\nğŸ† æœ€ä½³é…ç½®:")
        for param, value in best_config.items():
            print(f"  {param}: {value}")
        print(f"  æœ€ä½³æ€§èƒ½: {best_performance:.1f} GFLOPS")
        print(f"  ç›¸æ¯”é»˜è®¤é…ç½®æå‡: {(best_performance / kernel.estimated_throughput - 1) * 100:.1f}%")
    
    def _simulate_tuning_performance(self, config: Dict[str, Any]) -> float:
        """æ¨¡æ‹Ÿè°ƒä¼˜æ€§èƒ½æµ‹è¯•"""
        
        base_performance = 100.0  # GFLOPS
        
        # CIM é˜µåˆ—æ•°å½±å“
        cim_factor = min(config['cim_arrays'] / 16.0, 2.0)
        
        # åˆ†å—å¤§å°å½±å“
        tile_factor = 1.0
        if config['tile_size'] == 32:
            tile_factor = 1.1
        elif config['tile_size'] == 64:
            tile_factor = 1.2
        elif config['tile_size'] == 128:
            tile_factor = 1.0
        
        # SPM åˆ†é…å½±å“
        spm_factor = 1.0 + (config['smp_allocation'] - 32) / 128 * 0.2
        
        # ä¼˜åŒ–çº§åˆ«å½±å“
        opt_factor = 1.0 + config['optimization_level'] * 0.1
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§æ¨¡æ‹Ÿå®é™…æµ‹è¯•çš„å˜åŒ–
        noise = 1.0 + (np.random.random() - 0.5) * 0.1
        
        return base_performance * cim_factor * tile_factor * spm_factor * opt_factor * noise
    
    def generate_visualization_report(self):
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")
        print("=" * 60)
        
        if not self.generated_kernels or not self.benchmark_results:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")
            return
        
        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('YICA å†…æ ¸ç”Ÿæˆå™¨æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
        
        # å›¾1: å†…æ ¸ç±»å‹æ€§èƒ½å¯¹æ¯”
        kernel_types = [k.kernel_type for k in self.generated_kernels]
        throughputs = [k.estimated_throughput for k in self.generated_kernels]
        
        ax1.bar(kernel_types, throughputs, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])
        ax1.set_title('å„å†…æ ¸ç±»å‹æ€§èƒ½å¯¹æ¯”')
        ax1.set_ylabel('ååé‡ (GFLOPS)')
        ax1.tick_params(axis='x', rotation=45)
        
        # å›¾2: CIM é˜µåˆ—åˆ©ç”¨ç‡
        cim_utils = [k.cim_utilization for k in self.generated_kernels]
        ax2.hist(cim_utils, bins=10, color='#74B9FF', alpha=0.7, edgecolor='black')
        ax2.set_title('CIM é˜µåˆ—åˆ©ç”¨ç‡åˆ†å¸ƒ')
        ax2.set_xlabel('åˆ©ç”¨ç‡')
        ax2.set_ylabel('å†…æ ¸æ•°é‡')
        
        # å›¾3: å†…å­˜å ç”¨åˆ†æ
        memory_usage = [k.memory_footprint / (1024*1024) for k in self.generated_kernels]  # MB
        ax3.scatter(throughputs, memory_usage, c=cim_utils, cmap='viridis', s=100, alpha=0.7)
        ax3.set_title('æ€§èƒ½ vs å†…å­˜å ç”¨')
        ax3.set_xlabel('ååé‡ (GFLOPS)')
        ax3.set_ylabel('å†…å­˜å ç”¨ (MB)')
        cbar = plt.colorbar(ax3.collections[0], ax=ax3)
        cbar.set_label('CIM åˆ©ç”¨ç‡')
        
        # å›¾4: åŸºå‡†æµ‹è¯•ç»“æœè¶‹åŠ¿
        if self.benchmark_results:
            # æŒ‰è¾“å…¥å¤§å°åˆ†ç»„
            size_groups = {}
            for result in self.benchmark_results:
                size = result['shape'][0] * result['shape'][1]
                if size not in size_groups:
                    size_groups[size] = []
                size_groups[size].append(result['throughput'])
            
            sizes = sorted(size_groups.keys())
            avg_throughputs = [np.mean(size_groups[size]) for size in sizes]
            std_throughputs = [np.std(size_groups[size]) for size in sizes]
            
            ax4.errorbar(sizes, avg_throughputs, yerr=std_throughputs, 
                        marker='o', capsize=5, capthick=2, linewidth=2)
            ax4.set_title('ä¸åŒè¾“å…¥å¤§å°çš„æ€§èƒ½è¡¨ç°')
            ax4.set_xlabel('è¾“å…¥å¤§å° (å…ƒç´ æ•°)')
            ax4.set_ylabel('å¹³å‡ååé‡ (GFLOPS)')
            ax4.set_xscale('log')
        
        plt.tight_layout()
        plt.savefig('yica_kernel_generator_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: yica_kernel_generator_analysis.png")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ YICA å†…æ ¸ç”Ÿæˆå™¨æ¼”ç¤ºå¯åŠ¨")
    print("=" * 80)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = YICAKernelGeneratorDemo()
    
    try:
        # æ¼”ç¤ºå„ç§åŠŸèƒ½
        demo.demonstrate_kernel_templates()
        demo.demonstrate_kernel_fusion()
        demo.run_performance_benchmark()
        demo.demonstrate_auto_tuning()
        demo.generate_visualization_report()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ YICA å†…æ ¸ç”Ÿæˆå™¨æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡ç”Ÿæˆ {len(demo.generated_kernels)} ä¸ªå†…æ ¸")
        print(f"ğŸ“ˆ å®Œæˆ {len(demo.benchmark_results)} é¡¹åŸºå‡†æµ‹è¯•")
        print("ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 