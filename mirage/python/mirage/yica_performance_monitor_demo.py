#!/usr/bin/env python3
"""
YICA æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜æ¼”ç¤º

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº† YICA æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½ï¼š
1. å®æ—¶æ€§èƒ½ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹
2. å¤šç§è‡ªåŠ¨è°ƒä¼˜ç®—æ³•å’Œç­–ç•¥
3. æ€§èƒ½ç“¶é¢ˆåˆ†æå’Œä¼˜åŒ–å»ºè®®
4. å¯è§†åŒ–ä»ªè¡¨æ¿å’ŒæŠ¥å‘Šç”Ÿæˆ
5. å®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
"""

import os
import sys
import time
import json
import threading
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import pandas as pd
import seaborn as sns

# æ·»åŠ  Mirage è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from mirage.yica.config import YICAConfig


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
    type: str
    name: str
    value: float
    unit: str
    timestamp: float
    metadata: Dict[str, str]


@dataclass
class PerformanceAnomaly:
    """æ€§èƒ½å¼‚å¸¸æŠ¥å‘Š"""
    type: str
    description: str
    severity_score: float
    detection_time: float
    related_metrics: List[PerformanceMetric]
    suggested_actions: List[str]


@dataclass
class TuningRecommendation:
    """è°ƒä¼˜å»ºè®®"""
    parameter_name: str
    current_value: str
    recommended_value: str
    expected_improvement: float
    justification: str
    priority: int


@dataclass
class AutoTuningConfig:
    """è‡ªåŠ¨è°ƒä¼˜é…ç½®"""
    enable_aggressive_tuning: bool = False
    improvement_threshold: float = 0.05
    max_tuning_iterations: int = 50
    tuning_interval: int = 300  # seconds
    tunable_parameters: List[str] = None
    parameter_bounds: Dict[str, Tuple[float, float]] = None


class YICAPerformanceMonitorDemo:
    """YICA æ€§èƒ½ç›‘æ§æ¼”ç¤ºç±»"""
    
    def __init__(self):
        # YICA é…ç½®
        self.yica_config = YICAConfig(
            num_cim_arrays=32,
            spm_size_per_die=256 * 1024 * 1024,  # 256MB
            dram_size_per_cluster=16 * 1024 * 1024 * 1024,  # 16GB
            enable_quantization=True,
            target_precision="fp16"
        )
        
        # ç›‘æ§é…ç½®
        self.monitor_config = {
            'enabled_counters': [
                'CIM_UTILIZATION', 'SPM_HIT_RATE', 'DRAM_BANDWIDTH',
                'INSTRUCTION_THROUGHPUT', 'ENERGY_CONSUMPTION', 'TEMPERATURE',
                'MEMORY_USAGE', 'COMPUTE_EFFICIENCY', 'COMMUNICATION_LATENCY'
            ],
            'sampling_interval': 100,  # ms
            'max_history_size': 10000,
            'enable_real_time_analysis': True,
            'enable_anomaly_detection': True,
            'enable_auto_tuning': True
        }
        
        # è‡ªåŠ¨è°ƒä¼˜é…ç½®
        self.auto_tuning_config = AutoTuningConfig(
            enable_aggressive_tuning=False,
            improvement_threshold=0.05,
            max_tuning_iterations=50,
            tuning_interval=300,
            tunable_parameters=[
                'cim_array_count', 'spm_allocation_strategy', 'cache_line_size',
                'prefetch_distance', 'pipeline_depth', 'batch_size'
            ],
            parameter_bounds={
                'cim_array_count': (8, 64),
                'cache_line_size': (32, 128),
                'prefetch_distance': (1, 16),
                'pipeline_depth': (2, 8),
                'batch_size': (1, 128)
            }
        )
        
        # æ•°æ®å­˜å‚¨
        self.performance_history = defaultdict(deque)
        self.anomaly_history = []
        self.tuning_history = []
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring_active = False
        self.monitoring_thread = None
        self.auto_tuning_active = False
        self.auto_tuning_thread = None
        
        # æ€§èƒ½åŸºçº¿
        self.performance_baseline = {}
        
    def demonstrate_real_time_monitoring(self):
        """æ¼”ç¤ºå®æ—¶æ€§èƒ½ç›‘æ§"""
        print("ğŸ“Š YICA å®æ—¶æ€§èƒ½ç›‘æ§æ¼”ç¤º")
        print("=" * 60)
        
        print("ğŸš€ å¯åŠ¨æ€§èƒ½ç›‘æ§...")
        self.start_monitoring()
        
        # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½è¿è¡Œ
        print("âš¡ æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½è¿è¡Œ...")
        self._simulate_workload_execution()
        
        # æ˜¾ç¤ºå®æ—¶ç›‘æ§ç»“æœ
        print("\nğŸ“ˆ å®æ—¶æ€§èƒ½æŒ‡æ ‡:")
        current_metrics = self._get_current_metrics()
        
        for metric in current_metrics:
            status_icon = self._get_metric_status_icon(metric.value, metric.type)
            print(f"  {status_icon} {metric.name}: {metric.value:.2f} {metric.unit}")
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = self._detect_performance_anomalies(current_metrics)
        if anomalies:
            print(f"\nâš ï¸  æ£€æµ‹åˆ° {len(anomalies)} ä¸ªæ€§èƒ½å¼‚å¸¸:")
            for anomaly in anomalies:
                print(f"    ğŸ”´ {anomaly.type}: {anomaly.description}")
                print(f"        ä¸¥é‡ç¨‹åº¦: {anomaly.severity_score:.2f}")
                print(f"        å»ºè®®æªæ–½: {', '.join(anomaly.suggested_actions)}")
        else:
            print("\nâœ… æœªæ£€æµ‹åˆ°æ€§èƒ½å¼‚å¸¸")
        
        time.sleep(2)
        self.stop_monitoring()
        
        print("âœ… å®æ—¶ç›‘æ§æ¼”ç¤ºå®Œæˆ")
    
    def demonstrate_anomaly_detection(self):
        """æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½"""
        print("\nğŸ” æ€§èƒ½å¼‚å¸¸æ£€æµ‹æ¼”ç¤º")
        print("=" * 60)
        
        # ç”ŸæˆåŒ…å«å¼‚å¸¸çš„æ¨¡æ‹Ÿæ•°æ®
        print("ğŸ“Š ç”ŸæˆåŒ…å«å¼‚å¸¸çš„æ€§èƒ½æ•°æ®...")
        
        anomaly_scenarios = [
            {
                'name': 'é«˜å»¶è¿Ÿå¼‚å¸¸',
                'type': 'HIGH_LATENCY',
                'description': 'é€šä¿¡å»¶è¿Ÿå¼‚å¸¸å‡é«˜',
                'severity': 0.8,
                'affected_metrics': ['COMMUNICATION_LATENCY'],
                'trigger_condition': lambda x: x > 50.0  # ms
            },
            {
                'name': 'ä½åˆ©ç”¨ç‡å¼‚å¸¸',
                'type': 'LOW_UTILIZATION',
                'description': 'CIM é˜µåˆ—åˆ©ç”¨ç‡å¼‚å¸¸é™ä½',
                'severity': 0.6,
                'affected_metrics': ['CIM_UTILIZATION'],
                'trigger_condition': lambda x: x < 0.3  # 30%
            },
            {
                'name': 'å†…å­˜æ³„æ¼å¼‚å¸¸',
                'type': 'MEMORY_LEAK',
                'description': 'å†…å­˜ä½¿ç”¨é‡æŒç»­å¢é•¿',
                'severity': 0.9,
                'affected_metrics': ['MEMORY_USAGE'],
                'trigger_condition': lambda x: x > 0.9  # 90%
            },
            {
                'name': 'æ¸©åº¦é™åˆ¶å¼‚å¸¸',
                'type': 'THERMAL_THROTTLING',
                'description': 'èŠ¯ç‰‡æ¸©åº¦è¿‡é«˜å¯¼è‡´æ€§èƒ½ä¸‹é™',
                'severity': 0.7,
                'affected_metrics': ['TEMPERATURE'],
                'trigger_condition': lambda x: x > 85.0  # Â°C
            }
        ]
        
        # æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹
        detected_anomalies = []
        
        for scenario in anomaly_scenarios:
            print(f"\nğŸ”¬ æ£€æµ‹åœºæ™¯: {scenario['name']}")
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            normal_data = np.random.normal(50, 10, 100)  # æ­£å¸¸æ•°æ®
            anomaly_data = self._inject_anomaly(normal_data, scenario)
            
            # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹ç®—æ³•
            anomaly_detected = self._run_anomaly_detection(anomaly_data, scenario)
            
            if anomaly_detected:
                anomaly = PerformanceAnomaly(
                    type=scenario['type'],
                    description=scenario['description'],
                    severity_score=scenario['severity'],
                    detection_time=time.time(),
                    related_metrics=[],
                    suggested_actions=self._generate_anomaly_suggestions(scenario['type'])
                )
                detected_anomalies.append(anomaly)
                
                print(f"    ğŸ”´ å¼‚å¸¸æ£€æµ‹: {scenario['description']}")
                print(f"    ğŸ“Š ä¸¥é‡ç¨‹åº¦: {scenario['severity']:.1f}/1.0")
                print(f"    ğŸ’¡ å»ºè®®æªæ–½: {', '.join(anomaly.suggested_actions)}")
            else:
                print(f"    âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸")
        
        # å¼‚å¸¸ç»Ÿè®¡
        print(f"\nğŸ“ˆ å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡:")
        print(f"  æ€»æ£€æµ‹åœºæ™¯: {len(anomaly_scenarios)}")
        print(f"  æ£€æµ‹åˆ°å¼‚å¸¸: {len(detected_anomalies)}")
        print(f"  æ£€æµ‹å‡†ç¡®ç‡: {len(detected_anomalies)/len(anomaly_scenarios)*100:.1f}%")
        
        self.anomaly_history.extend(detected_anomalies)
        
        print("âœ… å¼‚å¸¸æ£€æµ‹æ¼”ç¤ºå®Œæˆ")
    
    def demonstrate_auto_tuning(self):
        """æ¼”ç¤ºè‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½"""
        print("\nğŸ¯ è‡ªåŠ¨è°ƒä¼˜æ¼”ç¤º")
        print("=" * 60)
        
        # å®šä¹‰è°ƒä¼˜å‚æ•°ç©ºé—´
        tuning_parameters = {
            'cim_array_count': {
                'current': 16,
                'candidates': [8, 12, 16, 20, 24, 32],
                'impact_weight': 0.4
            },
            'spm_allocation_strategy': {
                'current': 'locality_first',
                'candidates': ['locality_first', 'size_first', 'reuse_first', 'hybrid'],
                'impact_weight': 0.25
            },
            'cache_line_size': {
                'current': 64,
                'candidates': [32, 64, 128, 256],
                'impact_weight': 0.15
            },
            'prefetch_distance': {
                'current': 4,
                'candidates': [1, 2, 4, 8, 16],
                'impact_weight': 0.1
            },
            'pipeline_depth': {
                'current': 4,
                'candidates': [2, 3, 4, 5, 6, 8],
                'impact_weight': 0.1
            }
        }
        
        print("ğŸ“Š å½“å‰å‚æ•°é…ç½®:")
        for param_name, param_info in tuning_parameters.items():
            print(f"  {param_name}: {param_info['current']}")
        
        # æµ‹é‡åŸºçº¿æ€§èƒ½
        print("\nğŸ“ˆ æµ‹é‡åŸºçº¿æ€§èƒ½...")
        baseline_performance = self._measure_performance_score(tuning_parameters)
        print(f"  åŸºçº¿æ€§èƒ½å¾—åˆ†: {baseline_performance:.3f}")
        
        # æ¼”ç¤ºä¸åŒè°ƒä¼˜ç®—æ³•
        tuning_algorithms = [
            ('ç½‘æ ¼æœç´¢', self._grid_search_tuning),
            ('éšæœºæœç´¢', self._random_search_tuning),
            ('è´å¶æ–¯ä¼˜åŒ–', self._bayesian_optimization_tuning),
            ('é—ä¼ ç®—æ³•', self._genetic_algorithm_tuning)
        ]
        
        best_overall_config = None
        best_overall_performance = baseline_performance
        
        for algorithm_name, algorithm_func in tuning_algorithms:
            print(f"\nğŸ”§ è¿è¡Œ {algorithm_name}...")
            
            best_config, best_performance, tuning_steps = algorithm_func(
                tuning_parameters, baseline_performance
            )
            
            improvement = (best_performance - baseline_performance) / baseline_performance * 100
            
            print(f"    æœ€ä½³é…ç½®: {best_config}")
            print(f"    æ€§èƒ½å¾—åˆ†: {best_performance:.3f}")
            print(f"    æ€§èƒ½æå‡: {improvement:.1f}%")
            print(f"    è°ƒä¼˜æ­¥æ•°: {tuning_steps}")
            
            if best_performance > best_overall_performance:
                best_overall_performance = best_performance
                best_overall_config = best_config
            
            # è®°å½•è°ƒä¼˜å†å²
            self._record_tuning_attempt(algorithm_name, best_config, 
                                      baseline_performance, best_performance)
        
        # åº”ç”¨æœ€ä½³é…ç½®
        if best_overall_config:
            print(f"\nğŸ† æœ€ä½³æ•´ä½“é…ç½®:")
            for param, value in best_overall_config.items():
                print(f"  {param}: {tuning_parameters[param]['current']} â†’ {value}")
            
            overall_improvement = (best_overall_performance - baseline_performance) / baseline_performance * 100
            print(f"  æ•´ä½“æ€§èƒ½æå‡: {overall_improvement:.1f}%")
            
            # æ¨¡æ‹Ÿåº”ç”¨é…ç½®
            print("âš™ï¸  åº”ç”¨æœ€ä½³é…ç½®...")
            self._apply_tuning_configuration(best_overall_config)
            
        print("âœ… è‡ªåŠ¨è°ƒä¼˜æ¼”ç¤ºå®Œæˆ")
    
    def demonstrate_performance_analysis(self):
        """æ¼”ç¤ºæ€§èƒ½åˆ†æåŠŸèƒ½"""
        print("\nğŸ“Š æ€§èƒ½åˆ†ææ¼”ç¤º")
        print("=" * 60)
        
        # ç”Ÿæˆåˆ†ææ•°æ®
        print("ğŸ“ˆ ç”Ÿæˆæ€§èƒ½åˆ†ææ•°æ®...")
        analysis_data = self._generate_analysis_data()
        
        # ç“¶é¢ˆåˆ†æ
        print("\nğŸ” ç“¶é¢ˆåˆ†æ:")
        bottlenecks = self._identify_bottlenecks(analysis_data)
        
        for i, bottleneck in enumerate(bottlenecks, 1):
            print(f"  {i}. {bottleneck['component']}")
            print(f"     åˆ©ç”¨ç‡: {bottleneck['utilization']:.1%}")
            print(f"     å½±å“å¾—åˆ†: {bottleneck['impact_score']:.2f}")
            print(f"     æ ¹æœ¬åŸå› : {', '.join(bottleneck['causes'])}")
            print(f"     è§£å†³æ–¹æ¡ˆ: {', '.join(bottleneck['solutions'])}")
        
        # æ•ˆç‡åˆ†æ
        print("\nâš¡ æ•ˆç‡åˆ†æ:")
        efficiency_analysis = self._analyze_efficiency(analysis_data)
        
        print(f"  è®¡ç®—æ•ˆç‡: {efficiency_analysis['compute_efficiency']:.1%}")
        print(f"  å†…å­˜æ•ˆç‡: {efficiency_analysis['memory_efficiency']:.1%}")
        print(f"  èƒ½æ•ˆ: {efficiency_analysis['energy_efficiency']:.1%}")
        print(f"  é€šä¿¡æ•ˆç‡: {efficiency_analysis['communication_efficiency']:.1%}")
        print(f"  é™åˆ¶å› å­: {efficiency_analysis['limiting_factor']}")
        
        # è¶‹åŠ¿åˆ†æ
        print("\nğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        trend_analysis = self._analyze_performance_trends(analysis_data)
        
        for trend in trend_analysis:
            direction_icon = "ğŸ“ˆ" if trend['slope'] > 0 else "ğŸ“‰" if trend['slope'] < 0 else "â¡ï¸"
            print(f"  {direction_icon} {trend['metric']}: {trend['direction']}")
            print(f"     è¶‹åŠ¿æ–œç‡: {trend['slope']:.4f}")
            print(f"     ç›¸å…³ç³»æ•°: {trend['correlation']:.3f}")
            print(f"     é¢„æµ‹å‡†ç¡®åº¦: {trend['accuracy']:.1%}")
        
        # ä¼˜åŒ–å»ºè®®
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        optimization_suggestions = self._generate_optimization_suggestions(
            bottlenecks, efficiency_analysis
        )
        
        for i, suggestion in enumerate(optimization_suggestions, 1):
            priority_icon = "ğŸ”´" if suggestion['priority'] >= 8 else "ğŸŸ¡" if suggestion['priority'] >= 5 else "ğŸŸ¢"
            print(f"  {priority_icon} {suggestion['title']}")
            print(f"     é¢„æœŸæå‡: {suggestion['expected_improvement']:.1f}%")
            print(f"     å®æ–½éš¾åº¦: {suggestion['implementation_difficulty']}")
            print(f"     è¯¦ç»†è¯´æ˜: {suggestion['description']}")
        
        print("âœ… æ€§èƒ½åˆ†ææ¼”ç¤ºå®Œæˆ")
    
    def demonstrate_visualization_dashboard(self):
        """æ¼”ç¤ºå¯è§†åŒ–ä»ªè¡¨æ¿"""
        print("\nğŸ“Š æ€§èƒ½å¯è§†åŒ–ä»ªè¡¨æ¿æ¼”ç¤º")
        print("=" * 60)
        
        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–æ•°æ®...")
        viz_data = self._generate_visualization_data()
        
        # åˆ›å»ºæ€§èƒ½ä»ªè¡¨æ¿
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('YICA æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿', fontsize=16, fontweight='bold')
        
        # 1. CIM é˜µåˆ—åˆ©ç”¨ç‡æ—¶é—´åºåˆ—
        ax1 = axes[0, 0]
        times = viz_data['timestamps']
        cim_util = viz_data['cim_utilization']
        ax1.plot(times, cim_util, color='#2E86C1', linewidth=2)
        ax1.fill_between(times, cim_util, alpha=0.3, color='#2E86C1')
        ax1.set_title('CIM é˜µåˆ—åˆ©ç”¨ç‡')
        ax1.set_ylabel('åˆ©ç”¨ç‡ (%)')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 100)
        
        # 2. å†…å­˜ä½¿ç”¨æƒ…å†µ
        ax2 = axes[0, 1]
        memory_data = viz_data['memory_usage']
        labels = ['SPM', 'DRAM', 'æœªä½¿ç”¨']
        colors = ['#E74C3C', '#F39C12', '#95A5A6']
        ax2.pie(memory_data, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        ax2.set_title('å†…å­˜ä½¿ç”¨åˆ†å¸ƒ')
        
        # 3. ååé‡å¯¹æ¯”
        ax3 = axes[0, 2]
        categories = ['MatMul', 'Conv2D', 'Attention', 'MLP']
        baseline_throughput = viz_data['baseline_throughput']
        optimized_throughput = viz_data['optimized_throughput']
        
        x = np.arange(len(categories))
        width = 0.35
        
        ax3.bar(x - width/2, baseline_throughput, width, label='åŸºçº¿', color='#95A5A6')
        ax3.bar(x + width/2, optimized_throughput, width, label='YICAä¼˜åŒ–', color='#27AE60')
        ax3.set_title('ååé‡å¯¹æ¯”')
        ax3.set_ylabel('GFLOPS')
        ax3.set_xticks(x)
        ax3.set_xticklabels(categories)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. èƒ½è€—æ•ˆç‡çƒ­å›¾
        ax4 = axes[1, 0]
        workloads = ['Small', 'Medium', 'Large', 'XLarge']
        precisions = ['FP32', 'FP16', 'INT8']
        energy_efficiency = viz_data['energy_efficiency_matrix']
        
        im = ax4.imshow(energy_efficiency, cmap='RdYlGn', aspect='auto')
        ax4.set_title('èƒ½è€—æ•ˆç‡çƒ­å›¾')
        ax4.set_xticks(range(len(precisions)))
        ax4.set_xticklabels(precisions)
        ax4.set_yticks(range(len(workloads)))
        ax4.set_yticklabels(workloads)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(len(workloads)):
            for j in range(len(precisions)):
                ax4.text(j, i, f'{energy_efficiency[i, j]:.1f}', 
                        ha='center', va='center', color='black', fontweight='bold')
        
        # 5. å¼‚å¸¸æ£€æµ‹æ—¶é—´çº¿
        ax5 = axes[1, 1]
        anomaly_times = viz_data['anomaly_timestamps']
        anomaly_types = viz_data['anomaly_types']
        anomaly_severities = viz_data['anomaly_severities']
        
        colors_map = {'HIGH_LATENCY': '#E74C3C', 'LOW_UTILIZATION': '#F39C12', 
                     'MEMORY_LEAK': '#8E44AD', 'THERMAL_THROTTLING': '#E67E22'}
        
        for i, (time, atype, severity) in enumerate(zip(anomaly_times, anomaly_types, anomaly_severities)):
            color = colors_map.get(atype, '#95A5A6')
            ax5.scatter(time, severity, c=color, s=100, alpha=0.7, label=atype if i == 0 else "")
        
        ax5.set_title('å¼‚å¸¸æ£€æµ‹æ—¶é—´çº¿')
        ax5.set_xlabel('æ—¶é—´')
        ax5.set_ylabel('ä¸¥é‡ç¨‹åº¦')
        ax5.grid(True, alpha=0.3)
        ax5.set_ylim(0, 1)
        
        # 6. è°ƒä¼˜å†å²
        ax6 = axes[1, 2]
        tuning_iterations = viz_data['tuning_iterations']
        performance_scores = viz_data['performance_scores']
        
        ax6.plot(tuning_iterations, performance_scores, marker='o', linewidth=2, markersize=6)
        ax6.set_title('è‡ªåŠ¨è°ƒä¼˜å†å²')
        ax6.set_xlabel('è°ƒä¼˜è¿­ä»£')
        ax6.set_ylabel('æ€§èƒ½å¾—åˆ†')
        ax6.grid(True, alpha=0.3)
        
        # æ ‡æ³¨æœ€ä½³ç‚¹
        best_idx = np.argmax(performance_scores)
        ax6.annotate(f'æœ€ä½³: {performance_scores[best_idx]:.3f}', 
                    xy=(tuning_iterations[best_idx], performance_scores[best_idx]),
                    xytext=(10, 10), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
        
        plt.tight_layout()
        plt.savefig('yica_performance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š ä»ªè¡¨æ¿å·²ç”Ÿæˆ: yica_performance_dashboard.png")
        
        # ç”Ÿæˆå®æ—¶ç›‘æ§æŠ¥å‘Š
        self._generate_performance_report(viz_data)
        
        print("âœ… å¯è§†åŒ–ä»ªè¡¨æ¿æ¼”ç¤ºå®Œæˆ")
    
    def run_comprehensive_benchmark(self):
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸ ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        # å®šä¹‰åŸºå‡†æµ‹è¯•å¥—ä»¶
        benchmark_suite = {
            'matrix_multiplication': {
                'description': 'çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•',
                'test_cases': [
                    {'size': (512, 512), 'precision': 'fp16'},
                    {'size': (1024, 1024), 'precision': 'fp16'},
                    {'size': (2048, 2048), 'precision': 'fp16'},
                    {'size': (1024, 1024), 'precision': 'fp32'},
                ]
            },
            'convolution_2d': {
                'description': '2Då·ç§¯æ€§èƒ½æµ‹è¯•',
                'test_cases': [
                    {'input_shape': (32, 3, 224, 224), 'kernel_size': 3, 'precision': 'fp16'},
                    {'input_shape': (64, 64, 56, 56), 'kernel_size': 3, 'precision': 'fp16'},
                    {'input_shape': (32, 128, 28, 28), 'kernel_size': 1, 'precision': 'fp16'},
                ]
            },
            'attention_mechanism': {
                'description': 'æ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½æµ‹è¯•',
                'test_cases': [
                    {'batch_size': 32, 'seq_length': 512, 'hidden_size': 768, 'num_heads': 12},
                    {'batch_size': 16, 'seq_length': 1024, 'hidden_size': 1024, 'num_heads': 16},
                    {'batch_size': 8, 'seq_length': 2048, 'hidden_size': 1024, 'num_heads': 16},
                ]
            },
            'fused_mlp': {
                'description': 'èåˆMLPæ€§èƒ½æµ‹è¯•',
                'test_cases': [
                    {'input_size': 768, 'hidden_size': 3072, 'batch_size': 32},
                    {'input_size': 1024, 'hidden_size': 4096, 'batch_size': 64},
                    {'input_size': 2048, 'hidden_size': 8192, 'batch_size': 16},
                ]
            }
        }
        
        benchmark_results = {}
        
        for benchmark_name, benchmark_config in benchmark_suite.items():
            print(f"\nğŸ”¬ è¿è¡Œ {benchmark_config['description']}...")
            
            test_results = []
            
            for i, test_case in enumerate(benchmark_config['test_cases']):
                print(f"  æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(benchmark_config['test_cases'])}: {test_case}")
                
                # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•æ‰§è¡Œ
                result = self._run_single_benchmark(benchmark_name, test_case)
                test_results.append(result)
                
                print(f"    å»¶è¿Ÿ: {result['latency']:.3f} ms")
                print(f"    ååé‡: {result['throughput']:.1f} GFLOPS")
                print(f"    èƒ½è€—: {result['energy']:.2f} W")
                print(f"    æ•ˆç‡: {result['efficiency']:.1%}")
            
            benchmark_results[benchmark_name] = test_results
        
        # ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
        self._generate_benchmark_report(benchmark_results)
        
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•æ±‡æ€»:")
        self._print_benchmark_summary(benchmark_results)
        
        print("âœ… ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        
        return benchmark_results
    
    # ===========================================
    # å†…éƒ¨è¾…åŠ©æ–¹æ³•
    # ===========================================
    
    def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            metrics = self._collect_performance_metrics()
            
            # å­˜å‚¨å†å²æ•°æ®
            for metric in metrics:
                self.performance_history[metric.type].append(metric)
                
                # é™åˆ¶å†å²æ•°æ®å¤§å°
                if len(self.performance_history[metric.type]) > self.monitor_config['max_history_size']:
                    self.performance_history[metric.type].popleft()
            
            time.sleep(self.monitor_config['sampling_interval'] / 1000.0)
    
    def _simulate_workload_execution(self):
        """æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½æ‰§è¡Œ"""
        workloads = ['çŸ©é˜µä¹˜æ³•', 'å·ç§¯è®¡ç®—', 'æ³¨æ„åŠ›æœºåˆ¶', 'MLPå‰å‘']
        
        for i, workload in enumerate(workloads):
            print(f"  æ‰§è¡Œ {workload}... ({i+1}/{len(workloads)})")
            time.sleep(0.5)  # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
    
    def _get_current_metrics(self) -> List[PerformanceMetric]:
        """è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        return self._collect_performance_metrics()
    
    def _collect_performance_metrics(self) -> List[PerformanceMetric]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        current_time = time.time()
        
        # æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡é‡‡é›†
        metrics = [
            PerformanceMetric(
                type='CIM_UTILIZATION',
                name='CIMé˜µåˆ—åˆ©ç”¨ç‡',
                value=np.random.normal(75, 10),
                unit='%',
                timestamp=current_time,
                metadata={'array_count': '32'}
            ),
            PerformanceMetric(
                type='SPM_HIT_RATE',
                name='SPMå‘½ä¸­ç‡',
                value=np.random.normal(85, 5),
                unit='%',
                timestamp=current_time,
                metadata={'cache_size': '256MB'}
            ),
            PerformanceMetric(
                type='DRAM_BANDWIDTH',
                name='DRAMå¸¦å®½åˆ©ç”¨ç‡',
                value=np.random.normal(60, 15),
                unit='%',
                timestamp=current_time,
                metadata={'peak_bandwidth': '1TB/s'}
            ),
            PerformanceMetric(
                type='INSTRUCTION_THROUGHPUT',
                name='æŒ‡ä»¤ååé‡',
                value=np.random.normal(1200, 200),
                unit='GOPS',
                timestamp=current_time,
                metadata={'pipeline_depth': '4'}
            ),
            PerformanceMetric(
                type='ENERGY_CONSUMPTION',
                name='èƒ½è€—',
                value=np.random.normal(150, 30),
                unit='W',
                timestamp=current_time,
                metadata={'voltage': '1.0V'}
            ),
            PerformanceMetric(
                type='TEMPERATURE',
                name='æ¸©åº¦',
                value=np.random.normal(65, 8),
                unit='Â°C',
                timestamp=current_time,
                metadata={'sensor_location': 'core'}
            ),
            PerformanceMetric(
                type='MEMORY_USAGE',
                name='å†…å­˜ä½¿ç”¨ç‡',
                value=np.random.normal(70, 12),
                unit='%',
                timestamp=current_time,
                metadata={'total_memory': '16GB'}
            ),
            PerformanceMetric(
                type='COMMUNICATION_LATENCY',
                name='é€šä¿¡å»¶è¿Ÿ',
                value=np.random.normal(25, 8),
                unit='ms',
                timestamp=current_time,
                metadata={'protocol': 'YCCL'}
            )
        ]
        
        # ç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…
        for metric in metrics:
            if metric.unit == '%':
                metric.value = max(0, min(100, metric.value))
            elif metric.unit == 'Â°C':
                metric.value = max(20, min(100, metric.value))
            elif metric.unit == 'W':
                metric.value = max(50, min(300, metric.value))
            else:
                metric.value = max(0, metric.value)
        
        return metrics
    
    def _get_metric_status_icon(self, value: float, metric_type: str) -> str:
        """è·å–æŒ‡æ ‡çŠ¶æ€å›¾æ ‡"""
        if metric_type in ['CIM_UTILIZATION', 'SPM_HIT_RATE', 'DRAM_BANDWIDTH']:
            if value >= 80:
                return "ğŸŸ¢"
            elif value >= 60:
                return "ğŸŸ¡"
            else:
                return "ğŸ”´"
        elif metric_type == 'TEMPERATURE':
            if value <= 70:
                return "ğŸŸ¢"
            elif value <= 85:
                return "ğŸŸ¡"
            else:
                return "ğŸ”´"
        elif metric_type == 'COMMUNICATION_LATENCY':
            if value <= 20:
                return "ğŸŸ¢"
            elif value <= 40:
                return "ğŸŸ¡"
            else:
                return "ğŸ”´"
        else:
            return "ğŸ“Š"
    
    def _detect_performance_anomalies(self, metrics: List[PerformanceMetric]) -> List[PerformanceAnomaly]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        anomalies = []
        
        for metric in metrics:
            anomaly = None
            
            if metric.type == 'CIM_UTILIZATION' and metric.value < 30:
                anomaly = PerformanceAnomaly(
                    type='LOW_UTILIZATION',
                    description=f'CIMé˜µåˆ—åˆ©ç”¨ç‡å¼‚å¸¸ä½: {metric.value:.1f}%',
                    severity_score=0.7,
                    detection_time=metric.timestamp,
                    related_metrics=[metric],
                    suggested_actions=['æ£€æŸ¥å·¥ä½œè´Ÿè½½åˆ†å¸ƒ', 'ä¼˜åŒ–æ•°æ®å¸ƒå±€', 'è°ƒæ•´å¹¶è¡Œåº¦']
                )
            elif metric.type == 'TEMPERATURE' and metric.value > 85:
                anomaly = PerformanceAnomaly(
                    type='THERMAL_THROTTLING',
                    description=f'æ¸©åº¦è¿‡é«˜: {metric.value:.1f}Â°C',
                    severity_score=0.8,
                    detection_time=metric.timestamp,
                    related_metrics=[metric],
                    suggested_actions=['æ£€æŸ¥æ•£çƒ­ç³»ç»Ÿ', 'é™ä½å·¥ä½œé¢‘ç‡', 'ä¼˜åŒ–åŠŸè€—ç®¡ç†']
                )
            elif metric.type == 'COMMUNICATION_LATENCY' and metric.value > 50:
                anomaly = PerformanceAnomaly(
                    type='HIGH_LATENCY',
                    description=f'é€šä¿¡å»¶è¿Ÿè¿‡é«˜: {metric.value:.1f}ms',
                    severity_score=0.6,
                    detection_time=metric.timestamp,
                    related_metrics=[metric],
                    suggested_actions=['æ£€æŸ¥ç½‘ç»œè¿æ¥', 'ä¼˜åŒ–é€šä¿¡åè®®', 'è°ƒæ•´æ‰¹æ¬¡å¤§å°']
                )
            elif metric.type == 'MEMORY_USAGE' and metric.value > 90:
                anomaly = PerformanceAnomaly(
                    type='MEMORY_LEAK',
                    description=f'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metric.value:.1f}%',
                    severity_score=0.9,
                    detection_time=metric.timestamp,
                    related_metrics=[metric],
                    suggested_actions=['æ£€æŸ¥å†…å­˜æ³„æ¼', 'ä¼˜åŒ–å†…å­˜ç®¡ç†', 'å¢åŠ å†…å­˜å®¹é‡']
                )
            
            if anomaly:
                anomalies.append(anomaly)
        
        return anomalies
    
    def _inject_anomaly(self, normal_data: np.ndarray, scenario: Dict[str, Any]) -> np.ndarray:
        """å‘æ­£å¸¸æ•°æ®ä¸­æ³¨å…¥å¼‚å¸¸"""
        anomaly_data = normal_data.copy()
        
        # åœ¨ååŠéƒ¨åˆ†æ³¨å…¥å¼‚å¸¸
        anomaly_start = len(anomaly_data) // 2
        
        if scenario['type'] == 'HIGH_LATENCY':
            anomaly_data[anomaly_start:] += 30  # å¢åŠ å»¶è¿Ÿ
        elif scenario['type'] == 'LOW_UTILIZATION':
            anomaly_data[anomaly_start:] *= 0.3  # é™ä½åˆ©ç”¨ç‡
        elif scenario['type'] == 'MEMORY_LEAK':
            # æ¨¡æ‹Ÿå†…å­˜æ³„æ¼çš„çº¿æ€§å¢é•¿
            leak_growth = np.linspace(0, 40, len(anomaly_data) - anomaly_start)
            anomaly_data[anomaly_start:] += leak_growth
        elif scenario['type'] == 'THERMAL_THROTTLING':
            anomaly_data[anomaly_start:] += 25  # å¢åŠ æ¸©åº¦
        
        return anomaly_data
    
    def _run_anomaly_detection(self, data: np.ndarray, scenario: Dict[str, Any]) -> bool:
        """è¿è¡Œå¼‚å¸¸æ£€æµ‹ç®—æ³•"""
        # ç®€åŒ–çš„å¼‚å¸¸æ£€æµ‹ï¼šåŸºäºç»Ÿè®¡é˜ˆå€¼
        recent_data = data[-20:]  # æœ€è¿‘20ä¸ªæ•°æ®ç‚¹
        
        if len(recent_data) < 5:
            return False
        
        mean_recent = np.mean(recent_data)
        
        # æ ¹æ®åœºæ™¯ç±»å‹æ£€æµ‹å¼‚å¸¸
        return scenario['trigger_condition'](mean_recent)
    
    def _generate_anomaly_suggestions(self, anomaly_type: str) -> List[str]:
        """ç”Ÿæˆå¼‚å¸¸å¤„ç†å»ºè®®"""
        suggestions_map = {
            'HIGH_LATENCY': [
                'æ£€æŸ¥ç½‘ç»œè¿æ¥è´¨é‡',
                'ä¼˜åŒ–æ•°æ®ä¼ è¾“åè®®',
                'è°ƒæ•´æ‰¹æ¬¡å¤§å°',
                'å¯ç”¨æ•°æ®å‹ç¼©'
            ],
            'LOW_UTILIZATION': [
                'å¢åŠ å·¥ä½œè´Ÿè½½å¹¶è¡Œåº¦',
                'ä¼˜åŒ–æ•°æ®å¸ƒå±€',
                'è°ƒæ•´CIMé˜µåˆ—é…ç½®',
                'æ£€æŸ¥èµ„æºåˆ†é…ç­–ç•¥'
            ],
            'MEMORY_LEAK': [
                'æ£€æŸ¥å†…å­˜åˆ†é…å’Œé‡Šæ”¾',
                'ä¼˜åŒ–æ•°æ®ç»“æ„',
                'å¯ç”¨å†…å­˜æ± ',
                'å¢åŠ åƒåœ¾å›æ”¶é¢‘ç‡'
            ],
            'THERMAL_THROTTLING': [
                'æ£€æŸ¥æ•£çƒ­ç³»ç»Ÿ',
                'é™ä½å·¥ä½œé¢‘ç‡',
                'ä¼˜åŒ–åŠŸè€—ç®¡ç†',
                'è°ƒæ•´ç¯å¢ƒæ¸©åº¦'
            ]
        }
        
        return suggestions_map.get(anomaly_type, ['è”ç³»æŠ€æœ¯æ”¯æŒ'])
    
    def _measure_performance_score(self, parameters: Dict[str, Any]) -> float:
        """æµ‹é‡æ€§èƒ½å¾—åˆ†"""
        # æ¨¡æ‹Ÿæ€§èƒ½æµ‹é‡
        base_score = 0.7
        
        # æ ¹æ®å‚æ•°é…ç½®è®¡ç®—æ€§èƒ½å¾—åˆ†
        for param_name, param_info in parameters.items():
            current_value = param_info['current']
            weight = param_info['impact_weight']
            
            # ç®€åŒ–çš„æ€§èƒ½æ¨¡å‹
            if param_name == 'cim_array_count':
                # CIMé˜µåˆ—æ•°é‡çš„å½±å“
                optimal_count = 24
                efficiency = 1.0 - abs(current_value - optimal_count) / optimal_count * 0.3
                base_score += weight * efficiency * 0.5
            elif param_name == 'cache_line_size':
                # ç¼“å­˜è¡Œå¤§å°çš„å½±å“
                if current_value == 64:
                    base_score += weight * 0.1
                elif current_value == 128:
                    base_score += weight * 0.05
            # å…¶ä»–å‚æ•°çš„ç®€åŒ–å½±å“...
        
        # æ·»åŠ éšæœºå™ªå£°æ¨¡æ‹ŸçœŸå®æµ‹é‡çš„å˜åŒ–
        noise = np.random.normal(0, 0.02)
        return max(0.1, min(1.0, base_score + noise))
    
    def _grid_search_tuning(self, parameters: Dict[str, Any], baseline: float) -> Tuple[Dict[str, Any], float, int]:
        """ç½‘æ ¼æœç´¢è°ƒä¼˜"""
        best_config = {}
        best_performance = baseline
        steps = 0
        
        # ç®€åŒ–çš„ç½‘æ ¼æœç´¢ï¼šåªæœç´¢å‰ä¸¤ä¸ªæœ€é‡è¦çš„å‚æ•°
        important_params = sorted(parameters.items(), key=lambda x: x[1]['impact_weight'], reverse=True)[:2]
        
        for param1_name, param1_info in [important_params[0]]:
            for param1_value in param1_info['candidates'][:3]:  # é™åˆ¶æœç´¢ç©ºé—´
                for param2_name, param2_info in [important_params[1]]:
                    for param2_value in param2_info['candidates'][:3]:
                        # åˆ›å»ºæµ‹è¯•é…ç½®
                        test_config = {param1_name: param1_value, param2_name: param2_value}
                        
                        # æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•
                        performance = self._simulate_performance_test(test_config, parameters)
                        steps += 1
                        
                        if performance > best_performance:
                            best_performance = performance
                            best_config = test_config
        
        return best_config, best_performance, steps
    
    def _random_search_tuning(self, parameters: Dict[str, Any], baseline: float) -> Tuple[Dict[str, Any], float, int]:
        """éšæœºæœç´¢è°ƒä¼˜"""
        best_config = {}
        best_performance = baseline
        steps = 20  # éšæœºæœç´¢20æ¬¡
        
        for _ in range(steps):
            # éšæœºé€‰æ‹©å‚æ•°å€¼
            test_config = {}
            for param_name, param_info in parameters.items():
                test_config[param_name] = np.random.choice(param_info['candidates'])
            
            # æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•
            performance = self._simulate_performance_test(test_config, parameters)
            
            if performance > best_performance:
                best_performance = performance
                best_config = test_config
        
        return best_config, best_performance, steps
    
    def _bayesian_optimization_tuning(self, parameters: Dict[str, Any], baseline: float) -> Tuple[Dict[str, Any], float, int]:
        """è´å¶æ–¯ä¼˜åŒ–è°ƒä¼˜ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        best_config = {}
        best_performance = baseline
        steps = 15
        
        # ç®€åŒ–çš„è´å¶æ–¯ä¼˜åŒ–ï¼šåŸºäºå†å²æœ€ä½³ç»“æœè¿›è¡Œæ™ºèƒ½æœç´¢
        for i in range(steps):
            if i < 5:
                # å‰å‡ æ¬¡éšæœºæ¢ç´¢
                test_config = {}
                for param_name, param_info in parameters.items():
                    test_config[param_name] = np.random.choice(param_info['candidates'])
            else:
                # åç»­åŸºäºå·²çŸ¥æœ€ä½³é…ç½®è¿›è¡Œå±€éƒ¨æœç´¢
                test_config = best_config.copy()
                # éšæœºæ”¹å˜ä¸€ä¸ªå‚æ•°
                param_to_change = np.random.choice(list(parameters.keys()))
                param_info = parameters[param_to_change]
                test_config[param_to_change] = np.random.choice(param_info['candidates'])
            
            performance = self._simulate_performance_test(test_config, parameters)
            
            if performance > best_performance:
                best_performance = performance
                best_config = test_config
        
        return best_config, best_performance, steps
    
    def _genetic_algorithm_tuning(self, parameters: Dict[str, Any], baseline: float) -> Tuple[Dict[str, Any], float, int]:
        """é—ä¼ ç®—æ³•è°ƒä¼˜ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        population_size = 10
        generations = 5
        steps = population_size * generations
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = []
        for _ in range(population_size):
            individual = {}
            for param_name, param_info in parameters.items():
                individual[param_name] = np.random.choice(param_info['candidates'])
            population.append(individual)
        
        best_config = {}
        best_performance = baseline
        
        for generation in range(generations):
            # è¯„ä¼°ç§ç¾¤
            fitness_scores = []
            for individual in population:
                performance = self._simulate_performance_test(individual, parameters)
                fitness_scores.append(performance)
                
                if performance > best_performance:
                    best_performance = performance
                    best_config = individual.copy()
            
            # é€‰æ‹©å’Œäº¤å‰ï¼ˆç®€åŒ–ç‰ˆï¼‰
            # ä¿ç•™æœ€å¥½çš„ä¸€åŠ
            sorted_indices = np.argsort(fitness_scores)[-population_size//2:]
            new_population = [population[i] for i in sorted_indices]
            
            # ç”Ÿæˆæ–°ä¸ªä½“ï¼ˆäº¤å‰å’Œå˜å¼‚ï¼‰
            while len(new_population) < population_size:
                parent1, parent2 = np.random.choice(new_population, 2, replace=False)
                child = {}
                for param_name in parameters.keys():
                    # éšæœºé€‰æ‹©çˆ¶æ¯ä¹‹ä¸€çš„åŸºå› 
                    child[param_name] = np.random.choice([parent1[param_name], parent2[param_name]])
                
                # å˜å¼‚
                if np.random.random() < 0.1:  # 10% å˜å¼‚æ¦‚ç‡
                    param_to_mutate = np.random.choice(list(parameters.keys()))
                    param_info = parameters[param_to_mutate]
                    child[param_to_mutate] = np.random.choice(param_info['candidates'])
                
                new_population.append(child)
            
            population = new_population
        
        return best_config, best_performance, steps
    
    def _simulate_performance_test(self, config: Dict[str, Any], parameters: Dict[str, Any]) -> float:
        """æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•"""
        base_score = 0.7
        
        for param_name, param_value in config.items():
            if param_name in parameters:
                weight = parameters[param_name]['impact_weight']
                
                # ç®€åŒ–çš„å‚æ•°å½±å“æ¨¡å‹
                if param_name == 'cim_array_count':
                    optimal_value = 24
                    efficiency = 1.0 - abs(param_value - optimal_value) / optimal_value * 0.3
                    base_score += weight * efficiency * 0.3
                elif param_name == 'spm_allocation_strategy':
                    if param_value == 'hybrid':
                        base_score += weight * 0.15
                    elif param_value == 'locality_first':
                        base_score += weight * 0.1
                # å…¶ä»–å‚æ•°...
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.01)
        return max(0.1, min(1.0, base_score + noise))
    
    def _record_tuning_attempt(self, algorithm: str, config: Dict[str, Any], 
                             before: float, after: float):
        """è®°å½•è°ƒä¼˜å°è¯•"""
        self.tuning_history.append({
            'timestamp': time.time(),
            'algorithm': algorithm,
            'config': config,
            'performance_before': before,
            'performance_after': after,
            'improvement': (after - before) / before * 100
        })
    
    def _apply_tuning_configuration(self, config: Dict[str, Any]):
        """åº”ç”¨è°ƒä¼˜é…ç½®"""
        print("    é…ç½®å·²åº”ç”¨åˆ°YICAç³»ç»Ÿ")
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ç³»ç»ŸAPIæ¥åº”ç”¨é…ç½®
        time.sleep(0.5)
    
    def _generate_analysis_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ•°æ®"""
        return {
            'cim_utilization': np.random.normal(75, 10, 100),
            'spm_hit_rate': np.random.normal(85, 5, 100),
            'memory_usage': np.random.normal(70, 15, 100),
            'energy_consumption': np.random.normal(150, 20, 100),
            'throughput': np.random.normal(1200, 200, 100),
            'latency': np.random.normal(25, 8, 100)
        }
    
    def _identify_bottlenecks(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†æå„ç»„ä»¶åˆ©ç”¨ç‡
        if np.mean(data['cim_utilization']) < 60:
            bottlenecks.append({
                'component': 'CIMé˜µåˆ—',
                'utilization': np.mean(data['cim_utilization']) / 100,
                'impact_score': 0.8,
                'causes': ['æ•°æ®ä¾èµ–', 'è´Ÿè½½ä¸å‡è¡¡', 'åŒæ­¥å¼€é”€'],
                'solutions': ['ä¼˜åŒ–æ•°æ®å¸ƒå±€', 'å¢åŠ å¹¶è¡Œåº¦', 'å‡å°‘åŒæ­¥ç‚¹']
            })
        
        if np.mean(data['spm_hit_rate']) < 80:
            bottlenecks.append({
                'component': 'SPMç¼“å­˜',
                'utilization': np.mean(data['spm_hit_rate']) / 100,
                'impact_score': 0.6,
                'causes': ['ç¼“å­˜å®¹é‡ä¸è¶³', 'è®¿é—®æ¨¡å¼ä¸è§„å¾‹', 'é¢„å–ç­–ç•¥ä¸å½“'],
                'solutions': ['å¢åŠ ç¼“å­˜å®¹é‡', 'ä¼˜åŒ–è®¿é—®æ¨¡å¼', 'æ”¹è¿›é¢„å–ç®—æ³•']
            })
        
        if np.mean(data['memory_usage']) > 85:
            bottlenecks.append({
                'component': 'å†…å­˜å­ç³»ç»Ÿ',
                'utilization': np.mean(data['memory_usage']) / 100,
                'impact_score': 0.7,
                'causes': ['å†…å­˜å¸¦å®½é™åˆ¶', 'å†…å­˜ç¢ç‰‡', 'æ•°æ®å±€éƒ¨æ€§å·®'],
                'solutions': ['ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼', 'ä½¿ç”¨å†…å­˜æ± ', 'æé«˜æ•°æ®å±€éƒ¨æ€§']
            })
        
        return sorted(bottlenecks, key=lambda x: x['impact_score'], reverse=True)
    
    def _analyze_efficiency(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•ˆç‡"""
        return {
            'compute_efficiency': min(1.0, np.mean(data['cim_utilization']) / 100),
            'memory_efficiency': min(1.0, np.mean(data['spm_hit_rate']) / 100),
            'energy_efficiency': min(1.0, np.mean(data['throughput']) / np.mean(data['energy_consumption']) * 10),
            'communication_efficiency': min(1.0, max(0.1, 1.0 - np.mean(data['latency']) / 100)),
            'limiting_factor': 'å†…å­˜å¸¦å®½' if np.mean(data['memory_usage']) > 80 else 'CIMåˆ©ç”¨ç‡'
        }
    
    def _analyze_performance_trends(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        trends = []
        
        for metric_name, values in data.items():
            if len(values) > 10:
                # è®¡ç®—è¶‹åŠ¿æ–œç‡
                x = np.arange(len(values))
                slope, _ = np.polyfit(x, values, 1)
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                correlation = np.corrcoef(x, values)[0, 1]
                
                # ç¡®å®šè¶‹åŠ¿æ–¹å‘
                if abs(slope) < 0.1:
                    direction = "ç¨³å®š"
                elif slope > 0:
                    direction = "ä¸Šå‡"
                else:
                    direction = "ä¸‹é™"
                
                trends.append({
                    'metric': metric_name,
                    'slope': slope,
                    'correlation': correlation,
                    'direction': direction,
                    'accuracy': abs(correlation)
                })
        
        return trends
    
    def _generate_optimization_suggestions(self, bottlenecks: List[Dict[str, Any]], 
                                         efficiency: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        for bottleneck in bottlenecks:
            if bottleneck['component'] == 'CIMé˜µåˆ—':
                suggestions.append({
                    'title': 'ä¼˜åŒ–CIMé˜µåˆ—åˆ©ç”¨ç‡',
                    'expected_improvement': 25.0,
                    'implementation_difficulty': 'ä¸­ç­‰',
                    'priority': 8,
                    'description': 'é€šè¿‡ä¼˜åŒ–æ•°æ®å¸ƒå±€å’Œå¢åŠ å¹¶è¡Œåº¦æ¥æé«˜CIMé˜µåˆ—åˆ©ç”¨ç‡'
                })
            elif bottleneck['component'] == 'SPMç¼“å­˜':
                suggestions.append({
                    'title': 'æ”¹è¿›SPMç¼“å­˜ç­–ç•¥',
                    'expected_improvement': 15.0,
                    'implementation_difficulty': 'ç®€å•',
                    'priority': 6,
                    'description': 'ä¼˜åŒ–ç¼“å­˜é¢„å–ç­–ç•¥å’Œæ›¿æ¢ç®—æ³•'
                })
        
        # åŸºäºæ•ˆç‡çš„å»ºè®®
        if efficiency['energy_efficiency'] < 0.7:
            suggestions.append({
                'title': 'ä¼˜åŒ–èƒ½è€—æ•ˆç‡',
                'expected_improvement': 20.0,
                'implementation_difficulty': 'å›°éš¾',
                'priority': 7,
                'description': 'é€šè¿‡åŠ¨æ€ç”µå‹é¢‘ç‡è°ƒèŠ‚å’ŒåŠŸè€—ç®¡ç†æ¥æé«˜èƒ½æ•ˆ'
            })
        
        return sorted(suggestions, key=lambda x: x['priority'], reverse=True)
    
    def _generate_visualization_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
        # æ—¶é—´åºåˆ—æ•°æ®
        timestamps = np.arange(100)
        
        return {
            'timestamps': timestamps,
            'cim_utilization': np.random.normal(75, 10, 100).clip(0, 100),
            'memory_usage': [60, 25, 15],  # SPM, DRAM, æœªä½¿ç”¨
            'baseline_throughput': [800, 1200, 900, 1100],
            'optimized_throughput': [1200, 1800, 1350, 1650],
            'energy_efficiency_matrix': np.random.uniform(5, 15, (4, 3)),
            'anomaly_timestamps': np.random.choice(timestamps, 5),
            'anomaly_types': ['HIGH_LATENCY', 'LOW_UTILIZATION', 'MEMORY_LEAK', 'THERMAL_THROTTLING', 'HIGH_LATENCY'],
            'anomaly_severities': np.random.uniform(0.3, 0.9, 5),
            'tuning_iterations': np.arange(1, 21),
            'performance_scores': np.random.normal(0.75, 0.05, 20).cummax()  # å•è°ƒé€’å¢
        }
    
    def _generate_performance_report(self, viz_data: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report_content = f"""
# YICA æ€§èƒ½ç›‘æ§æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç³»ç»Ÿé…ç½®
- CIM é˜µåˆ—æ•°é‡: {self.yica_config.num_cim_arrays}
- SPM å®¹é‡: {self.yica_config.spm_size_per_die // (1024*1024)} MB
- DRAM å®¹é‡: {self.yica_config.dram_size_per_cluster // (1024*1024*1024)} GB
- ç›®æ ‡ç²¾åº¦: {self.yica_config.target_precision}

## æ€§èƒ½æ‘˜è¦
- å¹³å‡CIMåˆ©ç”¨ç‡: {np.mean(viz_data['cim_utilization']):.1f}%
- åŸºçº¿ååé‡: {np.mean(viz_data['baseline_throughput']):.0f} GFLOPS
- ä¼˜åŒ–åååé‡: {np.mean(viz_data['optimized_throughput']):.0f} GFLOPS
- æ€§èƒ½æå‡: {(np.mean(viz_data['optimized_throughput']) / np.mean(viz_data['baseline_throughput']) - 1) * 100:.1f}%

## å¼‚å¸¸æ£€æµ‹
- æ£€æµ‹åˆ°å¼‚å¸¸: {len(viz_data['anomaly_timestamps'])} ä¸ª
- å¹³å‡ä¸¥é‡ç¨‹åº¦: {np.mean(viz_data['anomaly_severities']):.2f}

## è°ƒä¼˜å†å²
- è°ƒä¼˜è¿­ä»£æ¬¡æ•°: {len(viz_data['tuning_iterations'])}
- æœ€ç»ˆæ€§èƒ½å¾—åˆ†: {viz_data['performance_scores'][-1]:.3f}
- è°ƒä¼˜æå‡: {(viz_data['performance_scores'][-1] / viz_data['performance_scores'][0] - 1) * 100:.1f}%

## å»ºè®®
1. ç»§ç»­ä¼˜åŒ–CIMé˜µåˆ—åˆ©ç”¨ç‡
2. æ”¹è¿›å†…å­˜è®¿é—®æ¨¡å¼
3. å¯ç”¨æ›´æ¿€è¿›çš„è°ƒä¼˜ç­–ç•¥
4. å®šæœŸç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
"""
        
        with open('yica_performance_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print("ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: yica_performance_report.md")
    
    def _run_single_benchmark(self, benchmark_name: str, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
        # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•æ‰§è¡Œ
        base_latency = 5.0  # ms
        base_throughput = 1000.0  # GFLOPS
        base_energy = 100.0  # W
        
        # æ ¹æ®æµ‹è¯•ç”¨ä¾‹è°ƒæ•´æ€§èƒ½
        if 'size' in test_case:
            size_factor = np.prod(test_case['size']) / (1024 * 1024)
            base_latency *= size_factor * 0.1
            base_throughput /= size_factor * 0.05
        
        if 'precision' in test_case:
            if test_case['precision'] == 'fp16':
                base_latency *= 0.7
                base_throughput *= 1.4
                base_energy *= 0.8
        
        # æ·»åŠ éšæœºå˜åŒ–
        latency = base_latency * (1 + np.random.normal(0, 0.1))
        throughput = base_throughput * (1 + np.random.normal(0, 0.1))
        energy = base_energy * (1 + np.random.normal(0, 0.1))
        
        efficiency = throughput / (throughput * 1.2)  # ç›¸å¯¹äºç†è®ºå³°å€¼
        
        return {
            'latency': latency,
            'throughput': throughput,
            'energy': energy,
            'efficiency': efficiency
        }
    
    def _generate_benchmark_report(self, results: Dict[str, Any]):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'yica_config': asdict(self.yica_config),
            'benchmark_results': results,
            'summary': {}
        }
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        all_latencies = []
        all_throughputs = []
        all_efficiencies = []
        
        for benchmark_name, test_results in results.items():
            for result in test_results:
                all_latencies.append(result['latency'])
                all_throughputs.append(result['throughput'])
                all_efficiencies.append(result['efficiency'])
        
        report_data['summary'] = {
            'avg_latency': np.mean(all_latencies),
            'avg_throughput': np.mean(all_throughputs),
            'avg_efficiency': np.mean(all_efficiencies),
            'total_tests': sum(len(results) for results in results.values())
        }
        
        with open('yica_benchmark_results.json', 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
        
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: yica_benchmark_results.json")
    
    def _print_benchmark_summary(self, results: Dict[str, Any]):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ±‡æ€»"""
        print("  åŸºå‡†æµ‹è¯•ç±»å‹ | å¹³å‡å»¶è¿Ÿ(ms) | å¹³å‡ååé‡(GFLOPS) | å¹³å‡æ•ˆç‡")
        print("  " + "-" * 60)
        
        for benchmark_name, test_results in results.items():
            avg_latency = np.mean([r['latency'] for r in test_results])
            avg_throughput = np.mean([r['throughput'] for r in test_results])
            avg_efficiency = np.mean([r['efficiency'] for r in test_results])
            
            print(f"  {benchmark_name:<15} | {avg_latency:>10.2f} | {avg_throughput:>15.1f} | {avg_efficiency:>8.1%}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ YICA æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜æ¼”ç¤ºå¯åŠ¨")
    print("=" * 80)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = YICAPerformanceMonitorDemo()
    
    try:
        # è¿è¡Œå„ç§æ¼”ç¤º
        demo.demonstrate_real_time_monitoring()
        demo.demonstrate_anomaly_detection()
        demo.demonstrate_auto_tuning()
        demo.demonstrate_performance_analysis()
        demo.demonstrate_visualization_dashboard()
        benchmark_results = demo.run_comprehensive_benchmark()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ YICA æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“Š å®Œæˆ {sum(len(results) for results in benchmark_results.values())} é¡¹åŸºå‡†æµ‹è¯•")
        print(f"ğŸ” æ£€æµ‹åˆ° {len(demo.anomaly_history)} ä¸ªæ€§èƒ½å¼‚å¸¸")
        print(f"ğŸ¯ æ‰§è¡Œäº† {len(demo.tuning_history)} æ¬¡è°ƒä¼˜å°è¯•")
        print("ğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        if demo.monitoring_active:
            demo.stop_monitoring()


if __name__ == "__main__":
    main() 