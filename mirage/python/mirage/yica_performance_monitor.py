#!/usr/bin/env python3
"""
YICA-Mirage é«˜çº§æ€§èƒ½ç›‘æ§æ¨¡å—

æä¾›å®æ—¶çš„ YICA æ€§èƒ½ç›‘æ§å’Œåˆ†æï¼ŒåŒ…æ‹¬ï¼š
- å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
- æ€§èƒ½ç“¶é¢ˆåˆ†æ
- è‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®
- æ€§èƒ½å¯è§†åŒ–å’ŒæŠ¥å‘Š
"""

import time
import json
import threading
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from collections import deque, defaultdict
import queue
from enum import Enum
import logging

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import matplotlib.animation as animation
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    MEMORY = "memory"
    ENERGY = "energy"
    TEMPERATURE = "temperature"
    UTILIZATION = "utilization"
    CACHE_HIT_RATE = "cache_hit_rate"
    BANDWIDTH = "bandwidth"


class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡"""
    name: str
    type: MetricType
    value: float
    unit: str
    timestamp: datetime
    source: str = "yica_device"
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    alert_id: str
    level: AlertLevel
    metric_name: str
    current_value: float
    threshold: float
    message: str
    timestamp: datetime
    resolved: bool = False
    resolution_time: Optional[datetime] = None


@dataclass
class PerformanceThreshold:
    """æ€§èƒ½é˜ˆå€¼"""
    metric_name: str
    warning_threshold: float
    error_threshold: float
    critical_threshold: float
    comparison_type: str = "greater_than"  # greater_than, less_than, equal


class MetricCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, collection_interval: float = 1.0):
        self.collection_interval = collection_interval
        self.running = False
        self.collector_thread = None
        self.metrics_queue = queue.Queue()
        self.device_simulators = {}
    
    def start_collection(self):
        """å¼€å§‹æŒ‡æ ‡æ”¶é›†"""
        if self.running:
            return
        
        self.running = True
        self.collector_thread = threading.Thread(target=self._collect_metrics)
        self.collector_thread.daemon = True
        self.collector_thread.start()
        
        logger.info("æ€§èƒ½æŒ‡æ ‡æ”¶é›†å·²å¼€å§‹")
    
    def stop_collection(self):
        """åœæ­¢æŒ‡æ ‡æ”¶é›†"""
        self.running = False
        if self.collector_thread:
            self.collector_thread.join()
        
        logger.info("æ€§èƒ½æŒ‡æ ‡æ”¶é›†å·²åœæ­¢")
    
    def _collect_metrics(self):
        """æ”¶é›†æŒ‡æ ‡çš„ä¸»å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†å„ç§æ€§èƒ½æŒ‡æ ‡
                metrics = self._collect_yica_metrics()
                
                for metric in metrics:
                    self.metrics_queue.put(metric)
                
                time.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"æŒ‡æ ‡æ”¶é›†å‡ºé”™: {e}")
                time.sleep(self.collection_interval)
    
    def _collect_yica_metrics(self) -> List[PerformanceMetric]:
        """æ”¶é›† YICA è®¾å¤‡æŒ‡æ ‡"""
        metrics = []
        current_time = datetime.now()
        
        # æ¨¡æ‹Ÿ CIM é˜µåˆ—åˆ©ç”¨ç‡
        cim_utilization = np.random.normal(75, 10) if NUMPY_AVAILABLE else 75.0
        cim_utilization = max(0, min(100, cim_utilization))
        
        metrics.append(PerformanceMetric(
            name="cim_utilization",
            type=MetricType.UTILIZATION,
            value=cim_utilization,
            unit="percent",
            timestamp=current_time,
            metadata={"array_count": 16, "array_size": 64}
        ))
        
        # æ¨¡æ‹Ÿ SPM ç¼“å­˜å‘½ä¸­ç‡
        spm_hit_rate = np.random.normal(85, 5) if NUMPY_AVAILABLE else 85.0
        spm_hit_rate = max(0, min(100, spm_hit_rate))
        
        metrics.append(PerformanceMetric(
            name="spm_cache_hit_rate",
            type=MetricType.CACHE_HIT_RATE,
            value=spm_hit_rate,
            unit="percent",
            timestamp=current_time,
            metadata={"spm_size_mb": 64}
        ))
        
        # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ç‡
        memory_usage = np.random.normal(60, 15) if NUMPY_AVAILABLE else 60.0
        memory_usage = max(0, min(100, memory_usage))
        
        metrics.append(PerformanceMetric(
            name="memory_usage",
            type=MetricType.MEMORY,
            value=memory_usage,
            unit="percent",
            timestamp=current_time
        ))
        
        # æ¨¡æ‹Ÿè®¾å¤‡æ¸©åº¦
        temperature = np.random.normal(45, 5) if NUMPY_AVAILABLE else 45.0
        temperature = max(20, min(85, temperature))
        
        metrics.append(PerformanceMetric(
            name="device_temperature",
            type=MetricType.TEMPERATURE,
            value=temperature,
            unit="celsius",
            timestamp=current_time
        ))
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿ
        latency = np.random.lognormal(2.0, 0.5) if NUMPY_AVAILABLE else 5.0
        
        metrics.append(PerformanceMetric(
            name="inference_latency",
            type=MetricType.LATENCY,
            value=latency,
            unit="milliseconds",
            timestamp=current_time
        ))
        
        # æ¨¡æ‹Ÿååé‡
        throughput = np.random.normal(1000, 100) if NUMPY_AVAILABLE else 1000.0
        throughput = max(0, throughput)
        
        metrics.append(PerformanceMetric(
            name="inference_throughput",
            type=MetricType.THROUGHPUT,
            value=throughput,
            unit="ops_per_second",
            timestamp=current_time
        ))
        
        # æ¨¡æ‹Ÿèƒ½è€—
        power_consumption = np.random.normal(150, 20) if NUMPY_AVAILABLE else 150.0
        power_consumption = max(50, power_consumption)
        
        metrics.append(PerformanceMetric(
            name="power_consumption",
            type=MetricType.ENERGY,
            value=power_consumption,
            unit="watts",
            timestamp=current_time
        ))
        
        return metrics
    
    def get_latest_metrics(self, count: int = 10) -> List[PerformanceMetric]:
        """è·å–æœ€æ–°çš„æŒ‡æ ‡"""
        metrics = []
        for _ in range(min(count, self.metrics_queue.qsize())):
            try:
                metric = self.metrics_queue.get_nowait()
                metrics.append(metric)
            except queue.Empty:
                break
        
        return metrics


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metric_history = defaultdict(lambda: deque(maxlen=window_size))
        self.thresholds = {}
        self.anomaly_callbacks = []
    
    def add_threshold(self, threshold: PerformanceThreshold):
        """æ·»åŠ æ€§èƒ½é˜ˆå€¼"""
        self.thresholds[threshold.metric_name] = threshold
    
    def add_anomaly_callback(self, callback: Callable[[Alert], None]):
        """æ·»åŠ å¼‚å¸¸å›è°ƒ"""
        self.anomaly_callbacks.append(callback)
    
    def check_metric(self, metric: PerformanceMetric) -> Optional[Alert]:
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å¼‚å¸¸"""
        self.metric_history[metric.name].append(metric.value)
        
        # æ£€æŸ¥é˜ˆå€¼
        if metric.name in self.thresholds:
            alert = self._check_threshold(metric)
            if alert:
                self._trigger_alert(alert)
                return alert
        
        # æ£€æŸ¥ç»Ÿè®¡å¼‚å¸¸
        if len(self.metric_history[metric.name]) >= self.window_size:
            alert = self._check_statistical_anomaly(metric)
            if alert:
                self._trigger_alert(alert)
                return alert
        
        return None
    
    def _check_threshold(self, metric: PerformanceMetric) -> Optional[Alert]:
        """æ£€æŸ¥é˜ˆå€¼å¼‚å¸¸"""
        threshold = self.thresholds[metric.name]
        
        if threshold.comparison_type == "greater_than":
            if metric.value >= threshold.critical_threshold:
                level = AlertLevel.CRITICAL
                threshold_value = threshold.critical_threshold
            elif metric.value >= threshold.error_threshold:
                level = AlertLevel.ERROR
                threshold_value = threshold.error_threshold
            elif metric.value >= threshold.warning_threshold:
                level = AlertLevel.WARNING
                threshold_value = threshold.warning_threshold
            else:
                return None
        else:  # less_than
            if metric.value <= threshold.critical_threshold:
                level = AlertLevel.CRITICAL
                threshold_value = threshold.critical_threshold
            elif metric.value <= threshold.error_threshold:
                level = AlertLevel.ERROR
                threshold_value = threshold.error_threshold
            elif metric.value <= threshold.warning_threshold:
                level = AlertLevel.WARNING
                threshold_value = threshold.warning_threshold
            else:
                return None
        
        alert_id = f"{metric.name}_{int(time.time())}"
        message = (f"{metric.name} å€¼ {metric.value:.2f} {metric.unit} "
                  f"è¶…è¿‡ {level.value} é˜ˆå€¼ {threshold_value:.2f}")
        
        return Alert(
            alert_id=alert_id,
            level=level,
            metric_name=metric.name,
            current_value=metric.value,
            threshold=threshold_value,
            message=message,
            timestamp=metric.timestamp
        )
    
    def _check_statistical_anomaly(self, metric: PerformanceMetric) -> Optional[Alert]:
        """æ£€æŸ¥ç»Ÿè®¡å¼‚å¸¸"""
        if not NUMPY_AVAILABLE:
            return None
        
        values = list(self.metric_history[metric.name])
        if len(values) < 10:
            return None
        
        mean = np.mean(values[:-1])
        std = np.std(values[:-1])
        
        # ä½¿ç”¨ 3-sigma è§„åˆ™
        if abs(metric.value - mean) > 3 * std:
            alert_id = f"{metric.name}_anomaly_{int(time.time())}"
            message = (f"{metric.name} å€¼ {metric.value:.2f} {metric.unit} "
                      f"å¼‚å¸¸åç¦»å†å²å‡å€¼ {mean:.2f} (Â±{3*std:.2f})")
            
            return Alert(
                alert_id=alert_id,
                level=AlertLevel.WARNING,
                metric_name=metric.name,
                current_value=metric.value,
                threshold=mean + 3 * std,
                message=message,
                timestamp=metric.timestamp
            )
        
        return None
    
    def _trigger_alert(self, alert: Alert):
        """è§¦å‘å‘Šè­¦"""
        logger.warning(f"æ€§èƒ½å‘Šè­¦: {alert.message}")
        
        for callback in self.anomaly_callbacks:
            try:
                callback(alert)
            except Exception as e:
                logger.error(f"å‘Šè­¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_history = []
    
    def analyze_performance(self, metrics: List[PerformanceMetric]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ•°æ®"""
        if not metrics:
            return {}
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "metric_count": len(metrics),
            "metric_summary": self._summarize_metrics(metrics),
            "bottlenecks": self._identify_bottlenecks(metrics),
            "recommendations": self._generate_recommendations(metrics),
            "efficiency_score": self._calculate_efficiency_score(metrics)
        }
        
        self.analysis_history.append(analysis)
        return analysis
    
    def _summarize_metrics(self, metrics: List[PerformanceMetric]) -> Dict[str, Any]:
        """æ±‡æ€»æŒ‡æ ‡"""
        summary = {}
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = defaultdict(list)
        for metric in metrics:
            by_type[metric.type.value].append(metric.value)
        
        for metric_type, values in by_type.items():
            if NUMPY_AVAILABLE and values:
                summary[metric_type] = {
                    "count": len(values),
                    "mean": float(np.mean(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values))
                }
            else:
                summary[metric_type] = {
                    "count": len(values),
                    "values": values
                }
        
        return summary
    
    def _identify_bottlenecks(self, metrics: List[PerformanceMetric]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # æŸ¥æ‰¾é«˜å»¶è¿Ÿ
        latency_metrics = [m for m in metrics if m.type == MetricType.LATENCY]
        if latency_metrics:
            high_latency = [m for m in latency_metrics if m.value > 10.0]  # 10ms é˜ˆå€¼
            if high_latency:
                bottlenecks.append({
                    "type": "high_latency",
                    "severity": "warning",
                    "affected_metrics": [m.name for m in high_latency],
                    "description": f"æ£€æµ‹åˆ° {len(high_latency)} ä¸ªé«˜å»¶è¿ŸæŒ‡æ ‡"
                })
        
        # æŸ¥æ‰¾ä½ç¼“å­˜å‘½ä¸­ç‡
        cache_metrics = [m for m in metrics if m.type == MetricType.CACHE_HIT_RATE]
        if cache_metrics:
            low_cache = [m for m in cache_metrics if m.value < 80.0]  # 80% é˜ˆå€¼
            if low_cache:
                bottlenecks.append({
                    "type": "low_cache_hit_rate",
                    "severity": "warning",
                    "affected_metrics": [m.name for m in low_cache],
                    "description": f"æ£€æµ‹åˆ° {len(low_cache)} ä¸ªä½ç¼“å­˜å‘½ä¸­ç‡æŒ‡æ ‡"
                })
        
        # æŸ¥æ‰¾é«˜å†…å­˜ä½¿ç”¨
        memory_metrics = [m for m in metrics if m.type == MetricType.MEMORY]
        if memory_metrics:
            high_memory = [m for m in memory_metrics if m.value > 90.0]  # 90% é˜ˆå€¼
            if high_memory:
                bottlenecks.append({
                    "type": "high_memory_usage",
                    "severity": "error",
                    "affected_metrics": [m.name for m in high_memory],
                    "description": f"æ£€æµ‹åˆ° {len(high_memory)} ä¸ªé«˜å†…å­˜ä½¿ç”¨æŒ‡æ ‡"
                })
        
        # æŸ¥æ‰¾é«˜æ¸©åº¦
        temp_metrics = [m for m in metrics if m.type == MetricType.TEMPERATURE]
        if temp_metrics:
            high_temp = [m for m in temp_metrics if m.value > 70.0]  # 70Â°C é˜ˆå€¼
            if high_temp:
                bottlenecks.append({
                    "type": "high_temperature",
                    "severity": "critical",
                    "affected_metrics": [m.name for m in high_temp],
                    "description": f"æ£€æµ‹åˆ° {len(high_temp)} ä¸ªé«˜æ¸©åº¦æŒ‡æ ‡"
                })
        
        return bottlenecks
    
    def _generate_recommendations(self, metrics: List[PerformanceMetric]) -> List[Dict[str, str]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæŒ‡æ ‡å€¼ç”Ÿæˆå»ºè®®
        for metric in metrics:
            if metric.type == MetricType.CACHE_HIT_RATE and metric.value < 85.0:
                recommendations.append({
                    "category": "memory_optimization",
                    "priority": "medium",
                    "title": "æå‡ç¼“å­˜å‘½ä¸­ç‡",
                    "description": f"{metric.name} å‘½ä¸­ç‡ä¸º {metric.value:.1f}%ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼æˆ–å¢åŠ  SPM å¤§å°"
                })
            
            elif metric.type == MetricType.UTILIZATION and metric.value < 60.0:
                recommendations.append({
                    "category": "resource_utilization",
                    "priority": "medium",
                    "title": "æå‡èµ„æºåˆ©ç”¨ç‡",
                    "description": f"{metric.name} åˆ©ç”¨ç‡ä¸º {metric.value:.1f}%ï¼Œå»ºè®®å¢åŠ æ‰¹æ¬¡å¤§å°æˆ–å¯ç”¨ç®—å­èåˆ"
                })
            
            elif metric.type == MetricType.LATENCY and metric.value > 15.0:
                recommendations.append({
                    "category": "performance_optimization",
                    "priority": "high",
                    "title": "é™ä½æ¨ç†å»¶è¿Ÿ",
                    "description": f"{metric.name} å»¶è¿Ÿä¸º {metric.value:.1f}msï¼Œå»ºè®®å¯ç”¨ç®¡é“ä¼˜åŒ–æˆ–è°ƒæ•´æ¨¡å‹ç²¾åº¦"
                })
            
            elif metric.type == MetricType.TEMPERATURE and metric.value > 65.0:
                recommendations.append({
                    "category": "thermal_management",
                    "priority": "high",
                    "title": "åŠ å¼ºæ•£çƒ­ç®¡ç†",
                    "description": f"{metric.name} æ¸©åº¦ä¸º {metric.value:.1f}Â°Cï¼Œå»ºè®®é™ä½è®¡ç®—é¢‘ç‡æˆ–æ”¹å–„æ•£çƒ­æ¡ä»¶"
                })
        
        # å»é‡
        seen = set()
        unique_recommendations = []
        for rec in recommendations:
            key = (rec["category"], rec["title"])
            if key not in seen:
                seen.add(key)
                unique_recommendations.append(rec)
        
        return unique_recommendations
    
    def _calculate_efficiency_score(self, metrics: List[PerformanceMetric]) -> float:
        """è®¡ç®—æ•ˆç‡è¯„åˆ† (0-100)"""
        if not metrics:
            return 0.0
        
        scores = []
        
        # è¯„ä¼°å„ç±»æŒ‡æ ‡
        for metric in metrics:
            if metric.type == MetricType.UTILIZATION:
                # åˆ©ç”¨ç‡è¶Šé«˜è¶Šå¥½ï¼Œä½†ä¸è¦è¶…è¿‡95%
                score = min(metric.value, 95.0) / 95.0 * 100
                scores.append(score)
            
            elif metric.type == MetricType.CACHE_HIT_RATE:
                # ç¼“å­˜å‘½ä¸­ç‡è¶Šé«˜è¶Šå¥½
                score = metric.value
                scores.append(score)
            
            elif metric.type == MetricType.LATENCY:
                # å»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼Œä»¥5msä¸ºåŸºå‡†
                score = max(0, 100 - metric.value * 10)
                scores.append(score)
            
            elif metric.type == MetricType.TEMPERATURE:
                # æ¸©åº¦é€‚ä¸­æœ€å¥½ï¼Œ40-50Â°Cä¸ºæœ€ä¼˜
                if 40 <= metric.value <= 50:
                    score = 100
                elif metric.value < 40:
                    score = 90 - (40 - metric.value) * 2
                else:
                    score = max(0, 100 - (metric.value - 50) * 3)
                scores.append(score)
        
        if scores:
            return sum(scores) / len(scores)
        else:
            return 50.0  # é»˜è®¤ä¸­ç­‰è¯„åˆ†


class RealTimeVisualizer:
    """å®æ—¶å¯è§†åŒ–å™¨"""
    
    def __init__(self, max_points: int = 100):
        self.max_points = max_points
        self.metric_data = defaultdict(lambda: {"x": deque(maxlen=max_points), 
                                              "y": deque(maxlen=max_points)})
        self.fig = None
        self.axes = {}
        self.animation = None
    
    def setup_realtime_plot(self, metric_names: List[str]):
        """è®¾ç½®å®æ—¶ç»˜å›¾"""
        if not MATPLOTLIB_AVAILABLE:
            logger.warning("Matplotlib ä¸å¯ç”¨ï¼Œè·³è¿‡å®æ—¶å¯è§†åŒ–")
            return
        
        plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
        
        # åˆ›å»ºå­å›¾
        n_metrics = len(metric_names)
        rows = (n_metrics + 1) // 2
        cols = 2 if n_metrics > 1 else 1
        
        self.fig, axes = plt.subplots(rows, cols, figsize=(12, 3*rows))
        if n_metrics == 1:
            axes = [axes]
        elif rows == 1:
            axes = list(axes)
        else:
            axes = [ax for row in axes for ax in row]
        
        for i, metric_name in enumerate(metric_names):
            if i < len(axes):
                self.axes[metric_name] = axes[i]
                self.axes[metric_name].set_title(metric_name)
                self.axes[metric_name].set_xlabel('æ—¶é—´')
                self.axes[metric_name].grid(True, alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_metrics, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def update_plot(self, metrics: List[PerformanceMetric]):
        """æ›´æ–°ç»˜å›¾"""
        if not MATPLOTLIB_AVAILABLE or not self.fig:
            return
        
        current_time = time.time()
        
        for metric in metrics:
            if metric.name in self.axes:
                self.metric_data[metric.name]["x"].append(current_time)
                self.metric_data[metric.name]["y"].append(metric.value)
                
                # æ›´æ–°å›¾è¡¨
                ax = self.axes[metric.name]
                ax.clear()
                
                x_data = list(self.metric_data[metric.name]["x"])
                y_data = list(self.metric_data[metric.name]["y"])
                
                if x_data and y_data:
                    ax.plot(x_data, y_data, 'b-', linewidth=2)
                    ax.set_title(f"{metric.name} ({metric.value:.2f} {metric.unit})")
                    ax.set_xlabel('æ—¶é—´')
                    ax.grid(True, alpha=0.3)
                    
                    # è®¾ç½®xè½´æ˜¾ç¤ºæœ€è¿‘çš„æ—¶é—´
                    if len(x_data) > 1:
                        ax.set_xlim(x_data[0], x_data[-1])
        
        plt.pause(0.01)  # çŸ­æš‚æš‚åœä»¥æ›´æ–°å›¾è¡¨


class YICAPerformanceMonitor:
    """YICA æ€§èƒ½ç›‘æ§ä¸»ç±»"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.collector = MetricCollector(
            collection_interval=self.config.get("collection_interval", 1.0)
        )
        self.detector = AnomalyDetector(
            window_size=self.config.get("window_size", 100)
        )
        self.analyzer = PerformanceAnalyzer()
        self.visualizer = RealTimeVisualizer(
            max_points=self.config.get("max_plot_points", 100)
        )
        
        self.alerts = []
        self.monitoring_active = False
        self.analysis_interval = self.config.get("analysis_interval", 10.0)
        self.last_analysis_time = 0
        
        # è®¾ç½®é»˜è®¤é˜ˆå€¼
        self._setup_default_thresholds()
        
        # è®¾ç½®å‘Šè­¦å›è°ƒ
        self.detector.add_anomaly_callback(self._handle_alert)
    
    def _setup_default_thresholds(self):
        """è®¾ç½®é»˜è®¤æ€§èƒ½é˜ˆå€¼"""
        thresholds = [
            PerformanceThreshold("cim_utilization", 90, 95, 98),
            PerformanceThreshold("memory_usage", 80, 90, 95),
            PerformanceThreshold("device_temperature", 60, 70, 80),
            PerformanceThreshold("inference_latency", 10, 20, 50),
            PerformanceThreshold("spm_cache_hit_rate", 70, 60, 50, "less_than"),
        ]
        
        for threshold in thresholds:
            self.detector.add_threshold(threshold)
    
    def _handle_alert(self, alert: Alert):
        """å¤„ç†å‘Šè­¦"""
        self.alerts.append(alert)
        
        # ä¿æŒæœ€è¿‘1000ä¸ªå‘Šè­¦
        if len(self.alerts) > 1000:
            self.alerts = self.alerts[-1000:]
    
    def start_monitoring(self, enable_visualization: bool = False, 
                        monitored_metrics: List[str] = None):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        if self.monitoring_active:
            logger.warning("æ€§èƒ½ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        logger.info("å¯åŠ¨ YICA æ€§èƒ½ç›‘æ§...")
        
        # å¯åŠ¨æŒ‡æ ‡æ”¶é›†
        self.collector.start_collection()
        
        # è®¾ç½®å¯è§†åŒ–
        if enable_visualization and monitored_metrics:
            self.visualizer.setup_realtime_plot(monitored_metrics)
        
        self.monitoring_active = True
        
        # å¯åŠ¨ç›‘æ§å¾ªç¯
        self._monitoring_loop(enable_visualization)
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        if not self.monitoring_active:
            return
        
        logger.info("åœæ­¢ YICA æ€§èƒ½ç›‘æ§...")
        
        self.monitoring_active = False
        self.collector.stop_collection()
        
        # å…³é—­å¯è§†åŒ–
        if MATPLOTLIB_AVAILABLE and self.visualizer.fig:
            plt.close(self.visualizer.fig)
    
    def _monitoring_loop(self, enable_visualization: bool):
        """ç›‘æ§ä¸»å¾ªç¯"""
        while self.monitoring_active:
            try:
                # è·å–æœ€æ–°æŒ‡æ ‡
                metrics = self.collector.get_latest_metrics()
                
                # å¼‚å¸¸æ£€æµ‹
                for metric in metrics:
                    self.detector.check_metric(metric)
                
                # æ›´æ–°å¯è§†åŒ–
                if enable_visualization and metrics:
                    self.visualizer.update_plot(metrics)
                
                # å®šæœŸæ€§èƒ½åˆ†æ
                current_time = time.time()
                if current_time - self.last_analysis_time >= self.analysis_interval:
                    self.analyzer.analyze_performance(metrics)
                    self.last_analysis_time = current_time
                
                time.sleep(0.1)  # çŸ­æš‚ä¼‘çœ 
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                time.sleep(1.0)
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç›‘æ§çŠ¶æ€"""
        recent_metrics = self.collector.get_latest_metrics(50)
        recent_alerts = [a for a in self.alerts if not a.resolved][-10:]
        
        status = {
            "monitoring_active": self.monitoring_active,
            "total_metrics_collected": len(recent_metrics),
            "active_alerts": len(recent_alerts),
            "recent_metrics": [asdict(m) for m in recent_metrics[-5:]],
            "recent_alerts": [asdict(a) for a in recent_alerts],
        }
        
        if recent_metrics:
            latest_analysis = self.analyzer.analyze_performance(recent_metrics)
            status["latest_analysis"] = latest_analysis
        
        return status
    
    def generate_report(self, output_file: str = None) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        if output_file is None:
            output_file = f"yica_monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "monitoring_config": self.config,
                "report_version": "1.0"
            },
            "monitoring_summary": self.get_current_status(),
            "analysis_history": self.analyzer.analysis_history[-10:],  # æœ€è¿‘10æ¬¡åˆ†æ
            "alert_summary": {
                "total_alerts": len(self.alerts),
                "alerts_by_level": self._summarize_alerts_by_level(),
                "recent_alerts": [asdict(a) for a in self.alerts[-20:]]
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return output_file
    
    def _summarize_alerts_by_level(self) -> Dict[str, int]:
        """æŒ‰çº§åˆ«æ±‡æ€»å‘Šè­¦"""
        summary = {level.value: 0 for level in AlertLevel}
        
        for alert in self.alerts:
            summary[alert.level.value] += 1
        
        return summary


def main():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§åŠŸèƒ½"""
    print("ğŸ” YICA é«˜çº§æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    config = {
        "collection_interval": 0.5,
        "analysis_interval": 5.0,
        "window_size": 50
    }
    
    monitor = YICAPerformanceMonitor(config)
    
    try:
        # å¯åŠ¨ç›‘æ§
        monitored_metrics = [
            "cim_utilization", 
            "memory_usage", 
            "device_temperature", 
            "inference_latency"
        ]
        
        print("ğŸš€ å¯åŠ¨æ€§èƒ½ç›‘æ§...")
        monitor.start_monitoring(
            enable_visualization=False,  # è®¾ç½®ä¸ºTrueå¯å¯ç”¨å®æ—¶å¯è§†åŒ–
            monitored_metrics=monitored_metrics
        )
        
        # è¿è¡Œç›‘æ§ä¸€æ®µæ—¶é—´
        print("ğŸ“Š ç›‘æ§è¿è¡Œä¸­... (æŒ‰ Ctrl+C åœæ­¢)")
        
        for i in range(30):  # è¿è¡Œ30ç§’
            time.sleep(1)
            
            if i % 10 == 0:
                status = monitor.get_current_status()
                print(f"  ç›‘æ§çŠ¶æ€: {status['total_metrics_collected']} ä¸ªæŒ‡æ ‡, "
                      f"{status['active_alerts']} ä¸ªæ´»è·ƒå‘Šè­¦")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = monitor.generate_report()
        print(f"ğŸ“‹ ç›‘æ§æŠ¥å‘Š: {report_file}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ç›‘æ§")
    
    finally:
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        print("âœ… æ€§èƒ½ç›‘æ§æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    main() 