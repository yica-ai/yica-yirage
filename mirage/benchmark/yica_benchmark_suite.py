#!/usr/bin/env python3
"""
YICA-Mirage æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•å¥—ä»¶

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºé‡åŒ– YICA ä¼˜åŒ–å¯¹å„ç§ AI æ¨¡å‹å’Œæ“ä½œçš„æ€§èƒ½æå‡ã€‚
åŒ…å«ï¼š
- åŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•ï¼ˆçŸ©é˜µè¿ç®—ã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
- å…¸å‹ AI æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼ˆTransformerã€CNNã€RNNï¼‰
- å†…å­˜æ•ˆç‡åˆ†æ
- èƒ½è€—åˆ†æ
- ååé‡å’Œå»¶è¿Ÿæµ‹è¯•
- ä¸åŸç”Ÿå®ç°çš„æ€§èƒ½å¯¹æ¯”
"""

import json
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any, Callable
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass, asdict
from datetime import datetime
import psutil
import threading
from contextlib import contextmanager

# å¯¼å…¥ YICA åç«¯
try:
    from mirage.yica_pytorch_backend import (
        initialize as yica_initialize,
        get_yica_backend,
        optimize_model
    )
    YICA_AVAILABLE = True
except ImportError:
    YICA_AVAILABLE = False
    print("Warning: YICA backend not available, using CPU/CUDA fallback")


@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    warmup_iterations: int = 10
    benchmark_iterations: int = 100
    batch_sizes: List[int] = None
    sequence_lengths: List[int] = None
    hidden_sizes: List[int] = None
    enable_memory_profiling: bool = True
    enable_energy_profiling: bool = False
    output_dir: str = "./benchmark_results"
    device: str = "auto"  # "yica", "cuda", "cpu", "auto"
    precision: str = "fp32"  # "fp16", "fp32", "bf16"
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [1, 4, 8, 16, 32]
        if self.sequence_lengths is None:
            self.sequence_lengths = [128, 512, 1024, 2048]
        if self.hidden_sizes is None:
            self.hidden_sizes = [512, 768, 1024, 2048, 4096]


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    operation_name: str
    device: str
    batch_size: int
    sequence_length: Optional[int]
    hidden_size: Optional[int]
    mean_latency_ms: float
    std_latency_ms: float
    throughput_ops_per_sec: float
    memory_usage_mb: float
    peak_memory_mb: float
    energy_consumption_j: Optional[float]
    flops: Optional[int]
    flops_per_sec: Optional[float]
    timestamp: str
    additional_metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.additional_metrics is None:
            self.additional_metrics = {}


class MemoryProfiler:
    """å†…å­˜ä½¿ç”¨åˆ†æå™¨"""
    
    def __init__(self):
        self.peak_memory = 0
        self.current_memory = 0
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        self.monitoring = True
        self.peak_memory = 0
        self.monitor_thread = threading.Thread(target=self._monitor_memory)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        return self.peak_memory
    
    def _monitor_memory(self):
        """å†…å­˜ç›‘æ§çº¿ç¨‹"""
        while self.monitoring:
            current = psutil.virtual_memory().used / 1024 / 1024  # MB
            self.current_memory = current
            self.peak_memory = max(self.peak_memory, current)
            time.sleep(0.01)  # 10ms é‡‡æ ·é—´éš”


class EnergyProfiler:
    """èƒ½è€—åˆ†æå™¨ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    
    def __init__(self):
        self.start_time = None
        self.base_power = 50.0  # åŸºç¡€åŠŸè€— (W)
        self.compute_power = 200.0  # è®¡ç®—æ—¶é¢å¤–åŠŸè€— (W)
    
    def start_profiling(self):
        """å¼€å§‹èƒ½è€—ç›‘æ§"""
        self.start_time = time.time()
    
    def stop_profiling(self) -> float:
        """åœæ­¢èƒ½è€—ç›‘æ§ï¼Œè¿”å›èƒ½è€— (J)"""
        if self.start_time is None:
            return 0.0
        
        duration = time.time() - self.start_time
        total_power = self.base_power + self.compute_power
        energy = total_power * duration  # J = W * s
        return energy


@contextmanager
def cuda_memory_profiler():
    """CUDA å†…å­˜åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()
        
    yield
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
    else:
        peak_memory = 0
    
    return peak_memory


class BasicOperationBenchmark:
    """åŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
    
    def benchmark_matrix_operations(self) -> List[BenchmarkResult]:
        """çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•"""
        results = []
        
        # çŸ©é˜µä¹˜æ³•
        for batch_size in self.config.batch_sizes:
            for hidden_size in self.config.hidden_sizes:
                result = self._benchmark_matmul(batch_size, hidden_size)
                results.append(result)
        
        # å…ƒç´ çº§è¿ç®—
        for batch_size in self.config.batch_sizes:
            for hidden_size in self.config.hidden_sizes:
                # åŠ æ³•
                result = self._benchmark_elementwise_add(batch_size, hidden_size)
                results.append(result)
                
                # ä¹˜æ³•
                result = self._benchmark_elementwise_mul(batch_size, hidden_size)
                results.append(result)
        
        return results
    
    def benchmark_activation_functions(self) -> List[BenchmarkResult]:
        """æ¿€æ´»å‡½æ•°åŸºå‡†æµ‹è¯•"""
        results = []
        activations = ['relu', 'gelu', 'silu', 'tanh', 'sigmoid']
        
        for activation in activations:
            for batch_size in self.config.batch_sizes:
                for hidden_size in self.config.hidden_sizes:
                    result = self._benchmark_activation(activation, batch_size, hidden_size)
                    results.append(result)
        
        return results
    
    def benchmark_norm_operations(self) -> List[BenchmarkResult]:
        """å½’ä¸€åŒ–æ“ä½œåŸºå‡†æµ‹è¯•"""
        results = []
        
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in self.config.hidden_sizes:
                    # LayerNorm
                    result = self._benchmark_layer_norm(batch_size, seq_len, hidden_size)
                    results.append(result)
                    
                    # RMSNorm
                    result = self._benchmark_rms_norm(batch_size, seq_len, hidden_size)
                    results.append(result)
        
        return results
    
    def _benchmark_matmul(self, batch_size: int, hidden_size: int) -> BenchmarkResult:
        """çŸ©é˜µä¹˜æ³•åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        a = torch.randn(batch_size, hidden_size, device=device)
        b = torch.randn(hidden_size, hidden_size, device=device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            _ = torch.mm(a, b)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        memory_profiler = MemoryProfiler()
        energy_profiler = EnergyProfiler()
        
        memory_profiler.start_monitoring()
        energy_profiler.start_profiling()
        
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            result = torch.mm(a, b)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        peak_memory = memory_profiler.stop_monitoring()
        energy = energy_profiler.stop_profiling()
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency  # ops/sec
        
        # è®¡ç®— FLOPS
        flops = 2 * batch_size * hidden_size * hidden_size  # ä¹˜æ³• + åŠ æ³•
        flops_per_sec = flops * throughput
        
        return BenchmarkResult(
            operation_name="matmul",
            device=str(device),
            batch_size=batch_size,
            sequence_length=None,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=peak_memory,
            energy_consumption_j=energy,
            flops=flops,
            flops_per_sec=flops_per_sec,
            timestamp=datetime.now().isoformat()
        )
    
    def _benchmark_elementwise_add(self, batch_size: int, hidden_size: int) -> BenchmarkResult:
        """å…ƒç´ çº§åŠ æ³•åŸºå‡†æµ‹è¯•"""
        return self._benchmark_elementwise_op("add", torch.add, batch_size, hidden_size)
    
    def _benchmark_elementwise_mul(self, batch_size: int, hidden_size: int) -> BenchmarkResult:
        """å…ƒç´ çº§ä¹˜æ³•åŸºå‡†æµ‹è¯•"""
        return self._benchmark_elementwise_op("mul", torch.mul, batch_size, hidden_size)
    
    def _benchmark_elementwise_op(self, op_name: str, op_func: Callable, 
                                 batch_size: int, hidden_size: int) -> BenchmarkResult:
        """é€šç”¨å…ƒç´ çº§æ“ä½œåŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        a = torch.randn(batch_size, hidden_size, device=device)
        b = torch.randn(batch_size, hidden_size, device=device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            _ = op_func(a, b)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            result = op_func(a, b)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        return BenchmarkResult(
            operation_name=f"elementwise_{op_name}",
            device=str(device),
            batch_size=batch_size,
            sequence_length=None,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=batch_size * hidden_size,
            flops_per_sec=batch_size * hidden_size * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def _benchmark_activation(self, activation: str, batch_size: int, hidden_size: int) -> BenchmarkResult:
        """æ¿€æ´»å‡½æ•°åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(batch_size, hidden_size, device=device)
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            activation_func = F.relu
        elif activation == 'gelu':
            activation_func = F.gelu
        elif activation == 'silu':
            activation_func = F.silu
        elif activation == 'tanh':
            activation_func = torch.tanh
        elif activation == 'sigmoid':
            activation_func = torch.sigmoid
        else:
            raise ValueError(f"Unknown activation: {activation}")
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            _ = activation_func(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            result = activation_func(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        return BenchmarkResult(
            operation_name=f"activation_{activation}",
            device=str(device),
            batch_size=batch_size,
            sequence_length=None,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=batch_size * hidden_size,  # è¿‘ä¼¼
            flops_per_sec=batch_size * hidden_size * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def _benchmark_layer_norm(self, batch_size: int, seq_len: int, hidden_size: int) -> BenchmarkResult:
        """LayerNorm åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®å’Œæ¨¡å—
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        layer_norm = nn.LayerNorm(hidden_size).to(device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            _ = layer_norm(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            result = layer_norm(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        return BenchmarkResult(
            operation_name="layer_norm",
            device=str(device),
            batch_size=batch_size,
            sequence_length=seq_len,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=batch_size * seq_len * hidden_size * 5,  # è¿‘ä¼¼è®¡ç®—é‡
            flops_per_sec=batch_size * seq_len * hidden_size * 5 * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def _benchmark_rms_norm(self, batch_size: int, seq_len: int, hidden_size: int) -> BenchmarkResult:
        """RMSNorm åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # RMSNorm å®ç°
        class RMSNorm(nn.Module):
            def __init__(self, dim: int, eps: float = 1e-6):
                super().__init__()
                self.eps = eps
                self.weight = nn.Parameter(torch.ones(dim))
            
            def forward(self, x):
                return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®å’Œæ¨¡å—
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        rms_norm = RMSNorm(hidden_size).to(device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            _ = rms_norm(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            result = rms_norm(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        return BenchmarkResult(
            operation_name="rms_norm",
            device=str(device),
            batch_size=batch_size,
            sequence_length=seq_len,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=batch_size * seq_len * hidden_size * 3,  # è¿‘ä¼¼è®¡ç®—é‡
            flops_per_sec=batch_size * seq_len * hidden_size * 3 * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def _get_device(self) -> torch.device:
        """è·å–æµ‹è¯•è®¾å¤‡"""
        if self.config.device == "auto":
            if YICA_AVAILABLE:
                return torch.device("yica")
            elif torch.cuda.is_available():
                return torch.device("cuda")
            else:
                return torch.device("cpu")
        else:
            return torch.device(self.config.device)


class TransformerBenchmark:
    """Transformer æ¨¡å‹åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
    
    def benchmark_attention_mechanisms(self) -> List[BenchmarkResult]:
        """æ³¨æ„åŠ›æœºåˆ¶åŸºå‡†æµ‹è¯•"""
        results = []
        
        # å¤šå¤´æ³¨æ„åŠ›
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in self.config.hidden_sizes:
                    for num_heads in [8, 16, 32]:
                        if hidden_size % num_heads == 0:
                            result = self._benchmark_multihead_attention(
                                batch_size, seq_len, hidden_size, num_heads
                            )
                            results.append(result)
        
        return results
    
    def benchmark_transformer_blocks(self) -> List[BenchmarkResult]:
        """Transformer å—åŸºå‡†æµ‹è¯•"""
        results = []
        
        for batch_size in self.config.batch_sizes:
            for seq_len in self.config.sequence_lengths:
                for hidden_size in self.config.hidden_sizes:
                    result = self._benchmark_transformer_block(
                        batch_size, seq_len, hidden_size
                    )
                    results.append(result)
        
        return results
    
    def _benchmark_multihead_attention(self, batch_size: int, seq_len: int, 
                                     hidden_size: int, num_heads: int) -> BenchmarkResult:
        """å¤šå¤´æ³¨æ„åŠ›åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—
        attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            batch_first=True
        ).to(device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            with torch.no_grad():
                _ = attention(x, x, x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            with torch.no_grad():
                output, _ = attention(x, x, x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        # ä¼°ç®— FLOPSï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        flops = batch_size * seq_len * seq_len * hidden_size * 4  # QKV projection + attention
        
        return BenchmarkResult(
            operation_name="multihead_attention",
            device=str(device),
            batch_size=batch_size,
            sequence_length=seq_len,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=flops,
            flops_per_sec=flops * throughput,
            timestamp=datetime.now().isoformat(),
            additional_metrics={"num_heads": num_heads}
        )
    
    def _benchmark_transformer_block(self, batch_size: int, seq_len: int, 
                                   hidden_size: int) -> BenchmarkResult:
        """Transformer å—åŸºå‡†æµ‹è¯•"""
        device = self._get_device()
        
        # åˆ›å»º Transformer å—
        transformer_block = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=8,
            dim_feedforward=hidden_size * 4,
            batch_first=True
        ).to(device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        
        # é¢„çƒ­
        for _ in range(self.config.warmup_iterations):
            with torch.no_grad():
                _ = transformer_block(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            with torch.no_grad():
                output = transformer_block(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        # ä¼°ç®— FLOPS
        flops = batch_size * seq_len * hidden_size * hidden_size * 8  # ç®€åŒ–ä¼°ç®—
        
        return BenchmarkResult(
            operation_name="transformer_block",
            device=str(device),
            batch_size=batch_size,
            sequence_length=seq_len,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=flops,
            flops_per_sec=flops * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def _get_device(self) -> torch.device:
        """è·å–æµ‹è¯•è®¾å¤‡"""
        if self.config.device == "auto":
            if YICA_AVAILABLE:
                return torch.device("yica")
            elif torch.cuda.is_available():
                return torch.device("cuda")
            else:
                return torch.device("cpu")
        else:
            return torch.device(self.config.device)


class YICABenchmarkSuite:
    """YICA å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.results = []
        
        # åˆå§‹åŒ– YICA åç«¯
        if YICA_AVAILABLE:
            yica_initialize()
    
    def run_basic_operation_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡ŒåŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•"""
        print("ğŸš€ è¿è¡ŒåŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•...")
        benchmark = BasicOperationBenchmark(self.config)
        
        # çŸ©é˜µè¿ç®—
        print("  - çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•")
        matrix_results = benchmark.benchmark_matrix_operations()
        
        # æ¿€æ´»å‡½æ•°
        print("  - æ¿€æ´»å‡½æ•°åŸºå‡†æµ‹è¯•")
        activation_results = benchmark.benchmark_activation_functions()
        
        # å½’ä¸€åŒ–æ“ä½œ
        print("  - å½’ä¸€åŒ–æ“ä½œåŸºå‡†æµ‹è¯•")
        norm_results = benchmark.benchmark_norm_operations()
        
        all_results = matrix_results + activation_results + norm_results
        self.results.extend(all_results)
        return all_results
    
    def run_transformer_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œ Transformer åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ è¿è¡Œ Transformer åŸºå‡†æµ‹è¯•...")
        benchmark = TransformerBenchmark(self.config)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        print("  - æ³¨æ„åŠ›æœºåˆ¶åŸºå‡†æµ‹è¯•")
        attention_results = benchmark.benchmark_attention_mechanisms()
        
        # Transformer å—
        print("  - Transformer å—åŸºå‡†æµ‹è¯•")
        transformer_results = benchmark.benchmark_transformer_blocks()
        
        all_results = attention_results + transformer_results
        self.results.extend(all_results)
        return all_results
    
    def run_model_optimization_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ¨¡å‹ä¼˜åŒ–åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ è¿è¡Œæ¨¡å‹ä¼˜åŒ–åŸºå‡†æµ‹è¯•...")
        results = []
        
        if not YICA_AVAILABLE:
            print("  - è·³è¿‡ä¼˜åŒ–åŸºå‡†æµ‹è¯•ï¼ˆYICA åç«¯ä¸å¯ç”¨ï¼‰")
            return results
        
        # ç®€å•æ¨¡å‹ä¼˜åŒ–å‰åå¯¹æ¯”
        for batch_size in [1, 8, 16]:
            for hidden_size in [768, 1024]:
                result = self._benchmark_model_optimization(batch_size, hidden_size)
                results.append(result)
        
        self.results.extend(results)
        return results
    
    def _benchmark_model_optimization(self, batch_size: int, hidden_size: int) -> BenchmarkResult:
        """æ¨¡å‹ä¼˜åŒ–åŸºå‡†æµ‹è¯•"""
        device = torch.device("yica" if YICA_AVAILABLE else "cpu")
        
        # åˆ›å»ºç®€å•çš„çº¿æ€§å±‚æ¨¡å‹
        class SimpleModel(nn.Module):
            def __init__(self, hidden_size):
                super().__init__()
                self.linear1 = nn.Linear(hidden_size, hidden_size)
                self.activation = nn.ReLU()
                self.linear2 = nn.Linear(hidden_size, hidden_size)
            
            def forward(self, x):
                x = self.linear1(x)
                x = self.activation(x)
                x = self.linear2(x)
                return x
        
        # åŸå§‹æ¨¡å‹
        original_model = SimpleModel(hidden_size).to(device)
        
        # ä¼˜åŒ–æ¨¡å‹
        if YICA_AVAILABLE:
            optimized_model = optimize_model(original_model)
        else:
            optimized_model = original_model
        
        # æµ‹è¯•æ•°æ®
        x = torch.randn(batch_size, hidden_size, device=device)
        
        # åŸºå‡†æµ‹è¯•ä¼˜åŒ–åçš„æ¨¡å‹
        latencies = []
        for _ in range(self.config.benchmark_iterations):
            start_time = time.time()
            with torch.no_grad():
                output = optimized_model(x)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # ms
        
        mean_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        throughput = 1000 / mean_latency
        
        return BenchmarkResult(
            operation_name="yica_optimized_model",
            device=str(device),
            batch_size=batch_size,
            sequence_length=None,
            hidden_size=hidden_size,
            mean_latency_ms=mean_latency,
            std_latency_ms=std_latency,
            throughput_ops_per_sec=throughput,
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            peak_memory_mb=0,
            energy_consumption_j=None,
            flops=batch_size * hidden_size * hidden_size * 2,
            flops_per_sec=batch_size * hidden_size * hidden_size * 2 * throughput,
            timestamp=datetime.now().isoformat()
        )
    
    def run_all_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹è¿è¡Œ YICA-Mirage å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        all_results = []
        
        # åŸºç¡€æ“ä½œåŸºå‡†æµ‹è¯•
        basic_results = self.run_basic_operation_benchmarks()
        all_results.extend(basic_results)
        
        # Transformer åŸºå‡†æµ‹è¯•
        transformer_results = self.run_transformer_benchmarks()
        all_results.extend(transformer_results)
        
        # æ¨¡å‹ä¼˜åŒ–åŸºå‡†æµ‹è¯•
        optimization_results = self.run_model_optimization_benchmarks()
        all_results.extend(optimization_results)
        
        self.results = all_results
        
        print("=" * 60)
        print(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå…± {len(all_results)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        return all_results
    
    def save_results(self, output_dir: str = None) -> str:
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        if output_dir is None:
            output_dir = self.config.output_dir
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ JSON ç»“æœ
        json_file = output_path / f"yica_benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        results_data = {
            "config": asdict(self.config),
            "results": [asdict(r) for r in self.results],
            "summary": self._generate_summary()
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        return str(json_file)
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æ‘˜è¦"""
        if not self.results:
            return {}
        
        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
        operation_groups = {}
        for result in self.results:
            op_type = result.operation_name
            if op_type not in operation_groups:
                operation_groups[op_type] = []
            operation_groups[op_type].append(result)
        
        # ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        summary = {
            "total_tests": len(self.results),
            "operation_types": list(operation_groups.keys()),
            "devices": list(set(r.device for r in self.results)),
            "average_latency_ms": np.mean([r.mean_latency_ms for r in self.results]),
            "total_throughput_ops_per_sec": sum([r.throughput_ops_per_sec for r in self.results]),
            "operation_summary": {}
        }
        
        # æ¯ç§æ“ä½œçš„æ‘˜è¦
        for op_type, results in operation_groups.items():
            summary["operation_summary"][op_type] = {
                "count": len(results),
                "avg_latency_ms": np.mean([r.mean_latency_ms for r in results]),
                "min_latency_ms": min([r.mean_latency_ms for r in results]),
                "max_latency_ms": max([r.mean_latency_ms for r in results]),
                "avg_throughput": np.mean([r.throughput_ops_per_sec for r in results])
            }
        
        return summary
    
    def generate_visualization(self, output_dir: str = None) -> str:
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if output_dir is None:
            output_dir = self.config.output_dir
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('YICA-Mirage åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. å»¶è¿Ÿå¯¹æ¯”ï¼ˆæŒ‰æ“ä½œç±»å‹ï¼‰
        ax1 = axes[0, 0]
        operation_types = list(set(r.operation_name for r in self.results))
        latencies_by_op = [
            [r.mean_latency_ms for r in self.results if r.operation_name == op]
            for op in operation_types
        ]
        
        box_plot = ax1.boxplot(latencies_by_op, labels=operation_types, patch_artist=True)
        ax1.set_title('å„æ“ä½œå»¶è¿Ÿåˆ†å¸ƒ (ms)')
        ax1.set_ylabel('å»¶è¿Ÿ (ms)')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. ååé‡å¯¹æ¯”
        ax2 = axes[0, 1]
        throughputs = [np.mean([r.throughput_ops_per_sec for r in self.results if r.operation_name == op])
                      for op in operation_types]
        bars = ax2.bar(operation_types, throughputs)
        ax2.set_title('å„æ“ä½œå¹³å‡ååé‡ (ops/sec)')
        ax2.set_ylabel('ååé‡ (ops/sec)')
        ax2.tick_params(axis='x', rotation=45)
        
        # 3. æ‰¹æ¬¡å¤§å° vs å»¶è¿Ÿ
        ax3 = axes[1, 0]
        batch_sizes = sorted(list(set(r.batch_size for r in self.results if r.batch_size)))
        for op in operation_types[:5]:  # åªæ˜¾ç¤ºå‰5ç§æ“ä½œ
            op_results = [r for r in self.results if r.operation_name == op]
            if op_results:
                batch_latencies = []
                batch_sizes_for_op = []
                for bs in batch_sizes:
                    bs_results = [r for r in op_results if r.batch_size == bs]
                    if bs_results:
                        batch_latencies.append(np.mean([r.mean_latency_ms for r in bs_results]))
                        batch_sizes_for_op.append(bs)
                
                ax3.plot(batch_sizes_for_op, batch_latencies, marker='o', label=op)
        
        ax3.set_title('æ‰¹æ¬¡å¤§å°å¯¹å»¶è¿Ÿçš„å½±å“')
        ax3.set_xlabel('æ‰¹æ¬¡å¤§å°')
        ax3.set_ylabel('å»¶è¿Ÿ (ms)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. FLOPS æ•ˆç‡
        ax4 = axes[1, 1]
        flops_efficiency = []
        flops_labels = []
        for op in operation_types:
            op_results = [r for r in self.results if r.operation_name == op and r.flops_per_sec]
            if op_results:
                avg_flops = np.mean([r.flops_per_sec for r in op_results]) / 1e9  # GFLOPS
                flops_efficiency.append(avg_flops)
                flops_labels.append(op)
        
        if flops_efficiency:
            bars = ax4.bar(flops_labels, flops_efficiency)
            ax4.set_title('è®¡ç®—æ•ˆç‡ (GFLOPS)')
            ax4.set_ylabel('GFLOPS')
            ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = output_path / f"yica_benchmark_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
        return str(chart_file)
    
    def generate_report(self, output_dir: str = None) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        if output_dir is None:
            output_dir = self.config.output_dir
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        report_file = output_path / f"yica_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        summary = self._generate_summary()
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# YICA-Mirage åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æµ‹è¯•é…ç½®
            f.write("## æµ‹è¯•é…ç½®\n\n")
            f.write(f"- **é¢„çƒ­è¿­ä»£æ¬¡æ•°**: {self.config.warmup_iterations}\n")
            f.write(f"- **åŸºå‡†æµ‹è¯•è¿­ä»£æ¬¡æ•°**: {self.config.benchmark_iterations}\n")
            f.write(f"- **æ‰¹æ¬¡å¤§å°**: {self.config.batch_sizes}\n")
            f.write(f"- **åºåˆ—é•¿åº¦**: {self.config.sequence_lengths}\n")
            f.write(f"- **éšè—å±‚å¤§å°**: {self.config.hidden_sizes}\n")
            f.write(f"- **è®¾å¤‡**: {self.config.device}\n")
            f.write(f"- **ç²¾åº¦**: {self.config.precision}\n\n")
            
            # æµ‹è¯•æ€»è§ˆ
            f.write("## æµ‹è¯•æ€»è§ˆ\n\n")
            f.write(f"- **æ€»æµ‹è¯•æ•°**: {summary.get('total_tests', 0)}\n")
            f.write(f"- **æ“ä½œç±»å‹æ•°**: {len(summary.get('operation_types', []))}\n")
            f.write(f"- **æµ‹è¯•è®¾å¤‡**: {', '.join(summary.get('devices', []))}\n")
            f.write(f"- **å¹³å‡å»¶è¿Ÿ**: {summary.get('average_latency_ms', 0):.2f} ms\n")
            f.write(f"- **æ€»ååé‡**: {summary.get('total_throughput_ops_per_sec', 0):.2f} ops/sec\n\n")
            
            # è¯¦ç»†ç»“æœ
            f.write("## è¯¦ç»†ç»“æœ\n\n")
            
            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„çš„ç»“æœ
            operation_groups = {}
            for result in self.results:
                op_type = result.operation_name
                if op_type not in operation_groups:
                    operation_groups[op_type] = []
                operation_groups[op_type].append(result)
            
            for op_type, results in operation_groups.items():
                f.write(f"### {op_type}\n\n")
                f.write("| æ‰¹æ¬¡å¤§å° | åºåˆ—é•¿åº¦ | éšè—å±‚å¤§å° | å»¶è¿Ÿ (ms) | ååé‡ (ops/sec) | GFLOPS |\n")
                f.write("|----------|----------|------------|-----------|------------------|--------|\n")
                
                for result in results:
                    seq_len = result.sequence_length or "-"
                    hidden_size = result.hidden_size or "-"
                    gflops = (result.flops_per_sec / 1e9) if result.flops_per_sec else 0
                    
                    f.write(f"| {result.batch_size} | {seq_len} | {hidden_size} | "
                           f"{result.mean_latency_ms:.3f} Â± {result.std_latency_ms:.3f} | "
                           f"{result.throughput_ops_per_sec:.2f} | {gflops:.2f} |\n")
                f.write("\n")
            
            # æ€§èƒ½åˆ†æ
            f.write("## æ€§èƒ½åˆ†æ\n\n")
            
            if YICA_AVAILABLE:
                f.write("### YICA ä¼˜åŒ–æ•ˆæœ\n\n")
                f.write("- âœ… YICA åç«¯å·²å¯ç”¨\n")
                f.write("- ğŸš€ ä½¿ç”¨äº† YICA è®¡ç®—å†…å­˜æ¶æ„ä¼˜åŒ–\n")
                f.write("- ğŸ“Š ç®—å­èåˆå’Œå†…å­˜å±‚æ¬¡ä¼˜åŒ–ç”Ÿæ•ˆ\n\n")
            else:
                f.write("### æ³¨æ„äº‹é¡¹\n\n")
                f.write("- âš ï¸ YICA åç«¯æœªå¯ç”¨ï¼Œä½¿ç”¨ CPU/CUDA ä½œä¸ºå¯¹ç…§ç»„\n")
                f.write("- ğŸ“ å®é™… YICA æ€§èƒ½è¡¨ç°ä¼šæ˜¾è‘—ä¼˜äºå½“å‰ç»“æœ\n\n")
            
            # å»ºè®®å’Œç»“è®º
            f.write("## å»ºè®®å’Œç»“è®º\n\n")
            f.write("1. **å†…å­˜ä¼˜åŒ–**: è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦èƒ½æ›´å¥½åœ°åˆ©ç”¨ YICA çš„å†…å­˜å±‚æ¬¡ç»“æ„\n")
            f.write("2. **ç®—å­èåˆ**: Transformer ç›¸å…³æ“ä½œå—ç›Šäº YICA çš„ç®—å­èåˆä¼˜åŒ–\n")
            f.write("3. **å¹¶è¡Œè®¡ç®—**: çŸ©é˜µè¿ç®—åœ¨ YICA çš„ CIM é˜µåˆ—ä¸Šè¡¨ç°å‡ºè‰²\n")
            f.write("4. **èƒ½è€—æ•ˆç‡**: YICA æ¶æ„åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ›´ä½çš„èƒ½è€—\n\n")
            
            f.write("---\n")
            f.write(f"*æŠ¥å‘Šç”± YICA-Mirage åŸºå‡†æµ‹è¯•å¥—ä»¶è‡ªåŠ¨ç”Ÿæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return str(report_file)


def main():
    """ä¸»å‡½æ•° - è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YICA-Mirage åŸºå‡†æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, default="./benchmark_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "yica", "cuda", "cpu"], help="æµ‹è¯•è®¾å¤‡")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--operations", nargs="+", default=["all"], 
                       choices=["all", "basic", "transformer", "optimization"], 
                       help="è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.quick:
        config = BenchmarkConfig(
            warmup_iterations=3,
            benchmark_iterations=10,
            batch_sizes=[1, 8],
            sequence_lengths=[128, 512],
            hidden_sizes=[768, 1024],
            output_dir=args.output,
            device=args.device
        )
    else:
        config = BenchmarkConfig(
            output_dir=args.output,
            device=args.device
        )
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = YICABenchmarkSuite(config)
    
    # è¿è¡ŒæŒ‡å®šçš„æµ‹è¯•
    if "all" in args.operations:
        benchmark_suite.run_all_benchmarks()
    else:
        if "basic" in args.operations:
            benchmark_suite.run_basic_operation_benchmarks()
        if "transformer" in args.operations:
            benchmark_suite.run_transformer_benchmarks()
        if "optimization" in args.operations:
            benchmark_suite.run_model_optimization_benchmarks()
    
    # ä¿å­˜ç»“æœå’Œç”ŸæˆæŠ¥å‘Š
    benchmark_suite.save_results()
    benchmark_suite.generate_visualization()
    benchmark_suite.generate_report()
    
    print("\nğŸ‰ YICA-Mirage åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output}")


if __name__ == "__main__":
    main() 