#!/usr/bin/env python3
"""
YICA-Yirage åŸºç¡€åŸºå‡†æµ‹è¯•

ä»¿ç…§ç°æœ‰åŸºå‡†æµ‹è¯•é¡¹ç›®ï¼Œä¸º YICA åŠŸèƒ½åˆ›å»ºå¯¹åº”çš„åŸºå‡†æµ‹è¯•ã€‚
åŒ…å«ï¼š
- çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•
- æ³¨æ„åŠ›æœºåˆ¶åŸºå‡†æµ‹è¯•  
- MLP åŸºå‡†æµ‹è¯•
- LoRA åŸºå‡†æµ‹è¯•
- æ€§èƒ½å¯¹æ¯”åˆ†æ
"""

import sys
import time
import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# æ·»åŠ  yirage åŒ…è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "yirage" / "python"))

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import yirage
    YIRAGE_AVAILABLE = True
except ImportError:
    YIRAGE_AVAILABLE = False


class YICABenchmarkRunner:
    """YICA åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, warmup_iters: int = 16, profile_iters: int = 1000):
        self.warmup_iters = warmup_iters
        self.profile_iters = profile_iters
        self.results = {}
        
    def time_operation(self, operation_func, *args, **kwargs) -> float:
        """æµ‹é‡æ“ä½œæ‰§è¡Œæ—¶é—´"""
        if not TORCH_AVAILABLE:
            # ä½¿ç”¨ time.time() è¿›è¡ŒåŸºå‡†æµ‹è¯•
            for _ in range(self.warmup_iters):
                operation_func(*args, **kwargs)
            
            start_time = time.time()
            for _ in range(self.profile_iters):
                operation_func(*args, **kwargs)
            end_time = time.time()
            
            return (end_time - start_time) * 1000 / self.profile_iters  # ms
        else:
            # ä½¿ç”¨ CUDA äº‹ä»¶è¿›è¡Œæ›´ç²¾ç¡®çš„æµ‹è¯•
            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
            
            # é¢„çƒ­
            for _ in range(self.warmup_iters):
                operation_func(*args, **kwargs)
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                starter = torch.cuda.Event(enable_timing=True)
                ender = torch.cuda.Event(enable_timing=True)
                
                starter.record()
                for _ in range(self.profile_iters):
                    operation_func(*args, **kwargs)
                ender.record()
                
                torch.cuda.synchronize()
                return starter.elapsed_time(ender) / self.profile_iters  # ms
            else:
                start_time = time.time()
                for _ in range(self.profile_iters):
                    operation_func(*args, **kwargs)
                end_time = time.time()
                
                return (end_time - start_time) * 1000 / self.profile_iters  # ms

    def benchmark_gated_mlp(self, batch_size: int = 8, hidden_size: int = 4096) -> Dict:
        """åŸºå‡†æµ‹è¯•é—¨æ§ MLP"""
        print(f"ğŸ§  åŸºå‡†æµ‹è¯•é—¨æ§ MLP (batch_size={batch_size}, hidden_size={hidden_size})")
        
        results = {
            "batch_size": batch_size,
            "hidden_size": hidden_size,
            "test_type": "gated_mlp"
        }
        
        if not TORCH_AVAILABLE:
            # NumPy å®ç°
            if NUMPY_AVAILABLE:
                def numpy_gated_mlp():
                    x = np.random.randn(batch_size, hidden_size).astype(np.float32)
                    w1 = np.random.randn(hidden_size, hidden_size).astype(np.float32)
                    w2 = np.random.randn(hidden_size, hidden_size).astype(np.float32)
                    
                    o1 = np.dot(x, w1)
                    o2 = np.dot(x, w2)
                    # ç®€åŒ–çš„ SiLU æ¿€æ´»
                    o1_activated = o1 / (1 + np.exp(-o1))
                    result = o1_activated * o2
                    return result
                
                numpy_time = self.time_operation(numpy_gated_mlp)
                results["numpy_time_ms"] = numpy_time
                results["numpy_throughput"] = 1000 / numpy_time
                
                print(f"  NumPy å®ç°: {numpy_time:.3f} ms")
            else:
                results["numpy_time_ms"] = "N/A - NumPy not available"
        else:
            # PyTorch å®ç°
            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
            
            def torch_gated_mlp():
                x = torch.randn(batch_size, hidden_size, device=device, dtype=torch.float16)
                w1 = torch.randn(hidden_size, hidden_size, device=device, dtype=torch.float16)
                w2 = torch.randn(hidden_size, hidden_size, device=device, dtype=torch.float16)
                
                o1 = torch.mm(x, w1)
                o2 = torch.mm(x, w2)
                # SiLU æ¿€æ´» (å…¼å®¹æ—§ç‰ˆ PyTorch)
                if hasattr(torch, 'silu'):
                    o1_activated = torch.silu(o1)
                else:
                    o1_activated = o1 * torch.sigmoid(o1)  # SiLU = x * sigmoid(x)
                result = o1_activated * o2
                return result
            
            torch_time = self.time_operation(torch_gated_mlp)
            results["torch_time_ms"] = torch_time
            results["torch_throughput"] = 1000 / torch_time
            results["device"] = device
            
            print(f"  PyTorch å®ç° ({device}): {torch_time:.3f} ms")
            
            # ä½¿ç”¨ nn.Linear çš„æ›´é«˜çº§å®ç°
            class GatedMLP(nn.Module):
                def __init__(self, hidden_size):
                    super().__init__()
                    self.gate_proj = nn.Linear(hidden_size, hidden_size, bias=False)
                    self.up_proj = nn.Linear(hidden_size, hidden_size, bias=False)
                    
                def forward(self, x):
                    gate = self.gate_proj(x)
                    up = self.up_proj(x)
                    # SiLU æ¿€æ´» (å…¼å®¹æ—§ç‰ˆ PyTorch)
                    if hasattr(torch, 'silu'):
                        return torch.silu(gate) * up
                    else:
                        return (gate * torch.sigmoid(gate)) * up
            
            model = GatedMLP(hidden_size).to(device).half()
            x = torch.randn(batch_size, hidden_size, device=device, dtype=torch.float16)
            
            def torch_nn_gated_mlp():
                return model(x)
            
            torch_nn_time = self.time_operation(torch_nn_gated_mlp)
            results["torch_nn_time_ms"] = torch_nn_time
            results["torch_nn_throughput"] = 1000 / torch_nn_time
            
            print(f"  PyTorch nn.Module å®ç° ({device}): {torch_nn_time:.3f} ms")
        
        return results

    def benchmark_group_query_attention(self, batch_size: int = 2, seq_len: int = 256, 
                                       hidden_size: int = 64, kv_len: int = 4096) -> Dict:
        """åŸºå‡†æµ‹è¯•ç»„æŸ¥è¯¢æ³¨æ„åŠ›"""
        print(f"ğŸ¯ åŸºå‡†æµ‹è¯•ç»„æŸ¥è¯¢æ³¨æ„åŠ› (batch_size={batch_size}, seq_len={seq_len})")
        
        results = {
            "batch_size": batch_size,
            "seq_len": seq_len,
            "hidden_size": hidden_size,
            "kv_len": kv_len,
            "test_type": "group_query_attention"
        }
        
        if not TORCH_AVAILABLE:
            results["status"] = "SKIPPED - PyTorch required for attention"
            return results
            
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        
        def torch_group_query_attention():
            # æ¨¡æ‹Ÿç»„æŸ¥è¯¢æ³¨æ„åŠ›
            Q = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.float16)
            K = torch.randn(batch_size, hidden_size, kv_len, device=device, dtype=torch.float16)
            V = torch.randn(batch_size, kv_len, hidden_size, device=device, dtype=torch.float16)
            
            # æ³¨æ„åŠ›è®¡ç®—
            A = torch.matmul(Q, K)  # [batch, seq_len, kv_len]
            A_exp = torch.exp(A)
            A_sum = torch.sum(A_exp, dim=-1, keepdim=True)  # [batch, seq_len, 1]
            A_softmax = A_exp / A_sum
            O = torch.matmul(A_softmax, V)  # [batch, seq_len, hidden_size]
            
            return O
        
        attention_time = self.time_operation(torch_group_query_attention)
        results["torch_time_ms"] = attention_time
        results["torch_throughput"] = 1000 / attention_time
        results["device"] = device
        
        print(f"  PyTorch å®ç° ({device}): {attention_time:.3f} ms")
        
        return results

    def benchmark_lora(self, input_size: int = 16, hidden_size: int = 256, 
                      output_size: int = 4096, rank: int = 16) -> Dict:
        """åŸºå‡†æµ‹è¯• LoRAï¼ˆä½ç§©é€‚åº”ï¼‰"""
        print(f"ğŸ”„ åŸºå‡†æµ‹è¯• LoRA (rank={rank}, sizes=[{input_size}, {hidden_size}, {output_size}])")
        
        results = {
            "input_size": input_size,
            "hidden_size": hidden_size,
            "output_size": output_size,
            "rank": rank,
            "test_type": "lora"
        }
        
        if not TORCH_AVAILABLE:
            results["status"] = "SKIPPED - PyTorch required for LoRA"
            return results
            
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        
        def torch_lora():
            # è¾“å…¥æ•°æ®
            X = torch.randn(input_size, hidden_size, device=device, dtype=torch.float16)
            
            # åŸå§‹æƒé‡çŸ©é˜µ
            W = torch.randn(hidden_size, output_size, device=device, dtype=torch.float16)
            
            # LoRA æƒé‡çŸ©é˜µ
            A = torch.randn(hidden_size, rank, device=device, dtype=torch.float16)
            B = torch.randn(rank, output_size, device=device, dtype=torch.float16)
            
            # LoRA è®¡ç®—ï¼šX @ W + X @ A @ B
            base_output = torch.matmul(X, W)
            lora_delta = torch.matmul(torch.matmul(X, A), B)
            final_output = base_output + lora_delta
            
            return final_output
        
        lora_time = self.time_operation(torch_lora)
        results["torch_time_ms"] = lora_time
        results["torch_throughput"] = 1000 / lora_time
        results["device"] = device
        
        print(f"  PyTorch LoRA å®ç° ({device}): {lora_time:.3f} ms")
        
        # å¯¹æ¯”æ ‡å‡†å…¨è¿æ¥
        def torch_standard():
            X = torch.randn(input_size, hidden_size, device=device, dtype=torch.float16)
            W = torch.randn(hidden_size, output_size, device=device, dtype=torch.float16)
            return torch.matmul(X, W)
        
        standard_time = self.time_operation(torch_standard)
        results["standard_time_ms"] = standard_time
        results["standard_throughput"] = 1000 / standard_time
        results["lora_overhead"] = (lora_time / standard_time - 1) * 100  # ç™¾åˆ†æ¯”å¼€é”€
        
        print(f"  æ ‡å‡†å…¨è¿æ¥ ({device}): {standard_time:.3f} ms")
        print(f"  LoRA å¼€é”€: {results['lora_overhead']:.1f}%")
        
        return results

    def benchmark_matrix_operations(self, sizes: List[int] = None) -> Dict:
        """åŸºå‡†æµ‹è¯•åŸºç¡€çŸ©é˜µè¿ç®—"""
        if sizes is None:
            sizes = [128, 256, 512, 1024]
            
        print("ğŸ“Š åŸºå‡†æµ‹è¯•çŸ©é˜µè¿ç®—")
        
        results = {
            "test_type": "matrix_operations",
            "sizes_tested": sizes,
            "results": {}
        }
        
        for size in sizes:
            print(f"  æµ‹è¯• {size}x{size} çŸ©é˜µ")
            size_results = {}
            
            # NumPy åŸºå‡†
            if NUMPY_AVAILABLE:
                def numpy_matmul():
                    a = np.random.randn(size, size).astype(np.float32)
                    b = np.random.randn(size, size).astype(np.float32)
                    return np.dot(a, b)
                
                numpy_time = self.time_operation(numpy_matmul)
                size_results["numpy_time_ms"] = numpy_time
                size_results["numpy_gflops"] = (2 * size**3) / (numpy_time * 1e6)  # GFLOPS
                
                print(f"    NumPy: {numpy_time:.3f} ms ({size_results['numpy_gflops']:.2f} GFLOPS)")
            
            # PyTorch åŸºå‡†
            if TORCH_AVAILABLE:
                device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
                
                def torch_matmul():
                    a = torch.randn(size, size, device=device, dtype=torch.float32)
                    b = torch.randn(size, size, device=device, dtype=torch.float32)
                    return torch.mm(a, b)
                
                torch_time = self.time_operation(torch_matmul)
                size_results["torch_time_ms"] = torch_time
                size_results["torch_gflops"] = (2 * size**3) / (torch_time * 1e6)  # GFLOPS
                size_results["device"] = device
                
                print(f"    PyTorch ({device}): {torch_time:.3f} ms ({size_results['torch_gflops']:.2f} GFLOPS)")
            
            results["results"][f"{size}x{size}"] = size_results
        
        return results

    def benchmark_yica_api(self) -> Dict:
        """åŸºå‡†æµ‹è¯• YICA API æ€§èƒ½"""
        print("ğŸ”§ åŸºå‡†æµ‹è¯• YICA API")
        
        results = {
            "test_type": "yica_api",
            "yirage_available": YIRAGE_AVAILABLE
        }
        
        if not YIRAGE_AVAILABLE:
            results["status"] = "SKIPPED - YICA package not available"
            return results
        
        # æµ‹è¯• API è°ƒç”¨æ€§èƒ½
        def test_optimizer_creation():
            return yirage.create_yica_optimizer()
        
        def test_performance_monitor_creation():
            return yirage.create_performance_monitor()
        
        def test_version_info():
            return yirage.get_version_info()
        
        # æµ‹è¯•å„ç§ API è°ƒç”¨çš„å»¶è¿Ÿ
        optimizer_time = self.time_operation(test_optimizer_creation)
        monitor_time = self.time_operation(test_performance_monitor_creation)
        version_time = self.time_operation(test_version_info)
        
        results["optimizer_creation_ms"] = optimizer_time
        results["monitor_creation_ms"] = monitor_time
        results["version_info_ms"] = version_time
        
        print(f"  ä¼˜åŒ–å™¨åˆ›å»º: {optimizer_time:.3f} ms")
        print(f"  æ€§èƒ½ç›‘æ§å™¨åˆ›å»º: {monitor_time:.3f} ms")  
        print(f"  ç‰ˆæœ¬ä¿¡æ¯è·å–: {version_time:.3f} ms")
        
        return results

    def run_all_benchmarks(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œ YICA åŸºå‡†æµ‹è¯•å¥—ä»¶")
        print(f"é…ç½®: warmup={self.warmup_iters}, profile={self.profile_iters}")
        
        all_results = {
            "benchmark_config": {
                "warmup_iterations": self.warmup_iters,
                "profile_iterations": self.profile_iters,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "environment": {
                "numpy_available": NUMPY_AVAILABLE,
                "torch_available": TORCH_AVAILABLE,
                "yirage_available": YIRAGE_AVAILABLE,
                "cuda_available": torch.cuda.is_available() if TORCH_AVAILABLE else False
            },
            "results": {}
        }
        
        # è¿è¡Œå„é¡¹åŸºå‡†æµ‹è¯•
        benchmarks = [
            ("matrix_operations", lambda: self.benchmark_matrix_operations()),
            ("gated_mlp", lambda: self.benchmark_gated_mlp()),
            ("group_query_attention", lambda: self.benchmark_group_query_attention()),
            ("lora", lambda: self.benchmark_lora()),
            ("yica_api", lambda: self.benchmark_yica_api()),
        ]
        
        for name, benchmark_func in benchmarks:
            try:
                print(f"\n--- {name.upper()} ---")
                result = benchmark_func()
                all_results["results"][name] = result
                print(f"âœ… {name} å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {name} å¤±è´¥: {str(e)}")
                all_results["results"][name] = {
                    "status": "FAILED",
                    "error": str(e)
                }
        
        return all_results
    
    def save_results(self, results: Dict, output_file: str = None):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        if output_file is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"yica_benchmark_results_{timestamp}.json"
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="YICA-Yirage åŸºç¡€åŸºå‡†æµ‹è¯•")
    parser.add_argument('--warmup', type=int, default=16, help='é¢„çƒ­è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--profile', type=int, default=1000, help='æ€§èƒ½æµ‹è¯•è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--test', type=str, choices=['all', 'matrix', 'gated_mlp', 'attention', 'lora', 'api'],
                       default='all', help='è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹')
    
    args = parser.parse_args()
    
    runner = YICABenchmarkRunner(warmup_iters=args.warmup, profile_iters=args.profile)
    
    if args.test == 'all':
        results = runner.run_all_benchmarks()
    elif args.test == 'matrix':
        results = {"results": {"matrix_operations": runner.benchmark_matrix_operations()}}
    elif args.test == 'gated_mlp':
        results = {"results": {"gated_mlp": runner.benchmark_gated_mlp()}}
    elif args.test == 'attention':
        results = {"results": {"group_query_attention": runner.benchmark_group_query_attention()}}
    elif args.test == 'lora':
        results = {"results": {"lora": runner.benchmark_lora()}}
    elif args.test == 'api':
        results = {"results": {"yica_api": runner.benchmark_yica_api()}}
    
    # ä¿å­˜ç»“æœ
    runner.save_results(results, args.output)
    
    print("\nğŸ åŸºå‡†æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main() 