#!/usr/bin/env python3
"""
YICA-Yirage ç»¼åˆæµ‹è¯•å¥—ä»¶

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºéªŒè¯å’ŒåŸºå‡†æµ‹è¯• YICA ä¼˜åŒ–çš„å„ç§åŠŸèƒ½ã€‚
åŒ…å«ï¼š
- åŸºç¡€æ“ä½œæµ‹è¯•ï¼ˆçŸ©é˜µè¿ç®—ã€ä¼˜åŒ–å™¨ã€æ€§èƒ½ç›‘æ§ï¼‰
- AI æ¨¡å‹ç»„ä»¶æµ‹è¯•ï¼ˆTransformerã€MLPã€Attentionï¼‰
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- å†…å­˜æ•ˆç‡åˆ†æ
- å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•
- Python API é›†æˆæµ‹è¯•
"""

import os
import sys
import time
import json
import logging
import traceback
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
import unittest
from contextlib import contextmanager

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
sys.path.insert(0, str(Path(__file__).parent.parent / "yirage" / "python"))

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import yirage
    YIRAGE_AVAILABLE = True
except ImportError:
    YIRAGE_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    warmup_iterations: int = 5
    benchmark_iterations: int = 20
    batch_sizes: List[int] = None
    sequence_lengths: List[int] = None
    hidden_sizes: List[int] = None
    enable_performance_tests: bool = True
    enable_memory_tests: bool = True
    enable_cli_tests: bool = True
    output_dir: str = "./test_results"
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [1, 4, 8, 16]
        if self.sequence_lengths is None:
            self.sequence_lengths = [128, 256, 512]
        if self.hidden_sizes is None:
            self.hidden_sizes = [512, 768, 1024]


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_name: str
    status: str  # "PASS", "FAIL", "SKIP"
    execution_time: float
    memory_usage: Optional[float] = None
    error_message: Optional[str] = None
    details: Optional[Dict] = None


class YICATestSuite:
    """YICA ç»¼åˆæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, config: TestConfig = None):
        self.config = config or TestConfig()
        self.results: List[TestResult] = []
        self.setup_output_dir()
        
    def setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_path = Path(self.config.output_dir)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
    @contextmanager
    def timer(self):
        """è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        yield
        end_time = time.time()
        self.last_execution_time = end_time - start_time
        
    def run_test(self, test_func: Callable, test_name: str, *args, **kwargs) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        logger.info(f"è¿è¡Œæµ‹è¯•: {test_name}")
        
        try:
            with self.timer():
                result_data = test_func(*args, **kwargs)
            
            result = TestResult(
                test_name=test_name,
                status="PASS",
                execution_time=self.last_execution_time,
                details=result_data
            )
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({self.last_execution_time:.3f}s)")
            
        except Exception as e:
            result = TestResult(
                test_name=test_name,
                status="FAIL",
                execution_time=getattr(self, 'last_execution_time', 0.0),
                error_message=str(e),
                details={"traceback": traceback.format_exc()}
            )
            logger.error(f"âŒ {test_name} - å¤±è´¥: {str(e)}")
            
        self.results.append(result)
        return result
    
    def test_package_import(self) -> Dict:
        """æµ‹è¯•åŒ…å¯¼å…¥"""
        results = {}
        
        # æµ‹è¯•åŸºç¡€å¯¼å…¥
        try:
            import yirage
            results["yirage_import"] = "SUCCESS"
            results["yirage_version"] = yirage.__version__
        except Exception as e:
            results["yirage_import"] = f"FAILED: {str(e)}"
            
        # æµ‹è¯•ç‰ˆæœ¬ä¿¡æ¯
        try:
            version_info = yirage.get_version_info()
            results["version_info"] = version_info
        except Exception as e:
            results["version_info"] = f"FAILED: {str(e)}"
            
        # æµ‹è¯•å¯ç”¨æ¨¡å—
        modules_to_test = [
            "yica_optimizer", "yica_performance_monitor", "yica_advanced",
            "yica_auto_tuner", "yica_distributed", "visualizer"
        ]
        
        for module_name in modules_to_test:
            try:
                module = getattr(yirage, module_name, None)
                if module:
                    results[f"{module_name}_available"] = "SUCCESS"
                else:
                    results[f"{module_name}_available"] = "NOT_FOUND"
            except Exception as e:
                results[f"{module_name}_available"] = f"FAILED: {str(e)}"
                
        return results
    
    def test_yica_optimizer(self) -> Dict:
        """æµ‹è¯• YICA ä¼˜åŒ–å™¨"""
        results = {}
        
        try:
            # æµ‹è¯•ä¼˜åŒ–å™¨åˆ›å»º
            optimizer = yirage.create_yica_optimizer()
            results["optimizer_creation"] = "SUCCESS"
            
            # æµ‹è¯•é…ç½®è®¾ç½®
            yirage.set_gpu_device_id(0)
            results["gpu_device_setting"] = "SUCCESS"
            
            # æµ‹è¯•é”™è¯¯å¤„ç†
            yirage.bypass_compile_errors(True)
            results["error_handling"] = "SUCCESS"
            
        except Exception as e:
            results["optimizer_test"] = f"FAILED: {str(e)}"
            
        return results
    
    def test_performance_monitor(self) -> Dict:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
        results = {}
        
        try:
            # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
            monitor = yirage.create_performance_monitor()
            results["monitor_creation"] = "SUCCESS"
            
            # æµ‹è¯•ç›‘æ§é…ç½®
            config = {
                "collection_interval": 1.0,
                "analysis_interval": 5.0,
                "window_size": 10
            }
            monitor_with_config = yirage.create_performance_monitor(config)
            results["monitor_with_config"] = "SUCCESS"
            
        except Exception as e:
            results["monitor_test"] = f"FAILED: {str(e)}"
            
        return results
    
    def test_cli_tools(self) -> Dict:
        """æµ‹è¯•å‘½ä»¤è¡Œå·¥å…·"""
        results = {}
        
        cli_commands = [
            "yica-optimizer",
            "yica-benchmark", 
            "yica-analyze"
        ]
        
        for cmd in cli_commands:
            try:
                # æµ‹è¯•å‘½ä»¤æ˜¯å¦å¯æ‰§è¡Œ
                result = subprocess.run(
                    [cmd], 
                    capture_output=True, 
                    text=True, 
                    timeout=10
                )
                
                if result.returncode == 0:
                    results[f"{cmd}_execution"] = "SUCCESS"
                    results[f"{cmd}_output"] = result.stdout.strip()
                else:
                    results[f"{cmd}_execution"] = f"FAILED: {result.stderr}"
                    
            except subprocess.TimeoutExpired:
                results[f"{cmd}_execution"] = "TIMEOUT"
            except FileNotFoundError:
                results[f"{cmd}_execution"] = "NOT_FOUND"
            except Exception as e:
                results[f"{cmd}_execution"] = f"ERROR: {str(e)}"
                
        return results
    
    def test_matrix_operations(self) -> Dict:
        """æµ‹è¯•çŸ©é˜µè¿ç®—æ€§èƒ½"""
        results = {}
        
        if not NUMPY_AVAILABLE or not TORCH_AVAILABLE:
            results["status"] = "SKIPPED - NumPy or PyTorch not available"
            return results
            
        try:
            # æµ‹è¯•ä¸åŒå¤§å°çš„çŸ©é˜µè¿ç®—
            sizes = [128, 256, 512, 1024]
            
            for size in sizes:
                # åˆ›å»ºæµ‹è¯•çŸ©é˜µ
                a = np.random.randn(size, size).astype(np.float32)
                b = np.random.randn(size, size).astype(np.float32)
                
                # NumPy åŸºå‡†
                start_time = time.time()
                for _ in range(self.config.warmup_iterations):
                    np_result = np.dot(a, b)
                numpy_time = time.time() - start_time
                
                results[f"numpy_matmul_{size}x{size}"] = {
                    "time": numpy_time,
                    "ops_per_sec": self.config.warmup_iterations / numpy_time
                }
                
                # PyTorch åŸºå‡†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if TORCH_AVAILABLE:
                    a_torch = torch.from_numpy(a)
                    b_torch = torch.from_numpy(b)
                    
                    start_time = time.time()
                    for _ in range(self.config.warmup_iterations):
                        torch_result = torch.mm(a_torch, b_torch)
                    torch_time = time.time() - start_time
                    
                    results[f"torch_matmul_{size}x{size}"] = {
                        "time": torch_time,
                        "ops_per_sec": self.config.warmup_iterations / torch_time
                    }
                    
        except Exception as e:
            results["matrix_operations"] = f"FAILED: {str(e)}"
            
        return results
    
    def test_transformer_components(self) -> Dict:
        """æµ‹è¯• Transformer ç»„ä»¶"""
        results = {}
        
        if not TORCH_AVAILABLE:
            results["status"] = "SKIPPED - PyTorch not available"
            return results
            
        try:
            # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
            batch_size, seq_len, hidden_size = 4, 128, 512
            num_heads = 8
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            x = torch.randn(batch_size, seq_len, hidden_size)
            
            # ç®€å•çš„å¤šå¤´æ³¨æ„åŠ›å®ç°
            class SimpleMultiHeadAttention(nn.Module):
                def __init__(self, hidden_size, num_heads):
                    super().__init__()
                    self.hidden_size = hidden_size
                    self.num_heads = num_heads
                    self.head_dim = hidden_size // num_heads
                    
                    self.q_proj = nn.Linear(hidden_size, hidden_size)
                    self.k_proj = nn.Linear(hidden_size, hidden_size)
                    self.v_proj = nn.Linear(hidden_size, hidden_size)
                    self.out_proj = nn.Linear(hidden_size, hidden_size)
                    
                def forward(self, x):
                    B, L, H = x.shape
                    q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
                    k = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
                    v = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
                    
                    attn = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
                    attn = F.softmax(attn, dim=-1)
                    out = torch.matmul(attn, v)
                    
                    out = out.transpose(1, 2).contiguous().view(B, L, H)
                    return self.out_proj(out)
            
            # æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶
            attention = SimpleMultiHeadAttention(hidden_size, num_heads)
            
            start_time = time.time()
            for _ in range(self.config.warmup_iterations):
                output = attention(x)
            attention_time = time.time() - start_time
            
            results["multi_head_attention"] = {
                "time": attention_time,
                "input_shape": list(x.shape),
                "output_shape": list(output.shape),
                "throughput": self.config.warmup_iterations / attention_time
            }
            
            # æµ‹è¯• MLP
            class SimpleMLP(nn.Module):
                def __init__(self, hidden_size, intermediate_size):
                    super().__init__()
                    self.dense1 = nn.Linear(hidden_size, intermediate_size)
                    self.dense2 = nn.Linear(intermediate_size, hidden_size)
                    
                def forward(self, x):
                    x = self.dense1(x)
                    x = F.gelu(x)
                    x = self.dense2(x)
                    return x
            
            mlp = SimpleMLP(hidden_size, hidden_size * 4)
            
            start_time = time.time()
            for _ in range(self.config.warmup_iterations):
                output = mlp(x)
            mlp_time = time.time() - start_time
            
            results["mlp"] = {
                "time": mlp_time,
                "throughput": self.config.warmup_iterations / mlp_time
            }
            
        except Exception as e:
            results["transformer_components"] = f"FAILED: {str(e)}"
            
        return results
    
    def test_memory_efficiency(self) -> Dict:
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        results = {}
        
        try:
            import psutil
            process = psutil.Process()
            
            # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ‰§è¡Œä¸€äº›å†…å­˜å¯†é›†å‹æ“ä½œ
            if NUMPY_AVAILABLE:
                arrays = []
                for i in range(10):
                    arr = np.random.randn(1000, 1000).astype(np.float32)
                    arrays.append(arr)
                
                # æµ‹é‡å³°å€¼å†…å­˜
                peak_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                # æ¸…ç†
                del arrays
                
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                results["memory_test"] = {
                    "initial_memory_mb": initial_memory,
                    "peak_memory_mb": peak_memory,
                    "final_memory_mb": final_memory,
                    "memory_increase_mb": peak_memory - initial_memory,
                    "memory_freed_mb": peak_memory - final_memory
                }
            else:
                results["memory_test"] = "SKIPPED - NumPy not available"
                
        except ImportError:
            results["memory_test"] = "SKIPPED - psutil not available"
        except Exception as e:
            results["memory_test"] = f"FAILED: {str(e)}"
            
        return results
    
    def test_advanced_features(self) -> Dict:
        """æµ‹è¯•é«˜çº§åŠŸèƒ½"""
        results = {}
        
        try:
            # æµ‹è¯•å¿«é€Ÿåˆ†æåŠŸèƒ½
            if hasattr(yirage, 'quick_analyze'):
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„"æ¨¡å‹"ç”¨äºåˆ†æ
                mock_model = {
                    "type": "transformer",
                    "layers": 12,
                    "hidden_size": 768,
                    "operations": ["matmul", "softmax", "layer_norm"]
                }
                
                analysis_result = yirage.quick_analyze(mock_model, "O2")
                results["quick_analyze"] = {
                    "status": "SUCCESS",
                    "result_keys": list(analysis_result.keys())
                }
            else:
                results["quick_analyze"] = "NOT_AVAILABLE"
                
        except Exception as e:
            results["advanced_features"] = f"FAILED: {str(e)}"
            
        return results
    
    def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œ YICA ç»¼åˆæµ‹è¯•å¥—ä»¶")
        logger.info(f"æµ‹è¯•é…ç½®: {self.config}")
        
        start_time = time.time()
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        test_suite = [
            (self.test_package_import, "Package Import Test"),
            (self.test_yica_optimizer, "YICA Optimizer Test"),
            (self.test_performance_monitor, "Performance Monitor Test"),
            (self.test_cli_tools, "CLI Tools Test"),
            (self.test_matrix_operations, "Matrix Operations Test"),
            (self.test_transformer_components, "Transformer Components Test"),
            (self.test_memory_efficiency, "Memory Efficiency Test"),
            (self.test_advanced_features, "Advanced Features Test"),
        ]
        
        for test_func, test_name in test_suite:
            self.run_test(test_func, test_name)
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self.generate_report(total_time)
        
        # ä¿å­˜ç»“æœ
        self.save_results(report)
        
        logger.info(f"ğŸ æµ‹è¯•å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        return report
    
    def generate_report(self, total_time: float) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        passed = sum(1 for r in self.results if r.status == "PASS")
        failed = sum(1 for r in self.results if r.status == "FAIL")
        skipped = sum(1 for r in self.results if r.status == "SKIP")
        
        report = {
            "test_summary": {
                "total_tests": len(self.results),
                "passed": passed,
                "failed": failed,
                "skipped": skipped,
                "success_rate": passed / len(self.results) * 100 if self.results else 0,
                "total_execution_time": total_time,
                "timestamp": datetime.now().isoformat()
            },
            "environment": {
                "python_version": sys.version,
                "numpy_available": NUMPY_AVAILABLE,
                "torch_available": TORCH_AVAILABLE,
                "yirage_available": YIRAGE_AVAILABLE,
            },
            "test_results": [asdict(result) for result in self.results],
            "config": asdict(self.config)
        }
        
        return report
    
    def save_results(self, report: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ JSON æŠ¥å‘Š
        json_file = self.output_path / f"yica_test_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_file = self.output_path / f"yica_test_report_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            self.write_text_report(f, report)
        
        logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"  JSON: {json_file}")
        logger.info(f"  æ–‡æœ¬: {txt_file}")
    
    def write_text_report(self, f, report: Dict):
        """å†™å…¥æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        f.write("=" * 80 + "\n")
        f.write("YICA-Yirage ç»¼åˆæµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # æµ‹è¯•æ‘˜è¦
        summary = report["test_summary"]
        f.write(f"æµ‹è¯•æ—¶é—´: {summary['timestamp']}\n")
        f.write(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}\n")
        f.write(f"é€šè¿‡: {summary['passed']}\n")
        f.write(f"å¤±è´¥: {summary['failed']}\n")
        f.write(f"è·³è¿‡: {summary['skipped']}\n")
        f.write(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%\n")
        f.write(f"æ€»è€—æ—¶: {summary['total_execution_time']:.2f}ç§’\n\n")
        
        # ç¯å¢ƒä¿¡æ¯
        f.write("ç¯å¢ƒä¿¡æ¯:\n")
        f.write("-" * 40 + "\n")
        env = report["environment"]
        f.write(f"Python ç‰ˆæœ¬: {env['python_version'].split()[0]}\n")
        f.write(f"NumPy å¯ç”¨: {env['numpy_available']}\n")
        f.write(f"PyTorch å¯ç”¨: {env['torch_available']}\n")
        f.write(f"Yirage å¯ç”¨: {env['yirage_available']}\n\n")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        f.write("è¯¦ç»†æµ‹è¯•ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        for result in report["test_results"]:
            status_symbol = "âœ…" if result["status"] == "PASS" else "âŒ" if result["status"] == "FAIL" else "â­ï¸"
            f.write(f"{status_symbol} {result['test_name']}\n")
            f.write(f"   çŠ¶æ€: {result['status']}\n")
            f.write(f"   è€—æ—¶: {result['execution_time']:.3f}ç§’\n")
            if result.get('error_message'):
                f.write(f"   é”™è¯¯: {result['error_message']}\n")
            f.write("\n")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        passed = sum(1 for r in self.results if r.status == "PASS")
        failed = sum(1 for r in self.results if r.status == "FAIL")
        skipped = sum(1 for r in self.results if r.status == "SKIP")
        
        print("\n" + "=" * 60)
        print("YICA æµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {len(self.results)}")
        print(f"âœ… é€šè¿‡: {passed}")
        print(f"âŒ å¤±è´¥: {failed}")
        print(f"â­ï¸  è·³è¿‡: {skipped}")
        print(f"ğŸ¯ æˆåŠŸç‡: {passed/len(self.results)*100:.1f}%" if self.results else "0%")
        print("=" * 60)
        
        # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
        if failed > 0:
            print("\nå¤±è´¥çš„æµ‹è¯•:")
            print("-" * 30)
            for result in self.results:
                if result.status == "FAIL":
                    print(f"âŒ {result.test_name}: {result.error_message}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YICA-Yirage ç»¼åˆæµ‹è¯•å¥—ä»¶")
    parser.add_argument("--config", default="default", choices=["default", "quick", "full"],
                      help="æµ‹è¯•é…ç½® (default, quick, full)")
    parser.add_argument("--output-dir", default="./test_results",
                      help="è¾“å‡ºç›®å½•")
    parser.add_argument("--skip-performance", action="store_true",
                      help="è·³è¿‡æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--skip-cli", action="store_true",
                      help="è·³è¿‡å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•")
    
    args = parser.parse_args()
    
    # æ ¹æ®é…ç½®è®¾ç½®æµ‹è¯•å‚æ•°
    if args.config == "quick":
        config = TestConfig(
            warmup_iterations=2,
            benchmark_iterations=5,
            batch_sizes=[1, 4],
            sequence_lengths=[128],
            hidden_sizes=[512],
            enable_performance_tests=not args.skip_performance,
            enable_cli_tests=not args.skip_cli,
            output_dir=args.output_dir
        )
    elif args.config == "full":
        config = TestConfig(
            warmup_iterations=10,
            benchmark_iterations=50,
            batch_sizes=[1, 4, 8, 16, 32],
            sequence_lengths=[128, 256, 512, 1024],
            hidden_sizes=[512, 768, 1024, 2048],
            enable_performance_tests=not args.skip_performance,
            enable_cli_tests=not args.skip_cli,
            output_dir=args.output_dir
        )
    else:  # default
        config = TestConfig(
            enable_performance_tests=not args.skip_performance,
            enable_cli_tests=not args.skip_cli,
            output_dir=args.output_dir
        )
    
    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    test_suite = YICATestSuite(config)
    report = test_suite.run_all_tests()
    test_suite.print_summary()
    
    # è¿”å›é€€å‡ºç 
    failed_count = sum(1 for r in test_suite.results if r.status == "FAIL")
    return 0 if failed_count == 0 else 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 